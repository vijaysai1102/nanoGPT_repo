{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqe0LxWS57UL",
        "outputId": "2eb0d9a1-9c4f-4b0b-8734-210cb2ed9bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Results will be saved to: /content/drive/MyDrive/nanoGPT_results\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/nanoGPT_results\"\n",
        "!mkdir -p \"$SAVE_DIR\"\n",
        "\n",
        "print(\"Results will be saved to:\", SAVE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install tqdm numpy requests matplotlib ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QvnvLGl6g8T",
        "outputId": "5c8692ae-81a5-4284-8c67-0ff9f1cc36d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 974.06 KiB | 42.35 MiB/s, done.\n",
            "Resolving deltas: 100% (380/380), done.\n",
            "/content/nanoGPT\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data/shakespeare_char\n",
        "!python prepare.py\n",
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VESp1ne56lOR",
        "outputId": "8258b52a-0366-4cd9-aa93-45803b5e7e39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT/data/shakespeare_char\n",
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_experiments.py\n",
        "import os, sys, itertools, subprocess, re, csv, time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "SAVE_DIR = os.environ.get(\"SAVE_DIR\", \"/content/drive/MyDrive/nanoGPT_results\")\n",
        "\n",
        "# Full Hyperparameter Grid\n",
        "BLOCK_SIZES = [64, 128]\n",
        "N_LAYERS = [4, 6]\n",
        "N_HEADS = [4, 8]\n",
        "N_EMBDS = [128, 256]\n",
        "BATCH_SIZES = [8, 16]\n",
        "MAX_ITERS = [1000, 2000]\n",
        "DROPOUTS = [0.1, 0.2]\n",
        "\n",
        "# Member → fixed hyperparams\n",
        "MEMBER_MAP = {\n",
        "    1: (64, 4),\n",
        "    2: (64, 6),\n",
        "    3: (128, 4),\n",
        "    4: (128, 6),\n",
        "}\n",
        "\n",
        "CONFIG_TEMPLATE = r\"\"\"\n",
        "out_dir = \"{save_dir}/{out_name}\"\n",
        "dataset = \"shakespeare_char\"\n",
        "eval_interval = 200\n",
        "log_interval = 10\n",
        "always_save_checkpoint = True\n",
        "\n",
        "batch_size = {batch_size}\n",
        "block_size = {block_size}\n",
        "n_layer = {n_layer}\n",
        "n_head = {n_head}\n",
        "n_embd = {n_embd}\n",
        "dropout = {dropout}\n",
        "\n",
        "learning_rate = 3e-4\n",
        "max_iters = {max_iters}\n",
        "lr_decay_iters = {max_iters}\n",
        "\n",
        "seed = {seed}\n",
        "device = \"{device}\"\n",
        "\n",
        "num_workers = 0\n",
        "compile = False\n",
        "\"\"\"\n",
        "\n",
        "def list_experiments(member_id):\n",
        "    block_size, n_layer = MEMBER_MAP[member_id]\n",
        "    grid = list(itertools.product(N_HEADS, N_EMBDS, BATCH_SIZES, MAX_ITERS, DROPOUTS))\n",
        "    exps = []\n",
        "    for seed, (nh, ne, bs, mi, do) in enumerate(grid, 1):\n",
        "        out_name = f\"b{block_size}_L{n_layer}_H{nh}_E{ne}_BS{bs}_MI{mi}_D{int(do*100)}_s{seed}\"\n",
        "        exps.append({\n",
        "            \"block_size\": block_size, \"n_layer\": n_layer,\n",
        "            \"n_head\": nh, \"n_embd\": ne,\n",
        "            \"batch_size\": bs, \"max_iters\": mi,\n",
        "            \"dropout\": do, \"seed\": seed,\n",
        "            \"out_name\": out_name\n",
        "        })\n",
        "    return exps\n",
        "\n",
        "def parse_losses(stdout_line):\n",
        "    m = re.search(r\"train loss ([0-9.]+).*val loss ([0-9.]+)\", stdout_line)\n",
        "    if m:\n",
        "        return float(m.group(1)), float(m.group(2))\n",
        "    return None, None\n",
        "\n",
        "def extract_model_params(logtext):\n",
        "    m = re.search(r\"number of parameters:\\s*([0-9.]+)M\", logtext)\n",
        "    if m:\n",
        "        return float(m.group(1)) * 1e6\n",
        "    return None\n",
        "\n",
        "def run_training(cfg, device):\n",
        "    cfg_file = Path(f\"{cfg['out_name']}.py\")\n",
        "    cfg_file.write_text(CONFIG_TEMPLATE.format(**cfg, save_dir=SAVE_DIR, device=device))\n",
        "\n",
        "    p = subprocess.Popen(\n",
        "        [\"python\", \"train.py\", str(cfg_file)],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "    )\n",
        "\n",
        "    train_loss = None\n",
        "    val_loss = None\n",
        "    param_count = None\n",
        "    log_buf = \"\"\n",
        "\n",
        "    for line in p.stdout:\n",
        "        print(line, end=\"\")\n",
        "        log_buf += line\n",
        "        tl, vl = parse_losses(line)\n",
        "        if tl is not None:\n",
        "            train_loss, val_loss = tl, vl\n",
        "\n",
        "        if param_count is None:\n",
        "            param_count = extract_model_params(log_buf)\n",
        "\n",
        "    p.wait()\n",
        "    loss_gap = val_loss - train_loss if train_loss and val_loss else None\n",
        "    return train_loss, val_loss, loss_gap, param_count, cfg_file\n",
        "\n",
        "def main():\n",
        "    member_id = int(sys.argv[1])\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    result_csv = Path(SAVE_DIR) / \"results.csv\"\n",
        "    if not result_csv.exists():\n",
        "        with open(result_csv, \"w\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                \"Experiment\",\n",
        "                \"Train Loss\",\n",
        "                \"Val Loss\",\n",
        "                \"Loss Gap\",\n",
        "                \"Total Params\",\n",
        "                \"Config Path\"\n",
        "            ])\n",
        "\n",
        "    exps = list_experiments(member_id)\n",
        "    print(f\"Running {len(exps)} experiments for Member {member_id}\")\n",
        "\n",
        "    for i, exp in enumerate(exps, 1):\n",
        "        print(f\"\\n=== Experiment {i}/{len(exps)}: {exp['out_name']} ===\")\n",
        "        tr, vl, gap, params, cfg_path = run_training(exp, device)\n",
        "\n",
        "        with open(result_csv, \"a\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                exp[\"out_name\"], tr, vl, gap, params, str(cfg_path.resolve())\n",
        "            ])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEn8kGId6oYU",
        "outputId": "ca156ad7-df18-42a5-9e6a-a0227e9fd2bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_experiments.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SAVE_DIR\"] = SAVE_DIR"
      ],
      "metadata": {
        "id": "uGKk9EDh6rJs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_experiments.py 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ED3vCJz6yCj",
        "outputId": "177e8ccb-33b6-4f2d-e764-8b8af773c38d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iter 1440: loss 2.0510, time 349.51ms, mfu 0.40%\n",
            "iter 1450: loss 2.0901, time 334.57ms, mfu 0.40%\n",
            "iter 1460: loss 1.9920, time 332.21ms, mfu 0.41%\n",
            "iter 1470: loss 2.0207, time 335.17ms, mfu 0.41%\n",
            "iter 1480: loss 2.0694, time 350.99ms, mfu 0.41%\n",
            "iter 1490: loss 2.0422, time 345.79ms, mfu 0.41%\n",
            "iter 1500: loss 2.0475, time 342.06ms, mfu 0.41%\n",
            "iter 1510: loss 2.0417, time 331.85ms, mfu 0.42%\n",
            "iter 1520: loss 2.0608, time 355.17ms, mfu 0.42%\n",
            "iter 1530: loss 2.0024, time 345.87ms, mfu 0.42%\n",
            "iter 1540: loss 1.9986, time 354.31ms, mfu 0.42%\n",
            "iter 1550: loss 1.9486, time 354.91ms, mfu 0.42%\n",
            "iter 1560: loss 1.9545, time 342.63ms, mfu 0.42%\n",
            "iter 1570: loss 2.0598, time 347.45ms, mfu 0.42%\n",
            "iter 1580: loss 2.0179, time 340.40ms, mfu 0.42%\n",
            "iter 1590: loss 1.9750, time 349.28ms, mfu 0.42%\n",
            "step 1600: train loss 1.8534, val loss 1.9519\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E128_BS16_MI2000_D20_s8\n",
            "iter 1600: loss 2.0078, time 1709.73ms, mfu 0.39%\n",
            "iter 1610: loss 2.0287, time 340.32ms, mfu 0.39%\n",
            "iter 1620: loss 2.0374, time 341.31ms, mfu 0.39%\n",
            "iter 1630: loss 1.9975, time 340.14ms, mfu 0.40%\n",
            "iter 1640: loss 1.9735, time 337.61ms, mfu 0.40%\n",
            "iter 1650: loss 1.9367, time 345.04ms, mfu 0.40%\n",
            "iter 1660: loss 1.9929, time 337.06ms, mfu 0.41%\n",
            "iter 1670: loss 1.9903, time 338.38ms, mfu 0.41%\n",
            "iter 1680: loss 1.9305, time 333.76ms, mfu 0.41%\n",
            "iter 1690: loss 1.9023, time 336.74ms, mfu 0.41%\n",
            "iter 1700: loss 1.9019, time 344.89ms, mfu 0.41%\n",
            "iter 1710: loss 1.9607, time 345.48ms, mfu 0.42%\n",
            "iter 1720: loss 1.9103, time 337.65ms, mfu 0.42%\n",
            "iter 1730: loss 1.8861, time 339.18ms, mfu 0.42%\n",
            "iter 1740: loss 1.9851, time 344.92ms, mfu 0.42%\n",
            "iter 1750: loss 1.8742, time 342.79ms, mfu 0.42%\n",
            "iter 1760: loss 1.9394, time 347.58ms, mfu 0.42%\n",
            "iter 1770: loss 1.9048, time 334.85ms, mfu 0.42%\n",
            "iter 1780: loss 1.9398, time 338.54ms, mfu 0.42%\n",
            "iter 1790: loss 1.8251, time 354.26ms, mfu 0.42%\n",
            "step 1800: train loss 1.7588, val loss 1.9016\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E128_BS16_MI2000_D20_s8\n",
            "iter 1800: loss 1.8706, time 1727.55ms, mfu 0.39%\n",
            "iter 1810: loss 1.8965, time 338.76ms, mfu 0.39%\n",
            "iter 1820: loss 1.8110, time 337.31ms, mfu 0.40%\n",
            "iter 1830: loss 1.8782, time 346.09ms, mfu 0.40%\n",
            "iter 1840: loss 1.9130, time 339.94ms, mfu 0.40%\n",
            "iter 1850: loss 1.8947, time 338.65ms, mfu 0.40%\n",
            "iter 1860: loss 1.9272, time 340.35ms, mfu 0.41%\n",
            "iter 1870: loss 1.8726, time 338.87ms, mfu 0.41%\n",
            "iter 1880: loss 1.9035, time 342.75ms, mfu 0.41%\n",
            "iter 1890: loss 1.8209, time 337.19ms, mfu 0.41%\n",
            "iter 1900: loss 1.9275, time 344.10ms, mfu 0.41%\n",
            "iter 1910: loss 1.8352, time 340.09ms, mfu 0.42%\n",
            "iter 1920: loss 1.8475, time 344.38ms, mfu 0.42%\n",
            "iter 1930: loss 1.7292, time 340.57ms, mfu 0.42%\n",
            "iter 1940: loss 1.8145, time 332.07ms, mfu 0.42%\n",
            "iter 1950: loss 1.8799, time 342.65ms, mfu 0.42%\n",
            "iter 1960: loss 1.8508, time 335.67ms, mfu 0.42%\n",
            "iter 1970: loss 1.8192, time 341.84ms, mfu 0.42%\n",
            "iter 1980: loss 1.8545, time 340.47ms, mfu 0.42%\n",
            "iter 1990: loss 1.8961, time 340.82ms, mfu 0.42%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 9/32: b128_L4_H4_E256_BS8_MI1000_D10_s9 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS8_MI1000_D10_s9.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 9\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1810, val loss 4.1766\n",
            "iter 0: loss 4.2102, time 1949.45ms, mfu -100.00%\n",
            "iter 10: loss 4.1347, time 316.92ms, mfu 0.85%\n",
            "iter 20: loss 3.9905, time 316.51ms, mfu 0.85%\n",
            "iter 30: loss 3.8396, time 315.29ms, mfu 0.85%\n",
            "iter 40: loss 3.6176, time 310.58ms, mfu 0.85%\n",
            "iter 50: loss 3.5673, time 322.53ms, mfu 0.85%\n",
            "iter 60: loss 3.4536, time 327.48ms, mfu 0.85%\n",
            "iter 70: loss 3.3856, time 314.18ms, mfu 0.85%\n",
            "iter 80: loss 3.2935, time 314.62ms, mfu 0.85%\n",
            "iter 90: loss 3.1861, time 312.33ms, mfu 0.85%\n",
            "iter 100: loss 3.1158, time 330.70ms, mfu 0.85%\n",
            "iter 110: loss 3.0879, time 327.68ms, mfu 0.85%\n",
            "iter 120: loss 2.9794, time 312.11ms, mfu 0.85%\n",
            "iter 130: loss 3.0097, time 318.02ms, mfu 0.85%\n",
            "iter 140: loss 2.8452, time 323.91ms, mfu 0.85%\n",
            "iter 150: loss 2.8243, time 328.56ms, mfu 0.84%\n",
            "iter 160: loss 2.8890, time 321.43ms, mfu 0.84%\n",
            "iter 170: loss 2.8241, time 327.55ms, mfu 0.84%\n",
            "iter 180: loss 2.8431, time 314.15ms, mfu 0.84%\n",
            "iter 190: loss 2.7889, time 314.88ms, mfu 0.84%\n",
            "step 200: train loss 2.7030, val loss 2.7117\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 200: loss 2.7306, time 5888.34ms, mfu 0.77%\n",
            "iter 210: loss 2.7130, time 319.82ms, mfu 0.77%\n",
            "iter 220: loss 2.6918, time 313.50ms, mfu 0.78%\n",
            "iter 230: loss 2.7285, time 314.21ms, mfu 0.79%\n",
            "iter 240: loss 2.7246, time 315.57ms, mfu 0.80%\n",
            "iter 250: loss 2.6895, time 322.20ms, mfu 0.80%\n",
            "iter 260: loss 2.6090, time 320.00ms, mfu 0.80%\n",
            "iter 270: loss 2.6084, time 327.55ms, mfu 0.81%\n",
            "iter 280: loss 2.6661, time 323.08ms, mfu 0.81%\n",
            "iter 290: loss 2.5887, time 326.72ms, mfu 0.81%\n",
            "iter 300: loss 2.5707, time 333.78ms, mfu 0.81%\n",
            "iter 310: loss 2.5730, time 331.50ms, mfu 0.81%\n",
            "iter 320: loss 2.5399, time 330.05ms, mfu 0.81%\n",
            "iter 330: loss 2.4974, time 324.59ms, mfu 0.81%\n",
            "iter 340: loss 2.4880, time 309.03ms, mfu 0.82%\n",
            "iter 350: loss 2.5369, time 321.62ms, mfu 0.82%\n",
            "iter 360: loss 2.4879, time 334.55ms, mfu 0.82%\n",
            "iter 370: loss 2.5688, time 323.72ms, mfu 0.82%\n",
            "iter 380: loss 2.4830, time 318.29ms, mfu 0.82%\n",
            "iter 390: loss 2.4343, time 317.11ms, mfu 0.83%\n",
            "step 400: train loss 2.4246, val loss 2.4299\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 400: loss 2.4996, time 1634.12ms, mfu 0.76%\n",
            "iter 410: loss 2.4505, time 319.97ms, mfu 0.77%\n",
            "iter 420: loss 2.4038, time 341.18ms, mfu 0.77%\n",
            "iter 430: loss 2.4268, time 318.95ms, mfu 0.78%\n",
            "iter 440: loss 2.4280, time 326.78ms, mfu 0.78%\n",
            "iter 450: loss 2.4940, time 326.25ms, mfu 0.79%\n",
            "iter 460: loss 2.3135, time 315.62ms, mfu 0.79%\n",
            "iter 470: loss 2.3520, time 319.21ms, mfu 0.80%\n",
            "iter 480: loss 2.3331, time 319.00ms, mfu 0.80%\n",
            "iter 490: loss 2.3306, time 335.15ms, mfu 0.80%\n",
            "iter 500: loss 2.3037, time 316.31ms, mfu 0.81%\n",
            "iter 510: loss 2.3096, time 318.05ms, mfu 0.81%\n",
            "iter 520: loss 2.2760, time 313.76ms, mfu 0.82%\n",
            "iter 530: loss 2.3165, time 317.21ms, mfu 0.82%\n",
            "iter 540: loss 2.3452, time 312.99ms, mfu 0.83%\n",
            "iter 550: loss 2.2851, time 318.24ms, mfu 0.83%\n",
            "iter 560: loss 2.3115, time 324.25ms, mfu 0.83%\n",
            "iter 570: loss 2.3259, time 316.40ms, mfu 0.83%\n",
            "iter 580: loss 2.3947, time 314.17ms, mfu 0.83%\n",
            "iter 590: loss 2.2285, time 314.44ms, mfu 0.84%\n",
            "step 600: train loss 2.2078, val loss 2.2316\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 600: loss 2.2239, time 1656.00ms, mfu 0.77%\n",
            "iter 610: loss 2.2767, time 314.67ms, mfu 0.78%\n",
            "iter 620: loss 2.1873, time 321.30ms, mfu 0.78%\n",
            "iter 630: loss 2.1773, time 322.67ms, mfu 0.79%\n",
            "iter 640: loss 2.1802, time 342.06ms, mfu 0.79%\n",
            "iter 650: loss 2.2617, time 315.52ms, mfu 0.80%\n",
            "iter 660: loss 2.1405, time 314.50ms, mfu 0.80%\n",
            "iter 670: loss 2.2620, time 328.99ms, mfu 0.80%\n",
            "iter 680: loss 2.2510, time 325.73ms, mfu 0.81%\n",
            "iter 690: loss 2.1545, time 319.53ms, mfu 0.81%\n",
            "iter 700: loss 2.0954, time 312.52ms, mfu 0.82%\n",
            "iter 710: loss 2.1868, time 328.37ms, mfu 0.82%\n",
            "iter 720: loss 2.1223, time 323.16ms, mfu 0.82%\n",
            "iter 730: loss 2.2333, time 313.12ms, mfu 0.82%\n",
            "iter 740: loss 2.1661, time 314.75ms, mfu 0.83%\n",
            "iter 750: loss 2.1599, time 324.74ms, mfu 0.83%\n",
            "iter 760: loss 2.1229, time 319.44ms, mfu 0.83%\n",
            "iter 770: loss 2.0781, time 312.42ms, mfu 0.83%\n",
            "iter 780: loss 2.1236, time 314.76ms, mfu 0.83%\n",
            "iter 790: loss 2.0918, time 327.77ms, mfu 0.83%\n",
            "step 800: train loss 1.9868, val loss 2.0402\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 800: loss 2.0589, time 1662.87ms, mfu 0.77%\n",
            "iter 810: loss 2.1337, time 329.80ms, mfu 0.77%\n",
            "iter 820: loss 1.9717, time 326.82ms, mfu 0.78%\n",
            "iter 830: loss 2.0531, time 319.37ms, mfu 0.78%\n",
            "iter 840: loss 2.0043, time 326.65ms, mfu 0.79%\n",
            "iter 850: loss 2.0138, time 325.02ms, mfu 0.79%\n",
            "iter 860: loss 2.0329, time 321.66ms, mfu 0.80%\n",
            "iter 870: loss 2.0087, time 319.46ms, mfu 0.80%\n",
            "iter 880: loss 2.0324, time 315.83ms, mfu 0.81%\n",
            "iter 890: loss 1.9728, time 315.33ms, mfu 0.81%\n",
            "iter 900: loss 2.0053, time 317.05ms, mfu 0.82%\n",
            "iter 910: loss 1.9229, time 316.79ms, mfu 0.82%\n",
            "iter 920: loss 1.8641, time 312.40ms, mfu 0.82%\n",
            "iter 930: loss 1.9280, time 315.14ms, mfu 0.83%\n",
            "iter 940: loss 2.0309, time 318.39ms, mfu 0.83%\n",
            "iter 950: loss 1.8341, time 314.14ms, mfu 0.83%\n",
            "iter 960: loss 1.9079, time 314.57ms, mfu 0.83%\n",
            "iter 970: loss 1.9577, time 319.75ms, mfu 0.84%\n",
            "iter 980: loss 1.8082, time 331.78ms, mfu 0.83%\n",
            "iter 990: loss 1.9522, time 317.70ms, mfu 0.84%\n",
            "step 1000: train loss 1.7878, val loss 1.9034\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 1000: loss 1.9138, time 1678.20ms, mfu 0.77%\n",
            "\n",
            "=== Experiment 10/32: b128_L4_H4_E256_BS8_MI1000_D20_s10 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS8_MI1000_D20_s10.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 10\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1810, val loss 4.1766\n",
            "iter 0: loss 4.2093, time 1932.34ms, mfu -100.00%\n",
            "iter 10: loss 4.1412, time 317.79ms, mfu 0.85%\n",
            "iter 20: loss 4.0299, time 324.53ms, mfu 0.85%\n",
            "iter 30: loss 3.8870, time 325.64ms, mfu 0.85%\n",
            "iter 40: loss 3.6718, time 324.00ms, mfu 0.84%\n",
            "iter 50: loss 3.6087, time 320.49ms, mfu 0.84%\n",
            "iter 60: loss 3.4950, time 320.90ms, mfu 0.84%\n",
            "iter 70: loss 3.4370, time 323.32ms, mfu 0.84%\n",
            "iter 80: loss 3.3567, time 317.32ms, mfu 0.84%\n",
            "iter 90: loss 3.2604, time 319.57ms, mfu 0.84%\n",
            "iter 100: loss 3.1909, time 319.06ms, mfu 0.84%\n",
            "iter 110: loss 3.1550, time 347.44ms, mfu 0.84%\n",
            "iter 120: loss 3.0485, time 335.73ms, mfu 0.83%\n",
            "iter 130: loss 3.0627, time 330.51ms, mfu 0.83%\n",
            "iter 140: loss 2.8897, time 326.62ms, mfu 0.83%\n",
            "iter 150: loss 2.8752, time 329.94ms, mfu 0.83%\n",
            "iter 160: loss 2.9594, time 316.72ms, mfu 0.83%\n",
            "iter 170: loss 2.8640, time 326.60ms, mfu 0.83%\n",
            "iter 180: loss 2.8728, time 328.96ms, mfu 0.83%\n",
            "iter 190: loss 2.8322, time 318.42ms, mfu 0.83%\n",
            "step 200: train loss 2.7184, val loss 2.7240\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 200: loss 2.7725, time 6681.41ms, mfu 0.75%\n",
            "iter 210: loss 2.7540, time 342.21ms, mfu 0.76%\n",
            "iter 220: loss 2.7353, time 321.12ms, mfu 0.77%\n",
            "iter 230: loss 2.7670, time 347.54ms, mfu 0.77%\n",
            "iter 240: loss 2.7740, time 322.54ms, mfu 0.77%\n",
            "iter 250: loss 2.7234, time 323.97ms, mfu 0.78%\n",
            "iter 260: loss 2.6353, time 323.30ms, mfu 0.78%\n",
            "iter 270: loss 2.6337, time 328.70ms, mfu 0.79%\n",
            "iter 280: loss 2.7060, time 329.09ms, mfu 0.79%\n",
            "iter 290: loss 2.6237, time 321.58ms, mfu 0.80%\n",
            "iter 300: loss 2.5893, time 320.37ms, mfu 0.80%\n",
            "iter 310: loss 2.6018, time 328.28ms, mfu 0.80%\n",
            "iter 320: loss 2.5712, time 320.81ms, mfu 0.81%\n",
            "iter 330: loss 2.5305, time 318.35ms, mfu 0.81%\n",
            "iter 340: loss 2.5409, time 326.36ms, mfu 0.81%\n",
            "iter 350: loss 2.5705, time 336.12ms, mfu 0.81%\n",
            "iter 360: loss 2.5219, time 317.12ms, mfu 0.82%\n",
            "iter 370: loss 2.6065, time 329.10ms, mfu 0.82%\n",
            "iter 380: loss 2.5245, time 317.12ms, mfu 0.82%\n",
            "iter 390: loss 2.4751, time 319.25ms, mfu 0.82%\n",
            "step 400: train loss 2.4649, val loss 2.4639\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 400: loss 2.5350, time 1658.60ms, mfu 0.76%\n",
            "iter 410: loss 2.4945, time 329.08ms, mfu 0.76%\n",
            "iter 420: loss 2.4441, time 324.86ms, mfu 0.77%\n",
            "iter 430: loss 2.4631, time 324.84ms, mfu 0.78%\n",
            "iter 440: loss 2.4944, time 333.24ms, mfu 0.78%\n",
            "iter 450: loss 2.5571, time 323.70ms, mfu 0.78%\n",
            "iter 460: loss 2.3583, time 327.69ms, mfu 0.79%\n",
            "iter 470: loss 2.4042, time 317.24ms, mfu 0.79%\n",
            "iter 480: loss 2.3569, time 331.16ms, mfu 0.80%\n",
            "iter 490: loss 2.3930, time 315.46ms, mfu 0.80%\n",
            "iter 500: loss 2.3712, time 319.63ms, mfu 0.81%\n",
            "iter 510: loss 2.3791, time 329.98ms, mfu 0.81%\n",
            "iter 520: loss 2.3246, time 325.49ms, mfu 0.81%\n",
            "iter 530: loss 2.3699, time 323.80ms, mfu 0.81%\n",
            "iter 540: loss 2.4182, time 326.53ms, mfu 0.81%\n",
            "iter 550: loss 2.3676, time 321.10ms, mfu 0.82%\n",
            "iter 560: loss 2.3554, time 331.07ms, mfu 0.82%\n",
            "iter 570: loss 2.3857, time 329.03ms, mfu 0.82%\n",
            "iter 580: loss 2.4593, time 328.61ms, mfu 0.82%\n",
            "iter 590: loss 2.3072, time 327.27ms, mfu 0.82%\n",
            "step 600: train loss 2.2666, val loss 2.2855\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 600: loss 2.3065, time 1709.88ms, mfu 0.75%\n",
            "iter 610: loss 2.3606, time 324.43ms, mfu 0.76%\n",
            "iter 620: loss 2.2853, time 317.64ms, mfu 0.77%\n",
            "iter 630: loss 2.2889, time 320.92ms, mfu 0.78%\n",
            "iter 640: loss 2.2750, time 324.80ms, mfu 0.78%\n",
            "iter 650: loss 2.3320, time 325.51ms, mfu 0.79%\n",
            "iter 660: loss 2.2290, time 324.63ms, mfu 0.79%\n",
            "iter 670: loss 2.3487, time 333.11ms, mfu 0.79%\n",
            "iter 680: loss 2.3234, time 319.13ms, mfu 0.80%\n",
            "iter 690: loss 2.2484, time 323.21ms, mfu 0.80%\n",
            "iter 700: loss 2.1944, time 325.33ms, mfu 0.80%\n",
            "iter 710: loss 2.2672, time 316.84ms, mfu 0.81%\n",
            "iter 720: loss 2.2103, time 327.71ms, mfu 0.81%\n",
            "iter 730: loss 2.3228, time 318.17ms, mfu 0.81%\n",
            "iter 740: loss 2.2812, time 319.26ms, mfu 0.82%\n",
            "iter 750: loss 2.2330, time 317.78ms, mfu 0.82%\n",
            "iter 760: loss 2.2015, time 326.15ms, mfu 0.82%\n",
            "iter 770: loss 2.1964, time 315.46ms, mfu 0.82%\n",
            "iter 780: loss 2.2453, time 317.53ms, mfu 0.83%\n",
            "iter 790: loss 2.1930, time 328.76ms, mfu 0.83%\n",
            "step 800: train loss 2.0655, val loss 2.1036\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 800: loss 2.1783, time 1658.64ms, mfu 0.76%\n",
            "iter 810: loss 2.2484, time 332.29ms, mfu 0.77%\n",
            "iter 820: loss 2.1000, time 327.09ms, mfu 0.77%\n",
            "iter 830: loss 2.1373, time 328.09ms, mfu 0.78%\n",
            "iter 840: loss 2.1387, time 343.98ms, mfu 0.78%\n",
            "iter 850: loss 2.1305, time 323.01ms, mfu 0.78%\n",
            "iter 860: loss 2.1387, time 343.25ms, mfu 0.78%\n",
            "iter 870: loss 2.1426, time 331.53ms, mfu 0.79%\n",
            "iter 880: loss 2.1678, time 319.12ms, mfu 0.79%\n",
            "iter 890: loss 2.0913, time 329.64ms, mfu 0.80%\n",
            "iter 900: loss 2.1408, time 339.88ms, mfu 0.80%\n",
            "iter 910: loss 2.0318, time 327.53ms, mfu 0.80%\n",
            "iter 920: loss 1.9821, time 321.90ms, mfu 0.80%\n",
            "iter 930: loss 2.0596, time 322.03ms, mfu 0.81%\n",
            "iter 940: loss 2.1268, time 328.34ms, mfu 0.81%\n",
            "iter 950: loss 1.9969, time 323.25ms, mfu 0.81%\n",
            "iter 960: loss 2.0097, time 324.50ms, mfu 0.81%\n",
            "iter 970: loss 2.0500, time 328.20ms, mfu 0.81%\n",
            "iter 980: loss 1.9329, time 322.80ms, mfu 0.82%\n",
            "iter 990: loss 2.0640, time 321.22ms, mfu 0.82%\n",
            "step 1000: train loss 1.8885, val loss 1.9842\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 1000: loss 2.0466, time 1668.40ms, mfu 0.75%\n",
            "\n",
            "=== Experiment 11/32: b128_L4_H4_E256_BS8_MI2000_D10_s11 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS8_MI2000_D10_s11.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 11\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1810, val loss 4.1766\n",
            "iter 0: loss 4.2102, time 1922.28ms, mfu -100.00%\n",
            "iter 10: loss 4.1347, time 324.04ms, mfu 0.83%\n",
            "iter 20: loss 3.9905, time 343.09ms, mfu 0.83%\n",
            "iter 30: loss 3.8396, time 322.27ms, mfu 0.83%\n",
            "iter 40: loss 3.6176, time 326.91ms, mfu 0.83%\n",
            "iter 50: loss 3.5673, time 319.88ms, mfu 0.83%\n",
            "iter 60: loss 3.4536, time 325.39ms, mfu 0.83%\n",
            "iter 70: loss 3.3856, time 330.23ms, mfu 0.83%\n",
            "iter 80: loss 3.2935, time 332.78ms, mfu 0.83%\n",
            "iter 90: loss 3.1861, time 320.76ms, mfu 0.83%\n",
            "iter 100: loss 3.1158, time 327.07ms, mfu 0.83%\n",
            "iter 110: loss 3.0879, time 320.13ms, mfu 0.83%\n",
            "iter 120: loss 2.9794, time 329.39ms, mfu 0.83%\n",
            "iter 130: loss 3.0097, time 322.38ms, mfu 0.83%\n",
            "iter 140: loss 2.8452, time 344.88ms, mfu 0.82%\n",
            "iter 150: loss 2.8243, time 328.22ms, mfu 0.82%\n",
            "iter 160: loss 2.8890, time 330.86ms, mfu 0.82%\n",
            "iter 170: loss 2.8241, time 329.48ms, mfu 0.82%\n",
            "iter 180: loss 2.8431, time 320.54ms, mfu 0.83%\n",
            "iter 190: loss 2.7889, time 320.24ms, mfu 0.83%\n",
            "step 200: train loss 2.7030, val loss 2.7117\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 200: loss 2.7306, time 5459.56ms, mfu 0.75%\n",
            "iter 210: loss 2.7130, time 316.28ms, mfu 0.76%\n",
            "iter 220: loss 2.6918, time 319.73ms, mfu 0.77%\n",
            "iter 230: loss 2.7285, time 332.81ms, mfu 0.77%\n",
            "iter 240: loss 2.7246, time 318.90ms, mfu 0.78%\n",
            "iter 250: loss 2.6895, time 323.99ms, mfu 0.79%\n",
            "iter 260: loss 2.6090, time 317.32ms, mfu 0.79%\n",
            "iter 270: loss 2.6084, time 322.97ms, mfu 0.80%\n",
            "iter 280: loss 2.6661, time 326.65ms, mfu 0.80%\n",
            "iter 290: loss 2.5887, time 314.01ms, mfu 0.81%\n",
            "iter 300: loss 2.5707, time 321.34ms, mfu 0.81%\n",
            "iter 310: loss 2.5730, time 336.13ms, mfu 0.81%\n",
            "iter 320: loss 2.5399, time 325.18ms, mfu 0.81%\n",
            "iter 330: loss 2.4974, time 327.07ms, mfu 0.81%\n",
            "iter 340: loss 2.4880, time 323.15ms, mfu 0.81%\n",
            "iter 350: loss 2.5369, time 318.35ms, mfu 0.82%\n",
            "iter 360: loss 2.4879, time 324.01ms, mfu 0.82%\n",
            "iter 370: loss 2.5688, time 320.60ms, mfu 0.82%\n",
            "iter 380: loss 2.4830, time 328.84ms, mfu 0.82%\n",
            "iter 390: loss 2.4343, time 320.64ms, mfu 0.82%\n",
            "step 400: train loss 2.4246, val loss 2.4299\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 400: loss 2.4996, time 1661.45ms, mfu 0.76%\n",
            "iter 410: loss 2.4505, time 324.46ms, mfu 0.76%\n",
            "iter 420: loss 2.4038, time 322.96ms, mfu 0.77%\n",
            "iter 430: loss 2.4268, time 319.62ms, mfu 0.78%\n",
            "iter 440: loss 2.4280, time 328.32ms, mfu 0.78%\n",
            "iter 450: loss 2.4940, time 317.64ms, mfu 0.79%\n",
            "iter 460: loss 2.3135, time 317.47ms, mfu 0.80%\n",
            "iter 470: loss 2.3520, time 318.65ms, mfu 0.80%\n",
            "iter 480: loss 2.3331, time 318.55ms, mfu 0.81%\n",
            "iter 490: loss 2.3306, time 321.42ms, mfu 0.81%\n",
            "iter 500: loss 2.3037, time 326.69ms, mfu 0.81%\n",
            "iter 510: loss 2.3096, time 318.73ms, mfu 0.81%\n",
            "iter 520: loss 2.2760, time 322.12ms, mfu 0.82%\n",
            "iter 530: loss 2.3165, time 374.58ms, mfu 0.81%\n",
            "iter 540: loss 2.3452, time 332.88ms, mfu 0.81%\n",
            "iter 550: loss 2.2851, time 323.39ms, mfu 0.81%\n",
            "iter 560: loss 2.3115, time 338.29ms, mfu 0.81%\n",
            "iter 570: loss 2.3259, time 337.87ms, mfu 0.81%\n",
            "iter 580: loss 2.3947, time 329.02ms, mfu 0.81%\n",
            "iter 590: loss 2.2285, time 332.88ms, mfu 0.81%\n",
            "step 600: train loss 2.2078, val loss 2.2316\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 600: loss 2.2239, time 1667.71ms, mfu 0.74%\n",
            "iter 610: loss 2.2767, time 328.56ms, mfu 0.75%\n",
            "iter 620: loss 2.1873, time 343.07ms, mfu 0.76%\n",
            "iter 630: loss 2.1773, time 346.90ms, mfu 0.76%\n",
            "iter 640: loss 2.1802, time 329.56ms, mfu 0.76%\n",
            "iter 650: loss 2.2617, time 329.02ms, mfu 0.77%\n",
            "iter 660: loss 2.1405, time 331.31ms, mfu 0.77%\n",
            "iter 670: loss 2.2620, time 333.75ms, mfu 0.78%\n",
            "iter 680: loss 2.2510, time 333.34ms, mfu 0.78%\n",
            "iter 690: loss 2.1545, time 327.55ms, mfu 0.79%\n",
            "iter 700: loss 2.0954, time 326.41ms, mfu 0.79%\n",
            "iter 710: loss 2.1868, time 322.91ms, mfu 0.79%\n",
            "iter 720: loss 2.1223, time 324.33ms, mfu 0.80%\n",
            "iter 730: loss 2.2333, time 332.17ms, mfu 0.80%\n",
            "iter 740: loss 2.1661, time 334.23ms, mfu 0.80%\n",
            "iter 750: loss 2.1599, time 329.22ms, mfu 0.80%\n",
            "iter 760: loss 2.1229, time 343.74ms, mfu 0.80%\n",
            "iter 770: loss 2.0781, time 324.63ms, mfu 0.80%\n",
            "iter 780: loss 2.1236, time 331.19ms, mfu 0.80%\n",
            "iter 790: loss 2.0918, time 331.65ms, mfu 0.81%\n",
            "step 800: train loss 1.9868, val loss 2.0402\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 800: loss 2.0589, time 1650.40ms, mfu 0.74%\n",
            "iter 810: loss 2.1337, time 329.96ms, mfu 0.75%\n",
            "iter 820: loss 1.9717, time 325.40ms, mfu 0.76%\n",
            "iter 830: loss 2.0531, time 320.93ms, mfu 0.77%\n",
            "iter 840: loss 2.0043, time 323.90ms, mfu 0.77%\n",
            "iter 850: loss 2.0138, time 330.57ms, mfu 0.78%\n",
            "iter 860: loss 2.0329, time 319.58ms, mfu 0.78%\n",
            "iter 870: loss 2.0087, time 330.40ms, mfu 0.79%\n",
            "iter 880: loss 2.0324, time 321.31ms, mfu 0.79%\n",
            "iter 890: loss 1.9728, time 321.08ms, mfu 0.80%\n",
            "iter 900: loss 2.0053, time 312.51ms, mfu 0.80%\n",
            "iter 910: loss 1.9229, time 320.26ms, mfu 0.81%\n",
            "iter 920: loss 1.8641, time 332.84ms, mfu 0.81%\n",
            "iter 930: loss 1.9280, time 321.14ms, mfu 0.81%\n",
            "iter 940: loss 2.0309, time 322.00ms, mfu 0.81%\n",
            "iter 950: loss 1.8341, time 319.88ms, mfu 0.82%\n",
            "iter 960: loss 1.9079, time 327.58ms, mfu 0.82%\n",
            "iter 970: loss 1.9577, time 321.57ms, mfu 0.82%\n",
            "iter 980: loss 1.8082, time 325.81ms, mfu 0.82%\n",
            "iter 990: loss 1.9522, time 320.39ms, mfu 0.82%\n",
            "step 1000: train loss 1.7878, val loss 1.9034\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1000: loss 1.9138, time 1677.49ms, mfu 0.76%\n",
            "iter 1010: loss 1.9952, time 323.55ms, mfu 0.76%\n",
            "iter 1020: loss 1.9319, time 323.94ms, mfu 0.77%\n",
            "iter 1030: loss 1.8187, time 322.74ms, mfu 0.78%\n",
            "iter 1040: loss 2.0125, time 330.65ms, mfu 0.78%\n",
            "iter 1050: loss 1.8979, time 321.94ms, mfu 0.79%\n",
            "iter 1060: loss 1.8986, time 318.51ms, mfu 0.79%\n",
            "iter 1070: loss 1.8091, time 320.64ms, mfu 0.80%\n",
            "iter 1080: loss 1.7846, time 325.80ms, mfu 0.80%\n",
            "iter 1090: loss 1.7873, time 332.84ms, mfu 0.80%\n",
            "iter 1100: loss 1.9144, time 317.91ms, mfu 0.81%\n",
            "iter 1110: loss 1.8128, time 328.50ms, mfu 0.81%\n",
            "iter 1120: loss 1.7159, time 326.61ms, mfu 0.81%\n",
            "iter 1130: loss 1.7842, time 323.09ms, mfu 0.81%\n",
            "iter 1140: loss 1.7453, time 318.24ms, mfu 0.82%\n",
            "iter 1150: loss 1.9055, time 326.17ms, mfu 0.82%\n",
            "iter 1160: loss 1.7880, time 315.96ms, mfu 0.82%\n",
            "iter 1170: loss 1.7206, time 319.42ms, mfu 0.82%\n",
            "iter 1180: loss 1.7034, time 327.36ms, mfu 0.82%\n",
            "iter 1190: loss 1.7277, time 336.01ms, mfu 0.82%\n",
            "step 1200: train loss 1.6384, val loss 1.8028\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1200: loss 1.7005, time 1648.90ms, mfu 0.76%\n",
            "iter 1210: loss 1.7993, time 316.37ms, mfu 0.77%\n",
            "iter 1220: loss 1.7229, time 333.39ms, mfu 0.77%\n",
            "iter 1230: loss 1.7067, time 337.31ms, mfu 0.77%\n",
            "iter 1240: loss 1.5594, time 327.06ms, mfu 0.78%\n",
            "iter 1250: loss 1.6801, time 323.87ms, mfu 0.78%\n",
            "iter 1260: loss 1.7273, time 316.80ms, mfu 0.79%\n",
            "iter 1270: loss 1.6604, time 318.71ms, mfu 0.80%\n",
            "iter 1280: loss 1.7032, time 319.56ms, mfu 0.80%\n",
            "iter 1290: loss 1.7209, time 319.89ms, mfu 0.81%\n",
            "iter 1300: loss 1.6229, time 322.06ms, mfu 0.81%\n",
            "iter 1310: loss 1.6956, time 321.94ms, mfu 0.81%\n",
            "iter 1320: loss 1.5503, time 325.18ms, mfu 0.81%\n",
            "iter 1330: loss 1.6893, time 324.36ms, mfu 0.82%\n",
            "iter 1340: loss 1.5463, time 325.46ms, mfu 0.82%\n",
            "iter 1350: loss 1.5988, time 319.52ms, mfu 0.82%\n",
            "iter 1360: loss 1.7281, time 316.78ms, mfu 0.82%\n",
            "iter 1370: loss 1.6296, time 329.73ms, mfu 0.82%\n",
            "iter 1380: loss 1.6090, time 326.71ms, mfu 0.82%\n",
            "iter 1390: loss 1.6033, time 326.83ms, mfu 0.82%\n",
            "step 1400: train loss 1.5356, val loss 1.7180\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1400: loss 1.6678, time 1664.35ms, mfu 0.76%\n",
            "iter 1410: loss 1.6320, time 321.43ms, mfu 0.77%\n",
            "iter 1420: loss 1.6547, time 315.60ms, mfu 0.77%\n",
            "iter 1430: loss 1.6510, time 319.26ms, mfu 0.78%\n",
            "iter 1440: loss 1.5779, time 325.81ms, mfu 0.79%\n",
            "iter 1450: loss 1.7176, time 319.73ms, mfu 0.79%\n",
            "iter 1460: loss 1.4561, time 321.30ms, mfu 0.80%\n",
            "iter 1470: loss 1.5148, time 321.49ms, mfu 0.80%\n",
            "iter 1480: loss 1.5819, time 331.07ms, mfu 0.80%\n",
            "iter 1490: loss 1.5749, time 332.13ms, mfu 0.80%\n",
            "iter 1500: loss 1.5244, time 320.87ms, mfu 0.81%\n",
            "iter 1510: loss 1.5498, time 330.92ms, mfu 0.81%\n",
            "iter 1520: loss 1.5858, time 322.74ms, mfu 0.81%\n",
            "iter 1530: loss 1.6063, time 314.53ms, mfu 0.82%\n",
            "iter 1540: loss 1.6518, time 314.89ms, mfu 0.82%\n",
            "iter 1550: loss 1.6314, time 326.53ms, mfu 0.82%\n",
            "iter 1560: loss 1.7470, time 330.84ms, mfu 0.82%\n",
            "iter 1570: loss 1.5662, time 333.70ms, mfu 0.82%\n",
            "iter 1580: loss 1.5501, time 343.46ms, mfu 0.82%\n",
            "iter 1590: loss 1.5825, time 317.37ms, mfu 0.82%\n",
            "step 1600: train loss 1.4527, val loss 1.6566\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1600: loss 1.5818, time 1663.08ms, mfu 0.75%\n",
            "iter 1610: loss 1.5894, time 331.69ms, mfu 0.76%\n",
            "iter 1620: loss 1.5216, time 323.19ms, mfu 0.77%\n",
            "iter 1630: loss 1.5923, time 335.23ms, mfu 0.77%\n",
            "iter 1640: loss 1.6122, time 324.19ms, mfu 0.78%\n",
            "iter 1650: loss 1.5522, time 328.44ms, mfu 0.78%\n",
            "iter 1660: loss 1.6001, time 325.83ms, mfu 0.79%\n",
            "iter 1670: loss 1.4293, time 330.49ms, mfu 0.79%\n",
            "iter 1680: loss 1.6100, time 322.88ms, mfu 0.79%\n",
            "iter 1690: loss 1.7340, time 325.37ms, mfu 0.80%\n",
            "iter 1700: loss 1.3768, time 327.55ms, mfu 0.80%\n",
            "iter 1710: loss 1.6069, time 317.51ms, mfu 0.81%\n",
            "iter 1720: loss 1.6081, time 317.90ms, mfu 0.81%\n",
            "iter 1730: loss 1.4827, time 327.48ms, mfu 0.81%\n",
            "iter 1740: loss 1.4180, time 319.43ms, mfu 0.81%\n",
            "iter 1750: loss 1.4420, time 326.47ms, mfu 0.82%\n",
            "iter 1760: loss 1.5670, time 322.36ms, mfu 0.82%\n",
            "iter 1770: loss 1.4941, time 319.56ms, mfu 0.82%\n",
            "iter 1780: loss 1.5083, time 318.33ms, mfu 0.82%\n",
            "iter 1790: loss 1.4579, time 324.47ms, mfu 0.82%\n",
            "step 1800: train loss 1.3982, val loss 1.6177\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1800: loss 1.5506, time 1681.55ms, mfu 0.76%\n",
            "iter 1810: loss 1.4916, time 317.75ms, mfu 0.77%\n",
            "iter 1820: loss 1.4573, time 315.67ms, mfu 0.78%\n",
            "iter 1830: loss 1.4845, time 317.47ms, mfu 0.78%\n",
            "iter 1840: loss 1.5139, time 322.62ms, mfu 0.79%\n",
            "iter 1850: loss 1.3712, time 322.96ms, mfu 0.79%\n",
            "iter 1860: loss 1.4291, time 319.00ms, mfu 0.80%\n",
            "iter 1870: loss 1.4965, time 335.02ms, mfu 0.80%\n",
            "iter 1880: loss 1.5026, time 332.70ms, mfu 0.80%\n",
            "iter 1890: loss 1.4069, time 323.78ms, mfu 0.80%\n",
            "iter 1900: loss 1.3700, time 314.07ms, mfu 0.81%\n",
            "iter 1910: loss 1.4092, time 322.21ms, mfu 0.81%\n",
            "iter 1920: loss 1.5162, time 320.87ms, mfu 0.82%\n",
            "iter 1930: loss 1.5560, time 325.97ms, mfu 0.82%\n",
            "iter 1940: loss 1.5133, time 320.73ms, mfu 0.82%\n",
            "iter 1950: loss 1.5496, time 344.38ms, mfu 0.82%\n",
            "iter 1960: loss 1.5500, time 321.46ms, mfu 0.82%\n",
            "iter 1970: loss 1.4131, time 324.85ms, mfu 0.82%\n",
            "iter 1980: loss 1.4928, time 327.92ms, mfu 0.82%\n",
            "iter 1990: loss 1.4303, time 322.39ms, mfu 0.82%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 12/32: b128_L4_H4_E256_BS8_MI2000_D20_s12 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS8_MI2000_D20_s12.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 12\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1810, val loss 4.1766\n",
            "iter 0: loss 4.2093, time 1925.75ms, mfu -100.00%\n",
            "iter 10: loss 4.1412, time 327.44ms, mfu 0.82%\n",
            "iter 20: loss 4.0299, time 322.77ms, mfu 0.83%\n",
            "iter 30: loss 3.8870, time 319.78ms, mfu 0.83%\n",
            "iter 40: loss 3.6718, time 340.63ms, mfu 0.82%\n",
            "iter 50: loss 3.6087, time 325.87ms, mfu 0.82%\n",
            "iter 60: loss 3.4950, time 327.96ms, mfu 0.82%\n",
            "iter 70: loss 3.4370, time 317.70ms, mfu 0.83%\n",
            "iter 80: loss 3.3567, time 318.85ms, mfu 0.83%\n",
            "iter 90: loss 3.2604, time 316.54ms, mfu 0.83%\n",
            "iter 100: loss 3.1909, time 330.79ms, mfu 0.83%\n",
            "iter 110: loss 3.1550, time 315.72ms, mfu 0.83%\n",
            "iter 120: loss 3.0485, time 329.71ms, mfu 0.83%\n",
            "iter 130: loss 3.0627, time 325.99ms, mfu 0.83%\n",
            "iter 140: loss 2.8897, time 313.53ms, mfu 0.83%\n",
            "iter 150: loss 2.8752, time 324.17ms, mfu 0.83%\n",
            "iter 160: loss 2.9594, time 323.42ms, mfu 0.83%\n",
            "iter 170: loss 2.8640, time 314.66ms, mfu 0.84%\n",
            "iter 180: loss 2.8728, time 316.28ms, mfu 0.84%\n",
            "iter 190: loss 2.8322, time 328.65ms, mfu 0.84%\n",
            "step 200: train loss 2.7184, val loss 2.7240\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 200: loss 2.7725, time 6280.12ms, mfu 0.76%\n",
            "iter 210: loss 2.7540, time 325.86ms, mfu 0.76%\n",
            "iter 220: loss 2.7353, time 321.14ms, mfu 0.77%\n",
            "iter 230: loss 2.7670, time 333.80ms, mfu 0.78%\n",
            "iter 240: loss 2.7740, time 323.61ms, mfu 0.78%\n",
            "iter 250: loss 2.7234, time 329.56ms, mfu 0.78%\n",
            "iter 260: loss 2.6353, time 320.60ms, mfu 0.79%\n",
            "iter 270: loss 2.6337, time 317.74ms, mfu 0.80%\n",
            "iter 280: loss 2.7060, time 320.15ms, mfu 0.80%\n",
            "iter 290: loss 2.6237, time 334.49ms, mfu 0.80%\n",
            "iter 300: loss 2.5893, time 315.90ms, mfu 0.81%\n",
            "iter 310: loss 2.6018, time 325.13ms, mfu 0.81%\n",
            "iter 320: loss 2.5712, time 333.27ms, mfu 0.81%\n",
            "iter 330: loss 2.5305, time 324.98ms, mfu 0.81%\n",
            "iter 340: loss 2.5409, time 325.46ms, mfu 0.81%\n",
            "iter 350: loss 2.5705, time 320.38ms, mfu 0.82%\n",
            "iter 360: loss 2.5219, time 335.36ms, mfu 0.82%\n",
            "iter 370: loss 2.6065, time 343.30ms, mfu 0.81%\n",
            "iter 380: loss 2.5245, time 332.01ms, mfu 0.81%\n",
            "iter 390: loss 2.4751, time 335.65ms, mfu 0.81%\n",
            "step 400: train loss 2.4649, val loss 2.4639\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 400: loss 2.5350, time 1678.89ms, mfu 0.75%\n",
            "iter 410: loss 2.4945, time 323.76ms, mfu 0.76%\n",
            "iter 420: loss 2.4441, time 334.71ms, mfu 0.76%\n",
            "iter 430: loss 2.4631, time 330.87ms, mfu 0.77%\n",
            "iter 440: loss 2.4944, time 344.65ms, mfu 0.77%\n",
            "iter 450: loss 2.5571, time 318.83ms, mfu 0.78%\n",
            "iter 460: loss 2.3583, time 327.07ms, mfu 0.78%\n",
            "iter 470: loss 2.4042, time 320.94ms, mfu 0.79%\n",
            "iter 480: loss 2.3569, time 317.36ms, mfu 0.79%\n",
            "iter 490: loss 2.3930, time 319.29ms, mfu 0.80%\n",
            "iter 500: loss 2.3712, time 317.56ms, mfu 0.80%\n",
            "iter 510: loss 2.3791, time 336.14ms, mfu 0.80%\n",
            "iter 520: loss 2.3246, time 317.45ms, mfu 0.81%\n",
            "iter 530: loss 2.3699, time 320.75ms, mfu 0.81%\n",
            "iter 540: loss 2.4182, time 327.22ms, mfu 0.81%\n",
            "iter 550: loss 2.3676, time 335.22ms, mfu 0.81%\n",
            "iter 560: loss 2.3554, time 335.77ms, mfu 0.81%\n",
            "iter 570: loss 2.3857, time 323.53ms, mfu 0.81%\n",
            "iter 580: loss 2.4593, time 320.95ms, mfu 0.82%\n",
            "iter 590: loss 2.3072, time 321.54ms, mfu 0.82%\n",
            "step 600: train loss 2.2666, val loss 2.2855\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 600: loss 2.3065, time 1657.70ms, mfu 0.75%\n",
            "iter 610: loss 2.3606, time 364.45ms, mfu 0.75%\n",
            "iter 620: loss 2.2853, time 333.36ms, mfu 0.76%\n",
            "iter 630: loss 2.2889, time 323.85ms, mfu 0.77%\n",
            "iter 640: loss 2.2750, time 324.43ms, mfu 0.77%\n",
            "iter 650: loss 2.3320, time 332.74ms, mfu 0.78%\n",
            "iter 660: loss 2.2290, time 320.98ms, mfu 0.78%\n",
            "iter 670: loss 2.3487, time 314.86ms, mfu 0.79%\n",
            "iter 680: loss 2.3234, time 314.14ms, mfu 0.80%\n",
            "iter 690: loss 2.2484, time 332.40ms, mfu 0.80%\n",
            "iter 700: loss 2.1944, time 326.80ms, mfu 0.80%\n",
            "iter 710: loss 2.2672, time 314.97ms, mfu 0.81%\n",
            "iter 720: loss 2.2103, time 327.75ms, mfu 0.81%\n",
            "iter 730: loss 2.3228, time 343.88ms, mfu 0.81%\n",
            "iter 740: loss 2.2812, time 391.88ms, mfu 0.79%\n",
            "iter 750: loss 2.2330, time 322.81ms, mfu 0.80%\n",
            "iter 760: loss 2.2015, time 325.80ms, mfu 0.80%\n",
            "iter 770: loss 2.1964, time 324.85ms, mfu 0.80%\n",
            "iter 780: loss 2.2453, time 322.40ms, mfu 0.81%\n",
            "iter 790: loss 2.1930, time 323.92ms, mfu 0.81%\n",
            "step 800: train loss 2.0655, val loss 2.1036\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 800: loss 2.1783, time 1673.93ms, mfu 0.75%\n",
            "iter 810: loss 2.2484, time 314.70ms, mfu 0.76%\n",
            "iter 820: loss 2.1000, time 322.30ms, mfu 0.76%\n",
            "iter 830: loss 2.1373, time 317.20ms, mfu 0.77%\n",
            "iter 840: loss 2.1387, time 323.92ms, mfu 0.78%\n",
            "iter 850: loss 2.1305, time 320.45ms, mfu 0.79%\n",
            "iter 860: loss 2.1387, time 317.70ms, mfu 0.79%\n",
            "iter 870: loss 2.1426, time 325.96ms, mfu 0.80%\n",
            "iter 880: loss 2.1678, time 321.09ms, mfu 0.80%\n",
            "iter 890: loss 2.0913, time 317.45ms, mfu 0.81%\n",
            "iter 900: loss 2.1408, time 338.78ms, mfu 0.80%\n",
            "iter 910: loss 2.0318, time 320.30ms, mfu 0.81%\n",
            "iter 920: loss 1.9821, time 321.30ms, mfu 0.81%\n",
            "iter 930: loss 2.0596, time 322.94ms, mfu 0.81%\n",
            "iter 940: loss 2.1268, time 319.72ms, mfu 0.82%\n",
            "iter 950: loss 1.9969, time 323.74ms, mfu 0.82%\n",
            "iter 960: loss 2.0097, time 319.63ms, mfu 0.82%\n",
            "iter 970: loss 2.0500, time 337.95ms, mfu 0.82%\n",
            "iter 980: loss 1.9329, time 320.39ms, mfu 0.82%\n",
            "iter 990: loss 2.0640, time 318.83ms, mfu 0.82%\n",
            "step 1000: train loss 1.8885, val loss 1.9842\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1000: loss 2.0466, time 1665.03ms, mfu 0.76%\n",
            "iter 1010: loss 2.1223, time 319.72ms, mfu 0.77%\n",
            "iter 1020: loss 2.0615, time 336.44ms, mfu 0.77%\n",
            "iter 1030: loss 1.9562, time 331.74ms, mfu 0.77%\n",
            "iter 1040: loss 2.1205, time 329.30ms, mfu 0.78%\n",
            "iter 1050: loss 1.9970, time 315.17ms, mfu 0.79%\n",
            "iter 1060: loss 2.0212, time 317.52ms, mfu 0.79%\n",
            "iter 1070: loss 1.9536, time 338.62ms, mfu 0.79%\n",
            "iter 1080: loss 1.9267, time 322.59ms, mfu 0.80%\n",
            "iter 1090: loss 1.9334, time 323.18ms, mfu 0.80%\n",
            "iter 1100: loss 2.0281, time 318.11ms, mfu 0.81%\n",
            "iter 1110: loss 1.9367, time 320.57ms, mfu 0.81%\n",
            "iter 1120: loss 1.8484, time 340.36ms, mfu 0.81%\n",
            "iter 1130: loss 1.9188, time 322.15ms, mfu 0.81%\n",
            "iter 1140: loss 1.8675, time 328.60ms, mfu 0.81%\n",
            "iter 1150: loss 2.0485, time 326.75ms, mfu 0.81%\n",
            "iter 1160: loss 1.8953, time 324.31ms, mfu 0.82%\n",
            "iter 1170: loss 1.8414, time 324.25ms, mfu 0.82%\n",
            "iter 1180: loss 1.8244, time 317.18ms, mfu 0.82%\n",
            "iter 1190: loss 1.8619, time 329.68ms, mfu 0.82%\n",
            "step 1200: train loss 1.7473, val loss 1.8956\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1200: loss 1.8273, time 1707.36ms, mfu 0.75%\n",
            "iter 1210: loss 1.9273, time 332.61ms, mfu 0.76%\n",
            "iter 1220: loss 1.8644, time 318.52ms, mfu 0.77%\n",
            "iter 1230: loss 1.8427, time 323.43ms, mfu 0.78%\n",
            "iter 1240: loss 1.7099, time 340.24ms, mfu 0.78%\n",
            "iter 1250: loss 1.8028, time 314.32ms, mfu 0.79%\n",
            "iter 1260: loss 1.8597, time 319.30ms, mfu 0.79%\n",
            "iter 1270: loss 1.8272, time 326.73ms, mfu 0.79%\n",
            "iter 1280: loss 1.8283, time 319.17ms, mfu 0.80%\n",
            "iter 1290: loss 1.8705, time 326.14ms, mfu 0.80%\n",
            "iter 1300: loss 1.7974, time 327.74ms, mfu 0.80%\n",
            "iter 1310: loss 1.8590, time 324.14ms, mfu 0.81%\n",
            "iter 1320: loss 1.7166, time 328.10ms, mfu 0.81%\n",
            "iter 1330: loss 1.8059, time 325.94ms, mfu 0.81%\n",
            "iter 1340: loss 1.6904, time 331.23ms, mfu 0.81%\n",
            "iter 1350: loss 1.7111, time 329.74ms, mfu 0.81%\n",
            "iter 1360: loss 1.8648, time 316.58ms, mfu 0.82%\n",
            "iter 1370: loss 1.8050, time 315.30ms, mfu 0.82%\n",
            "iter 1380: loss 1.7054, time 318.92ms, mfu 0.82%\n",
            "iter 1390: loss 1.7283, time 328.59ms, mfu 0.82%\n",
            "step 1400: train loss 1.6281, val loss 1.8005\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1400: loss 1.7815, time 1652.46ms, mfu 0.76%\n",
            "iter 1410: loss 1.7272, time 322.72ms, mfu 0.76%\n",
            "iter 1420: loss 1.7800, time 350.48ms, mfu 0.77%\n",
            "iter 1430: loss 1.7771, time 323.41ms, mfu 0.77%\n",
            "iter 1440: loss 1.6998, time 326.32ms, mfu 0.78%\n",
            "iter 1450: loss 1.8604, time 321.94ms, mfu 0.78%\n",
            "iter 1460: loss 1.5725, time 328.42ms, mfu 0.79%\n",
            "iter 1470: loss 1.6600, time 320.54ms, mfu 0.79%\n",
            "iter 1480: loss 1.6943, time 318.57ms, mfu 0.80%\n",
            "iter 1490: loss 1.6941, time 321.46ms, mfu 0.80%\n",
            "iter 1500: loss 1.6799, time 324.89ms, mfu 0.81%\n",
            "iter 1510: loss 1.6384, time 320.74ms, mfu 0.81%\n",
            "iter 1520: loss 1.7068, time 320.80ms, mfu 0.81%\n",
            "iter 1530: loss 1.7679, time 326.78ms, mfu 0.81%\n",
            "iter 1540: loss 1.7873, time 315.84ms, mfu 0.82%\n",
            "iter 1550: loss 1.7379, time 320.20ms, mfu 0.82%\n",
            "iter 1560: loss 1.8713, time 314.98ms, mfu 0.82%\n",
            "iter 1570: loss 1.6969, time 322.33ms, mfu 0.83%\n",
            "iter 1580: loss 1.6599, time 313.74ms, mfu 0.83%\n",
            "iter 1590: loss 1.6939, time 321.69ms, mfu 0.83%\n",
            "step 1600: train loss 1.5412, val loss 1.7456\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1600: loss 1.6760, time 1657.40ms, mfu 0.76%\n",
            "iter 1610: loss 1.7018, time 324.11ms, mfu 0.77%\n",
            "iter 1620: loss 1.6711, time 338.61ms, mfu 0.77%\n",
            "iter 1630: loss 1.7063, time 322.87ms, mfu 0.78%\n",
            "iter 1640: loss 1.7329, time 329.30ms, mfu 0.78%\n",
            "iter 1650: loss 1.6966, time 321.72ms, mfu 0.79%\n",
            "iter 1660: loss 1.7188, time 328.80ms, mfu 0.79%\n",
            "iter 1670: loss 1.5411, time 328.83ms, mfu 0.79%\n",
            "iter 1680: loss 1.7450, time 320.63ms, mfu 0.80%\n",
            "iter 1690: loss 1.8623, time 319.45ms, mfu 0.80%\n",
            "iter 1700: loss 1.5106, time 316.97ms, mfu 0.81%\n",
            "iter 1710: loss 1.7348, time 321.47ms, mfu 0.81%\n",
            "iter 1720: loss 1.7228, time 325.27ms, mfu 0.81%\n",
            "iter 1730: loss 1.5795, time 318.02ms, mfu 0.82%\n",
            "iter 1740: loss 1.5566, time 324.26ms, mfu 0.82%\n",
            "iter 1750: loss 1.5429, time 321.24ms, mfu 0.82%\n",
            "iter 1760: loss 1.6867, time 334.47ms, mfu 0.82%\n",
            "iter 1770: loss 1.6280, time 319.89ms, mfu 0.82%\n",
            "iter 1780: loss 1.6405, time 316.65ms, mfu 0.82%\n",
            "iter 1790: loss 1.5728, time 326.05ms, mfu 0.83%\n",
            "step 1800: train loss 1.4844, val loss 1.6937\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1800: loss 1.6583, time 1662.72ms, mfu 0.76%\n",
            "iter 1810: loss 1.5944, time 321.92ms, mfu 0.77%\n",
            "iter 1820: loss 1.5638, time 317.75ms, mfu 0.78%\n",
            "iter 1830: loss 1.5982, time 315.66ms, mfu 0.78%\n",
            "iter 1840: loss 1.6265, time 323.45ms, mfu 0.79%\n",
            "iter 1850: loss 1.5085, time 319.87ms, mfu 0.79%\n",
            "iter 1860: loss 1.5566, time 333.69ms, mfu 0.80%\n",
            "iter 1870: loss 1.6062, time 321.01ms, mfu 0.80%\n",
            "iter 1880: loss 1.6257, time 322.91ms, mfu 0.80%\n",
            "iter 1890: loss 1.5617, time 318.16ms, mfu 0.81%\n",
            "iter 1900: loss 1.4846, time 332.74ms, mfu 0.81%\n",
            "iter 1910: loss 1.5404, time 331.64ms, mfu 0.81%\n",
            "iter 1920: loss 1.6454, time 318.99ms, mfu 0.81%\n",
            "iter 1930: loss 1.6987, time 322.87ms, mfu 0.81%\n",
            "iter 1940: loss 1.6426, time 344.85ms, mfu 0.81%\n",
            "iter 1950: loss 1.6656, time 315.26ms, mfu 0.82%\n",
            "iter 1960: loss 1.7007, time 316.56ms, mfu 0.82%\n",
            "iter 1970: loss 1.5157, time 315.41ms, mfu 0.82%\n",
            "iter 1980: loss 1.5933, time 321.71ms, mfu 0.82%\n",
            "iter 1990: loss 1.5762, time 328.05ms, mfu 0.82%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 13/32: b128_L4_H4_E256_BS16_MI1000_D10_s13 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS16_MI1000_D10_s13.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 13\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1814, val loss 4.1757\n",
            "iter 0: loss 4.1873, time 2069.52ms, mfu -100.00%\n",
            "iter 10: loss 4.1275, time 340.09ms, mfu 1.59%\n",
            "iter 20: loss 4.0029, time 341.20ms, mfu 1.59%\n",
            "iter 30: loss 3.8243, time 349.57ms, mfu 1.58%\n",
            "iter 40: loss 3.6516, time 342.92ms, mfu 1.58%\n",
            "iter 50: loss 3.5346, time 332.43ms, mfu 1.59%\n",
            "iter 60: loss 3.4716, time 329.80ms, mfu 1.59%\n",
            "iter 70: loss 3.3809, time 339.55ms, mfu 1.59%\n",
            "iter 80: loss 3.2981, time 330.73ms, mfu 1.60%\n",
            "iter 90: loss 3.2085, time 330.86ms, mfu 1.60%\n",
            "iter 100: loss 3.1011, time 347.79ms, mfu 1.59%\n",
            "iter 110: loss 3.0043, time 335.99ms, mfu 1.60%\n",
            "iter 120: loss 3.0239, time 328.00ms, mfu 1.60%\n",
            "iter 130: loss 2.9556, time 331.32ms, mfu 1.60%\n",
            "iter 140: loss 2.8688, time 338.08ms, mfu 1.60%\n",
            "iter 150: loss 2.8154, time 339.00ms, mfu 1.60%\n",
            "iter 160: loss 2.8569, time 341.72ms, mfu 1.60%\n",
            "iter 170: loss 2.8285, time 334.78ms, mfu 1.60%\n",
            "iter 180: loss 2.7214, time 334.90ms, mfu 1.60%\n",
            "iter 190: loss 2.7213, time 332.27ms, mfu 1.60%\n",
            "step 200: train loss 2.7018, val loss 2.7071\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 200: loss 2.7330, time 5664.12ms, mfu 1.45%\n",
            "iter 210: loss 2.7469, time 331.69ms, mfu 1.47%\n",
            "iter 220: loss 2.7096, time 333.06ms, mfu 1.49%\n",
            "iter 230: loss 2.6528, time 345.51ms, mfu 1.49%\n",
            "iter 240: loss 2.6176, time 338.71ms, mfu 1.50%\n",
            "iter 250: loss 2.5979, time 336.41ms, mfu 1.51%\n",
            "iter 260: loss 2.6391, time 332.02ms, mfu 1.52%\n",
            "iter 270: loss 2.5930, time 333.85ms, mfu 1.53%\n",
            "iter 280: loss 2.5651, time 346.50ms, mfu 1.54%\n",
            "iter 290: loss 2.5274, time 334.30ms, mfu 1.54%\n",
            "iter 300: loss 2.5157, time 340.07ms, mfu 1.55%\n",
            "iter 310: loss 2.5624, time 333.31ms, mfu 1.56%\n",
            "iter 320: loss 2.5230, time 335.28ms, mfu 1.56%\n",
            "iter 330: loss 2.4733, time 334.19ms, mfu 1.57%\n",
            "iter 340: loss 2.5270, time 340.80ms, mfu 1.57%\n",
            "iter 350: loss 2.5204, time 331.90ms, mfu 1.57%\n",
            "iter 360: loss 2.4953, time 338.25ms, mfu 1.58%\n",
            "iter 370: loss 2.4510, time 339.98ms, mfu 1.58%\n",
            "iter 380: loss 2.4649, time 367.26ms, mfu 1.57%\n",
            "iter 390: loss 2.4509, time 339.37ms, mfu 1.57%\n",
            "step 400: train loss 2.3844, val loss 2.4000\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 400: loss 2.3994, time 1755.51ms, mfu 1.44%\n",
            "iter 410: loss 2.4387, time 328.01ms, mfu 1.46%\n",
            "iter 420: loss 2.3789, time 332.72ms, mfu 1.48%\n",
            "iter 430: loss 2.3467, time 334.17ms, mfu 1.49%\n",
            "iter 440: loss 2.4102, time 329.32ms, mfu 1.51%\n",
            "iter 450: loss 2.3325, time 329.21ms, mfu 1.52%\n",
            "iter 460: loss 2.3721, time 334.21ms, mfu 1.53%\n",
            "iter 470: loss 2.3358, time 328.81ms, mfu 1.54%\n",
            "iter 480: loss 2.3332, time 344.07ms, mfu 1.54%\n",
            "iter 490: loss 2.3200, time 333.84ms, mfu 1.55%\n",
            "iter 500: loss 2.2960, time 326.77ms, mfu 1.56%\n",
            "iter 510: loss 2.2759, time 330.33ms, mfu 1.57%\n",
            "iter 520: loss 2.3352, time 370.19ms, mfu 1.56%\n",
            "iter 530: loss 2.3428, time 333.36ms, mfu 1.56%\n",
            "iter 540: loss 2.2405, time 328.83ms, mfu 1.57%\n",
            "iter 550: loss 2.2814, time 337.12ms, mfu 1.57%\n",
            "iter 560: loss 2.2090, time 333.08ms, mfu 1.58%\n",
            "iter 570: loss 2.2480, time 333.05ms, mfu 1.58%\n",
            "iter 580: loss 2.1852, time 337.19ms, mfu 1.59%\n",
            "iter 590: loss 2.1890, time 328.48ms, mfu 1.59%\n",
            "step 600: train loss 2.1460, val loss 2.1766\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 600: loss 2.2850, time 1738.49ms, mfu 1.46%\n",
            "iter 610: loss 2.2543, time 327.83ms, mfu 1.48%\n",
            "iter 620: loss 2.2404, time 336.29ms, mfu 1.49%\n",
            "iter 630: loss 2.1448, time 329.00ms, mfu 1.51%\n",
            "iter 640: loss 2.1850, time 339.83ms, mfu 1.52%\n",
            "iter 650: loss 2.1655, time 331.02ms, mfu 1.53%\n",
            "iter 660: loss 2.1559, time 340.50ms, mfu 1.53%\n",
            "iter 670: loss 2.1605, time 338.45ms, mfu 1.54%\n",
            "iter 680: loss 2.1755, time 333.31ms, mfu 1.55%\n",
            "iter 690: loss 2.1208, time 346.64ms, mfu 1.55%\n",
            "iter 700: loss 2.0534, time 335.92ms, mfu 1.55%\n",
            "iter 710: loss 2.0169, time 331.20ms, mfu 1.56%\n",
            "iter 720: loss 2.0977, time 339.84ms, mfu 1.56%\n",
            "iter 730: loss 2.0952, time 353.32ms, mfu 1.56%\n",
            "iter 740: loss 2.0690, time 333.54ms, mfu 1.57%\n",
            "iter 750: loss 2.0666, time 332.96ms, mfu 1.57%\n",
            "iter 760: loss 2.0145, time 342.84ms, mfu 1.57%\n",
            "iter 770: loss 2.0229, time 337.26ms, mfu 1.58%\n",
            "iter 780: loss 2.0099, time 347.63ms, mfu 1.57%\n",
            "iter 790: loss 2.0663, time 342.63ms, mfu 1.57%\n",
            "step 800: train loss 1.8986, val loss 2.0004\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 800: loss 2.0266, time 1886.10ms, mfu 1.44%\n",
            "iter 810: loss 1.9480, time 342.49ms, mfu 1.46%\n",
            "iter 820: loss 2.0078, time 326.96ms, mfu 1.48%\n",
            "iter 830: loss 1.9494, time 330.25ms, mfu 1.49%\n",
            "iter 840: loss 1.9568, time 338.34ms, mfu 1.50%\n",
            "iter 850: loss 1.9371, time 327.22ms, mfu 1.52%\n",
            "iter 860: loss 1.9579, time 329.38ms, mfu 1.53%\n",
            "iter 870: loss 1.8960, time 338.86ms, mfu 1.54%\n",
            "iter 880: loss 1.9358, time 326.38ms, mfu 1.55%\n",
            "iter 890: loss 1.8936, time 331.94ms, mfu 1.56%\n",
            "iter 900: loss 1.9241, time 331.83ms, mfu 1.56%\n",
            "iter 910: loss 1.9135, time 334.93ms, mfu 1.57%\n",
            "iter 920: loss 1.8728, time 339.88ms, mfu 1.57%\n",
            "iter 930: loss 1.9126, time 337.91ms, mfu 1.57%\n",
            "iter 940: loss 1.8458, time 341.20ms, mfu 1.57%\n",
            "iter 950: loss 1.7796, time 335.14ms, mfu 1.58%\n",
            "iter 960: loss 1.8645, time 339.09ms, mfu 1.58%\n",
            "iter 970: loss 1.8596, time 334.69ms, mfu 1.58%\n",
            "iter 980: loss 1.7499, time 333.20ms, mfu 1.59%\n",
            "iter 990: loss 1.7796, time 325.30ms, mfu 1.59%\n",
            "step 1000: train loss 1.6984, val loss 1.8534\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 1000: loss 1.7638, time 1771.43ms, mfu 1.46%\n",
            "\n",
            "=== Experiment 14/32: b128_L4_H4_E256_BS16_MI1000_D20_s14 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS16_MI1000_D20_s14.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 14\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1814, val loss 4.1757\n",
            "iter 0: loss 4.1905, time 2086.34ms, mfu -100.00%\n",
            "iter 10: loss 4.1454, time 337.30ms, mfu 1.60%\n",
            "iter 20: loss 4.0386, time 330.07ms, mfu 1.60%\n",
            "iter 30: loss 3.8738, time 350.24ms, mfu 1.60%\n",
            "iter 40: loss 3.6998, time 335.73ms, mfu 1.60%\n",
            "iter 50: loss 3.5774, time 334.35ms, mfu 1.60%\n",
            "iter 60: loss 3.5133, time 345.80ms, mfu 1.60%\n",
            "iter 70: loss 3.4314, time 333.96ms, mfu 1.60%\n",
            "iter 80: loss 3.3612, time 338.94ms, mfu 1.60%\n",
            "iter 90: loss 3.2844, time 335.20ms, mfu 1.60%\n",
            "iter 100: loss 3.1788, time 341.02ms, mfu 1.60%\n",
            "iter 110: loss 3.0724, time 334.96ms, mfu 1.60%\n",
            "iter 120: loss 3.0923, time 335.01ms, mfu 1.60%\n",
            "iter 130: loss 3.0226, time 344.18ms, mfu 1.60%\n",
            "iter 140: loss 2.9281, time 340.43ms, mfu 1.60%\n",
            "iter 150: loss 2.8631, time 332.67ms, mfu 1.60%\n",
            "iter 160: loss 2.9115, time 341.96ms, mfu 1.60%\n",
            "iter 170: loss 2.8734, time 348.26ms, mfu 1.59%\n",
            "iter 180: loss 2.7599, time 337.87ms, mfu 1.59%\n",
            "iter 190: loss 2.7625, time 334.22ms, mfu 1.59%\n",
            "step 200: train loss 2.7175, val loss 2.7193\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 200: loss 2.7746, time 5715.49ms, mfu 1.44%\n",
            "iter 210: loss 2.7773, time 334.33ms, mfu 1.46%\n",
            "iter 220: loss 2.7443, time 341.58ms, mfu 1.47%\n",
            "iter 230: loss 2.6884, time 343.88ms, mfu 1.48%\n",
            "iter 240: loss 2.6664, time 333.98ms, mfu 1.50%\n",
            "iter 250: loss 2.6325, time 332.25ms, mfu 1.51%\n",
            "iter 260: loss 2.6711, time 351.28ms, mfu 1.51%\n",
            "iter 270: loss 2.6294, time 339.36ms, mfu 1.52%\n",
            "iter 280: loss 2.5984, time 335.63ms, mfu 1.53%\n",
            "iter 290: loss 2.5642, time 339.44ms, mfu 1.54%\n",
            "iter 300: loss 2.5531, time 374.04ms, mfu 1.53%\n",
            "iter 310: loss 2.5959, time 332.47ms, mfu 1.54%\n",
            "iter 320: loss 2.5446, time 331.41ms, mfu 1.54%\n",
            "iter 330: loss 2.5139, time 338.84ms, mfu 1.55%\n",
            "iter 340: loss 2.5591, time 346.70ms, mfu 1.55%\n",
            "iter 350: loss 2.5623, time 371.51ms, mfu 1.54%\n",
            "iter 360: loss 2.5311, time 346.92ms, mfu 1.54%\n",
            "iter 370: loss 2.4979, time 350.31ms, mfu 1.54%\n",
            "iter 380: loss 2.5048, time 337.13ms, mfu 1.55%\n",
            "iter 390: loss 2.4983, time 341.73ms, mfu 1.55%\n",
            "step 400: train loss 2.4304, val loss 2.4398\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 400: loss 2.4419, time 1813.57ms, mfu 1.43%\n",
            "iter 410: loss 2.4823, time 326.53ms, mfu 1.45%\n",
            "iter 420: loss 2.4268, time 334.37ms, mfu 1.47%\n",
            "iter 430: loss 2.4031, time 332.38ms, mfu 1.48%\n",
            "iter 440: loss 2.4531, time 334.10ms, mfu 1.49%\n",
            "iter 450: loss 2.3795, time 337.67ms, mfu 1.51%\n",
            "iter 460: loss 2.4328, time 336.30ms, mfu 1.52%\n",
            "iter 470: loss 2.4012, time 335.08ms, mfu 1.52%\n",
            "iter 480: loss 2.3879, time 339.24ms, mfu 1.53%\n",
            "iter 490: loss 2.3715, time 344.37ms, mfu 1.53%\n",
            "iter 500: loss 2.3502, time 332.03ms, mfu 1.54%\n",
            "iter 510: loss 2.3339, time 348.97ms, mfu 1.54%\n",
            "iter 520: loss 2.3945, time 331.92ms, mfu 1.55%\n",
            "iter 530: loss 2.3986, time 338.40ms, mfu 1.56%\n",
            "iter 540: loss 2.3056, time 329.36ms, mfu 1.57%\n",
            "iter 550: loss 2.3382, time 328.11ms, mfu 1.57%\n",
            "iter 560: loss 2.2851, time 347.24ms, mfu 1.57%\n",
            "iter 570: loss 2.3107, time 338.92ms, mfu 1.57%\n",
            "iter 580: loss 2.2536, time 348.78ms, mfu 1.57%\n",
            "iter 590: loss 2.2450, time 329.57ms, mfu 1.58%\n",
            "step 600: train loss 2.2063, val loss 2.2305\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 600: loss 2.3613, time 1807.24ms, mfu 1.45%\n",
            "iter 610: loss 2.3265, time 343.27ms, mfu 1.46%\n",
            "iter 620: loss 2.3182, time 333.18ms, mfu 1.48%\n",
            "iter 630: loss 2.2251, time 340.61ms, mfu 1.49%\n",
            "iter 640: loss 2.2652, time 336.25ms, mfu 1.50%\n",
            "iter 650: loss 2.2529, time 344.74ms, mfu 1.51%\n",
            "iter 660: loss 2.2138, time 344.64ms, mfu 1.51%\n",
            "iter 670: loss 2.2392, time 330.75ms, mfu 1.52%\n",
            "iter 680: loss 2.2786, time 364.40ms, mfu 1.52%\n",
            "iter 690: loss 2.2037, time 333.82ms, mfu 1.53%\n",
            "iter 700: loss 2.1453, time 334.10ms, mfu 1.54%\n",
            "iter 710: loss 2.1225, time 335.68ms, mfu 1.55%\n",
            "iter 720: loss 2.1845, time 338.89ms, mfu 1.55%\n",
            "iter 730: loss 2.2024, time 337.21ms, mfu 1.56%\n",
            "iter 740: loss 2.1549, time 365.95ms, mfu 1.55%\n",
            "iter 750: loss 2.1658, time 339.35ms, mfu 1.55%\n",
            "iter 760: loss 2.1152, time 377.92ms, mfu 1.54%\n",
            "iter 770: loss 2.1298, time 343.28ms, mfu 1.54%\n",
            "iter 780: loss 2.1095, time 342.80ms, mfu 1.55%\n",
            "iter 790: loss 2.1598, time 332.47ms, mfu 1.55%\n",
            "step 800: train loss 1.9745, val loss 2.0553\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 800: loss 2.1346, time 1782.41ms, mfu 1.43%\n",
            "iter 810: loss 2.0644, time 335.07ms, mfu 1.45%\n",
            "iter 820: loss 2.0981, time 343.67ms, mfu 1.46%\n",
            "iter 830: loss 2.0393, time 332.91ms, mfu 1.48%\n",
            "iter 840: loss 2.0682, time 337.45ms, mfu 1.49%\n",
            "iter 850: loss 2.0382, time 336.36ms, mfu 1.50%\n",
            "iter 860: loss 2.0697, time 335.90ms, mfu 1.51%\n",
            "iter 870: loss 2.0176, time 340.99ms, mfu 1.52%\n",
            "iter 880: loss 2.0624, time 328.40ms, mfu 1.53%\n",
            "iter 890: loss 2.0035, time 337.81ms, mfu 1.54%\n",
            "iter 900: loss 2.0346, time 336.95ms, mfu 1.54%\n",
            "iter 910: loss 2.0358, time 332.84ms, mfu 1.55%\n",
            "iter 920: loss 1.9763, time 328.70ms, mfu 1.56%\n",
            "iter 930: loss 2.0269, time 345.82ms, mfu 1.56%\n",
            "iter 940: loss 1.9712, time 334.66ms, mfu 1.57%\n",
            "iter 950: loss 1.9227, time 339.98ms, mfu 1.57%\n",
            "iter 960: loss 1.9939, time 329.79ms, mfu 1.57%\n",
            "iter 970: loss 1.9744, time 336.05ms, mfu 1.58%\n",
            "iter 980: loss 1.8710, time 336.00ms, mfu 1.58%\n",
            "iter 990: loss 1.9030, time 329.73ms, mfu 1.59%\n",
            "step 1000: train loss 1.7979, val loss 1.9337\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 1000: loss 1.8894, time 1823.58ms, mfu 1.46%\n",
            "\n",
            "=== Experiment 15/32: b128_L4_H4_E256_BS16_MI2000_D10_s15 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS16_MI2000_D10_s15.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 15\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1814, val loss 4.1757\n",
            "iter 0: loss 4.1873, time 2077.43ms, mfu -100.00%\n",
            "iter 10: loss 4.1275, time 333.43ms, mfu 1.62%\n",
            "iter 20: loss 4.0029, time 341.01ms, mfu 1.62%\n",
            "iter 30: loss 3.8243, time 340.26ms, mfu 1.61%\n",
            "iter 40: loss 3.6516, time 334.24ms, mfu 1.61%\n",
            "iter 50: loss 3.5346, time 348.82ms, mfu 1.61%\n",
            "iter 60: loss 3.4716, time 333.00ms, mfu 1.61%\n",
            "iter 70: loss 3.3809, time 335.76ms, mfu 1.61%\n",
            "iter 80: loss 3.2981, time 335.11ms, mfu 1.61%\n",
            "iter 90: loss 3.2085, time 344.25ms, mfu 1.60%\n",
            "iter 100: loss 3.1011, time 336.72ms, mfu 1.60%\n",
            "iter 110: loss 3.0043, time 331.30ms, mfu 1.61%\n",
            "iter 120: loss 3.0239, time 344.91ms, mfu 1.60%\n",
            "iter 130: loss 2.9556, time 345.16ms, mfu 1.60%\n",
            "iter 140: loss 2.8688, time 354.62ms, mfu 1.59%\n",
            "iter 150: loss 2.8154, time 342.89ms, mfu 1.59%\n",
            "iter 160: loss 2.8569, time 349.39ms, mfu 1.58%\n",
            "iter 170: loss 2.8285, time 343.75ms, mfu 1.58%\n",
            "iter 180: loss 2.7214, time 332.51ms, mfu 1.59%\n",
            "iter 190: loss 2.7213, time 349.04ms, mfu 1.58%\n",
            "step 200: train loss 2.7018, val loss 2.7071\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 200: loss 2.7330, time 6101.62ms, mfu 1.43%\n",
            "iter 210: loss 2.7469, time 342.32ms, mfu 1.45%\n",
            "iter 220: loss 2.7096, time 345.73ms, mfu 1.46%\n",
            "iter 230: loss 2.6528, time 352.35ms, mfu 1.47%\n",
            "iter 240: loss 2.6176, time 338.67ms, mfu 1.48%\n",
            "iter 250: loss 2.5979, time 342.79ms, mfu 1.49%\n",
            "iter 260: loss 2.6391, time 350.65ms, mfu 1.49%\n",
            "iter 270: loss 2.5930, time 342.58ms, mfu 1.50%\n",
            "iter 280: loss 2.5651, time 338.70ms, mfu 1.51%\n",
            "iter 290: loss 2.5274, time 346.03ms, mfu 1.52%\n",
            "iter 300: loss 2.5157, time 331.34ms, mfu 1.53%\n",
            "iter 310: loss 2.5624, time 343.18ms, mfu 1.53%\n",
            "iter 320: loss 2.5230, time 366.01ms, mfu 1.53%\n",
            "iter 330: loss 2.4733, time 347.20ms, mfu 1.53%\n",
            "iter 340: loss 2.5270, time 346.58ms, mfu 1.53%\n",
            "iter 350: loss 2.5204, time 342.45ms, mfu 1.54%\n",
            "iter 360: loss 2.4953, time 339.31ms, mfu 1.54%\n",
            "iter 370: loss 2.4510, time 333.33ms, mfu 1.55%\n",
            "iter 380: loss 2.4649, time 340.92ms, mfu 1.55%\n",
            "iter 390: loss 2.4509, time 354.74ms, mfu 1.55%\n",
            "step 400: train loss 2.3844, val loss 2.4000\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 400: loss 2.3994, time 1785.57ms, mfu 1.43%\n",
            "iter 410: loss 2.4387, time 337.93ms, mfu 1.44%\n",
            "iter 420: loss 2.3789, time 338.39ms, mfu 1.46%\n",
            "iter 430: loss 2.3467, time 337.18ms, mfu 1.47%\n",
            "iter 440: loss 2.4102, time 346.65ms, mfu 1.48%\n",
            "iter 450: loss 2.3325, time 342.39ms, mfu 1.49%\n",
            "iter 460: loss 2.3721, time 340.70ms, mfu 1.50%\n",
            "iter 470: loss 2.3358, time 359.29ms, mfu 1.50%\n",
            "iter 480: loss 2.3332, time 337.69ms, mfu 1.51%\n",
            "iter 490: loss 2.3200, time 345.44ms, mfu 1.52%\n",
            "iter 500: loss 2.2960, time 345.72ms, mfu 1.52%\n",
            "iter 510: loss 2.2759, time 341.67ms, mfu 1.53%\n",
            "iter 520: loss 2.3352, time 340.15ms, mfu 1.53%\n",
            "iter 530: loss 2.3428, time 336.36ms, mfu 1.54%\n",
            "iter 540: loss 2.2405, time 337.13ms, mfu 1.55%\n",
            "iter 550: loss 2.2814, time 330.30ms, mfu 1.55%\n",
            "iter 560: loss 2.2090, time 339.18ms, mfu 1.56%\n",
            "iter 570: loss 2.2480, time 350.80ms, mfu 1.56%\n",
            "iter 580: loss 2.1852, time 338.84ms, mfu 1.56%\n",
            "iter 590: loss 2.1890, time 330.13ms, mfu 1.57%\n",
            "step 600: train loss 2.1460, val loss 2.1766\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 600: loss 2.2850, time 1844.68ms, mfu 1.44%\n",
            "iter 610: loss 2.2543, time 331.60ms, mfu 1.46%\n",
            "iter 620: loss 2.2404, time 335.83ms, mfu 1.47%\n",
            "iter 630: loss 2.1448, time 356.27ms, mfu 1.48%\n",
            "iter 640: loss 2.1850, time 347.57ms, mfu 1.49%\n",
            "iter 650: loss 2.1655, time 334.27ms, mfu 1.50%\n",
            "iter 660: loss 2.1559, time 332.09ms, mfu 1.51%\n",
            "iter 670: loss 2.1605, time 338.57ms, mfu 1.52%\n",
            "iter 680: loss 2.1755, time 333.38ms, mfu 1.53%\n",
            "iter 690: loss 2.1208, time 344.26ms, mfu 1.53%\n",
            "iter 700: loss 2.0534, time 346.57ms, mfu 1.54%\n",
            "iter 710: loss 2.0169, time 336.90ms, mfu 1.54%\n",
            "iter 720: loss 2.0977, time 334.42ms, mfu 1.55%\n",
            "iter 730: loss 2.0952, time 336.67ms, mfu 1.55%\n",
            "iter 740: loss 2.0690, time 332.44ms, mfu 1.56%\n",
            "iter 750: loss 2.0666, time 340.00ms, mfu 1.56%\n",
            "iter 760: loss 2.0145, time 340.63ms, mfu 1.57%\n",
            "iter 770: loss 2.0229, time 349.22ms, mfu 1.56%\n",
            "iter 780: loss 2.0099, time 340.98ms, mfu 1.57%\n",
            "iter 790: loss 2.0663, time 341.94ms, mfu 1.57%\n",
            "step 800: train loss 1.8986, val loss 2.0004\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 800: loss 2.0266, time 1818.81ms, mfu 1.44%\n",
            "iter 810: loss 1.9480, time 337.29ms, mfu 1.46%\n",
            "iter 820: loss 2.0078, time 332.31ms, mfu 1.47%\n",
            "iter 830: loss 1.9494, time 345.61ms, mfu 1.48%\n",
            "iter 840: loss 1.9568, time 344.79ms, mfu 1.49%\n",
            "iter 850: loss 1.9371, time 348.51ms, mfu 1.50%\n",
            "iter 860: loss 1.9579, time 346.32ms, mfu 1.50%\n",
            "iter 870: loss 1.8960, time 344.99ms, mfu 1.51%\n",
            "iter 880: loss 1.9358, time 338.24ms, mfu 1.52%\n",
            "iter 890: loss 1.8936, time 345.86ms, mfu 1.52%\n",
            "iter 900: loss 1.9241, time 335.40ms, mfu 1.53%\n",
            "iter 910: loss 1.9135, time 337.77ms, mfu 1.54%\n",
            "iter 920: loss 1.8728, time 346.52ms, mfu 1.54%\n",
            "iter 930: loss 1.9126, time 331.06ms, mfu 1.55%\n",
            "iter 940: loss 1.8458, time 336.50ms, mfu 1.55%\n",
            "iter 950: loss 1.7796, time 337.12ms, mfu 1.56%\n",
            "iter 960: loss 1.8645, time 335.27ms, mfu 1.56%\n",
            "iter 970: loss 1.8596, time 337.88ms, mfu 1.57%\n",
            "iter 980: loss 1.7499, time 335.39ms, mfu 1.57%\n",
            "iter 990: loss 1.7796, time 340.94ms, mfu 1.57%\n",
            "step 1000: train loss 1.6984, val loss 1.8534\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1000: loss 1.7638, time 1794.03ms, mfu 1.45%\n",
            "iter 1010: loss 1.8846, time 332.16ms, mfu 1.46%\n",
            "iter 1020: loss 1.7975, time 333.02ms, mfu 1.48%\n",
            "iter 1030: loss 1.8201, time 340.30ms, mfu 1.49%\n",
            "iter 1040: loss 1.7005, time 329.68ms, mfu 1.50%\n",
            "iter 1050: loss 1.8481, time 343.18ms, mfu 1.51%\n",
            "iter 1060: loss 1.7283, time 335.14ms, mfu 1.52%\n",
            "iter 1070: loss 1.8629, time 333.84ms, mfu 1.53%\n",
            "iter 1080: loss 1.7558, time 335.99ms, mfu 1.54%\n",
            "iter 1090: loss 1.7095, time 336.05ms, mfu 1.55%\n",
            "iter 1100: loss 1.7959, time 339.45ms, mfu 1.55%\n",
            "iter 1110: loss 1.7348, time 357.10ms, mfu 1.55%\n",
            "iter 1120: loss 1.6725, time 352.91ms, mfu 1.54%\n",
            "iter 1130: loss 1.7336, time 350.38ms, mfu 1.54%\n",
            "iter 1140: loss 1.7003, time 335.54ms, mfu 1.55%\n",
            "iter 1150: loss 1.7612, time 349.22ms, mfu 1.55%\n",
            "iter 1160: loss 1.6752, time 336.33ms, mfu 1.56%\n",
            "iter 1170: loss 1.6726, time 338.33ms, mfu 1.56%\n",
            "iter 1180: loss 1.7018, time 342.33ms, mfu 1.56%\n",
            "iter 1190: loss 1.7430, time 354.85ms, mfu 1.56%\n",
            "step 1200: train loss 1.5588, val loss 1.7418\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1200: loss 1.6270, time 1787.73ms, mfu 1.43%\n",
            "iter 1210: loss 1.6693, time 348.49ms, mfu 1.44%\n",
            "iter 1220: loss 1.6580, time 363.22ms, mfu 1.45%\n",
            "iter 1230: loss 1.7235, time 339.22ms, mfu 1.46%\n",
            "iter 1240: loss 1.6191, time 345.17ms, mfu 1.47%\n",
            "iter 1250: loss 1.6924, time 344.66ms, mfu 1.48%\n",
            "iter 1260: loss 1.5535, time 344.75ms, mfu 1.49%\n",
            "iter 1270: loss 1.5724, time 333.67ms, mfu 1.50%\n",
            "iter 1280: loss 1.5683, time 337.39ms, mfu 1.51%\n",
            "iter 1290: loss 1.5442, time 340.69ms, mfu 1.52%\n",
            "iter 1300: loss 1.6411, time 343.73ms, mfu 1.52%\n",
            "iter 1310: loss 1.5757, time 344.32ms, mfu 1.53%\n",
            "iter 1320: loss 1.6300, time 346.17ms, mfu 1.53%\n",
            "iter 1330: loss 1.5914, time 336.01ms, mfu 1.54%\n",
            "iter 1340: loss 1.5620, time 341.21ms, mfu 1.54%\n",
            "iter 1350: loss 1.6044, time 333.57ms, mfu 1.55%\n",
            "iter 1360: loss 1.5980, time 338.09ms, mfu 1.56%\n",
            "iter 1370: loss 1.5160, time 345.72ms, mfu 1.56%\n",
            "iter 1380: loss 1.6032, time 337.20ms, mfu 1.56%\n",
            "iter 1390: loss 1.5568, time 343.86ms, mfu 1.56%\n",
            "step 1400: train loss 1.4649, val loss 1.6526\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1400: loss 1.5454, time 1831.63ms, mfu 1.44%\n",
            "iter 1410: loss 1.4775, time 343.43ms, mfu 1.45%\n",
            "iter 1420: loss 1.6385, time 335.72ms, mfu 1.46%\n",
            "iter 1430: loss 1.6065, time 341.34ms, mfu 1.48%\n",
            "iter 1440: loss 1.5528, time 331.42ms, mfu 1.49%\n",
            "iter 1450: loss 1.5212, time 359.34ms, mfu 1.49%\n",
            "iter 1460: loss 1.5171, time 335.54ms, mfu 1.50%\n",
            "iter 1470: loss 1.5156, time 348.20ms, mfu 1.51%\n",
            "iter 1480: loss 1.4988, time 343.88ms, mfu 1.52%\n",
            "iter 1490: loss 1.5672, time 339.82ms, mfu 1.52%\n",
            "iter 1500: loss 1.5777, time 346.32ms, mfu 1.53%\n",
            "iter 1510: loss 1.4870, time 333.66ms, mfu 1.54%\n",
            "iter 1520: loss 1.5397, time 339.09ms, mfu 1.54%\n",
            "iter 1530: loss 1.4772, time 336.08ms, mfu 1.55%\n",
            "iter 1540: loss 1.4691, time 347.18ms, mfu 1.55%\n",
            "iter 1550: loss 1.5625, time 340.85ms, mfu 1.55%\n",
            "iter 1560: loss 1.3921, time 335.99ms, mfu 1.56%\n",
            "iter 1570: loss 1.5568, time 338.55ms, mfu 1.56%\n",
            "iter 1580: loss 1.5139, time 336.31ms, mfu 1.57%\n",
            "iter 1590: loss 1.4050, time 336.82ms, mfu 1.57%\n",
            "step 1600: train loss 1.3999, val loss 1.5981\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1600: loss 1.5968, time 1802.42ms, mfu 1.44%\n",
            "iter 1610: loss 1.4903, time 343.17ms, mfu 1.46%\n",
            "iter 1620: loss 1.4681, time 346.80ms, mfu 1.47%\n",
            "iter 1630: loss 1.4629, time 348.38ms, mfu 1.47%\n",
            "iter 1640: loss 1.4906, time 348.50ms, mfu 1.48%\n",
            "iter 1650: loss 1.4149, time 339.57ms, mfu 1.49%\n",
            "iter 1660: loss 1.3871, time 333.44ms, mfu 1.50%\n",
            "iter 1670: loss 1.4636, time 335.12ms, mfu 1.52%\n",
            "iter 1680: loss 1.4219, time 348.62ms, mfu 1.52%\n",
            "iter 1690: loss 1.3858, time 332.83ms, mfu 1.53%\n",
            "iter 1700: loss 1.4758, time 334.92ms, mfu 1.54%\n",
            "iter 1710: loss 1.5068, time 332.04ms, mfu 1.55%\n",
            "iter 1720: loss 1.4930, time 342.77ms, mfu 1.55%\n",
            "iter 1730: loss 1.3915, time 353.99ms, mfu 1.55%\n",
            "iter 1740: loss 1.4687, time 335.73ms, mfu 1.55%\n",
            "iter 1750: loss 1.4165, time 333.85ms, mfu 1.56%\n",
            "iter 1760: loss 1.5804, time 340.89ms, mfu 1.56%\n",
            "iter 1770: loss 1.4363, time 344.62ms, mfu 1.56%\n",
            "iter 1780: loss 1.3796, time 343.87ms, mfu 1.56%\n",
            "iter 1790: loss 1.4342, time 338.14ms, mfu 1.57%\n",
            "step 1800: train loss 1.3453, val loss 1.5623\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1800: loss 1.4622, time 1797.22ms, mfu 1.44%\n",
            "iter 1810: loss 1.3567, time 336.88ms, mfu 1.46%\n",
            "iter 1820: loss 1.4360, time 340.50ms, mfu 1.47%\n",
            "iter 1830: loss 1.4946, time 336.78ms, mfu 1.48%\n",
            "iter 1840: loss 1.4858, time 335.66ms, mfu 1.49%\n",
            "iter 1850: loss 1.4371, time 336.53ms, mfu 1.51%\n",
            "iter 1860: loss 1.3914, time 338.34ms, mfu 1.51%\n",
            "iter 1870: loss 1.3480, time 340.69ms, mfu 1.52%\n",
            "iter 1880: loss 1.3190, time 328.77ms, mfu 1.53%\n",
            "iter 1890: loss 1.4394, time 356.82ms, mfu 1.53%\n",
            "iter 1900: loss 1.4140, time 353.52ms, mfu 1.53%\n",
            "iter 1910: loss 1.3887, time 344.64ms, mfu 1.53%\n",
            "iter 1920: loss 1.3812, time 356.47ms, mfu 1.53%\n",
            "iter 1930: loss 1.4580, time 340.23ms, mfu 1.54%\n",
            "iter 1940: loss 1.4165, time 346.45ms, mfu 1.54%\n",
            "iter 1950: loss 1.3534, time 336.17ms, mfu 1.55%\n",
            "iter 1960: loss 1.3785, time 363.42ms, mfu 1.54%\n",
            "iter 1970: loss 1.3737, time 335.44ms, mfu 1.55%\n",
            "iter 1980: loss 1.5028, time 337.10ms, mfu 1.55%\n",
            "iter 1990: loss 1.3471, time 346.27ms, mfu 1.55%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 16/32: b128_L4_H4_E256_BS16_MI2000_D20_s16 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H4_E256_BS16_MI2000_D20_s16.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 16\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1814, val loss 4.1757\n",
            "iter 0: loss 4.1905, time 2087.82ms, mfu -100.00%\n",
            "iter 10: loss 4.1454, time 353.07ms, mfu 1.53%\n",
            "iter 20: loss 4.0386, time 336.92ms, mfu 1.54%\n",
            "iter 30: loss 3.8738, time 343.48ms, mfu 1.54%\n",
            "iter 40: loss 3.6998, time 334.49ms, mfu 1.55%\n",
            "iter 50: loss 3.5774, time 353.75ms, mfu 1.55%\n",
            "iter 60: loss 3.5133, time 336.57ms, mfu 1.55%\n",
            "iter 70: loss 3.4314, time 337.47ms, mfu 1.56%\n",
            "iter 80: loss 3.3612, time 346.69ms, mfu 1.56%\n",
            "iter 90: loss 3.2844, time 334.26ms, mfu 1.56%\n",
            "iter 100: loss 3.1788, time 349.13ms, mfu 1.56%\n",
            "iter 110: loss 3.0724, time 338.94ms, mfu 1.56%\n",
            "iter 120: loss 3.0923, time 359.43ms, mfu 1.56%\n",
            "iter 130: loss 3.0226, time 347.12ms, mfu 1.56%\n",
            "iter 140: loss 2.9281, time 346.05ms, mfu 1.56%\n",
            "iter 150: loss 2.8631, time 356.81ms, mfu 1.55%\n",
            "iter 160: loss 2.9115, time 342.44ms, mfu 1.56%\n",
            "iter 170: loss 2.8734, time 354.33ms, mfu 1.55%\n",
            "iter 180: loss 2.7599, time 353.93ms, mfu 1.55%\n",
            "iter 190: loss 2.7625, time 353.71ms, mfu 1.55%\n",
            "step 200: train loss 2.7175, val loss 2.7193\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 200: loss 2.7746, time 5980.54ms, mfu 1.40%\n",
            "iter 210: loss 2.7773, time 361.65ms, mfu 1.41%\n",
            "iter 220: loss 2.7443, time 343.61ms, mfu 1.43%\n",
            "iter 230: loss 2.6884, time 343.86ms, mfu 1.44%\n",
            "iter 240: loss 2.6664, time 345.01ms, mfu 1.45%\n",
            "iter 250: loss 2.6325, time 342.14ms, mfu 1.47%\n",
            "iter 260: loss 2.6711, time 346.66ms, mfu 1.47%\n",
            "iter 270: loss 2.6294, time 338.21ms, mfu 1.49%\n",
            "iter 280: loss 2.5984, time 341.92ms, mfu 1.50%\n",
            "iter 290: loss 2.5642, time 340.47ms, mfu 1.51%\n",
            "iter 300: loss 2.5531, time 345.94ms, mfu 1.51%\n",
            "iter 310: loss 2.5959, time 351.98ms, mfu 1.51%\n",
            "iter 320: loss 2.5446, time 339.52ms, mfu 1.52%\n",
            "iter 330: loss 2.5139, time 341.51ms, mfu 1.53%\n",
            "iter 340: loss 2.5591, time 341.16ms, mfu 1.53%\n",
            "iter 350: loss 2.5623, time 345.40ms, mfu 1.54%\n",
            "iter 360: loss 2.5311, time 346.71ms, mfu 1.54%\n",
            "iter 370: loss 2.4979, time 344.35ms, mfu 1.54%\n",
            "iter 380: loss 2.5048, time 342.99ms, mfu 1.54%\n",
            "iter 390: loss 2.4983, time 339.25ms, mfu 1.55%\n",
            "step 400: train loss 2.4304, val loss 2.4398\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 400: loss 2.4419, time 1809.40ms, mfu 1.42%\n",
            "iter 410: loss 2.4823, time 343.57ms, mfu 1.44%\n",
            "iter 420: loss 2.4268, time 338.41ms, mfu 1.45%\n",
            "iter 430: loss 2.4031, time 334.86ms, mfu 1.47%\n",
            "iter 440: loss 2.4531, time 336.43ms, mfu 1.48%\n",
            "iter 450: loss 2.3795, time 339.61ms, mfu 1.49%\n",
            "iter 460: loss 2.4328, time 338.45ms, mfu 1.50%\n",
            "iter 470: loss 2.4012, time 334.62ms, mfu 1.52%\n",
            "iter 480: loss 2.3879, time 338.10ms, mfu 1.52%\n",
            "iter 490: loss 2.3715, time 351.46ms, mfu 1.52%\n",
            "iter 500: loss 2.3502, time 331.91ms, mfu 1.53%\n",
            "iter 510: loss 2.3339, time 335.03ms, mfu 1.54%\n",
            "iter 520: loss 2.3945, time 344.81ms, mfu 1.54%\n",
            "iter 530: loss 2.3986, time 346.73ms, mfu 1.55%\n",
            "iter 540: loss 2.3056, time 332.44ms, mfu 1.55%\n",
            "iter 550: loss 2.3382, time 344.41ms, mfu 1.56%\n",
            "iter 560: loss 2.2851, time 360.89ms, mfu 1.55%\n",
            "iter 570: loss 2.3107, time 343.13ms, mfu 1.55%\n",
            "iter 580: loss 2.2536, time 336.99ms, mfu 1.56%\n",
            "iter 590: loss 2.2450, time 342.39ms, mfu 1.56%\n",
            "step 600: train loss 2.2063, val loss 2.2305\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 600: loss 2.3613, time 1812.91ms, mfu 1.43%\n",
            "iter 610: loss 2.3265, time 338.48ms, mfu 1.45%\n",
            "iter 620: loss 2.3182, time 338.82ms, mfu 1.46%\n",
            "iter 630: loss 2.2251, time 338.00ms, mfu 1.48%\n",
            "iter 640: loss 2.2652, time 344.34ms, mfu 1.49%\n",
            "iter 650: loss 2.2529, time 353.08ms, mfu 1.49%\n",
            "iter 660: loss 2.2138, time 349.74ms, mfu 1.50%\n",
            "iter 670: loss 2.2392, time 348.85ms, mfu 1.50%\n",
            "iter 680: loss 2.2786, time 347.41ms, mfu 1.51%\n",
            "iter 690: loss 2.2037, time 345.01ms, mfu 1.51%\n",
            "iter 700: loss 2.1453, time 348.29ms, mfu 1.52%\n",
            "iter 710: loss 2.1225, time 338.92ms, mfu 1.52%\n",
            "iter 720: loss 2.1845, time 339.57ms, mfu 1.53%\n",
            "iter 730: loss 2.2024, time 347.05ms, mfu 1.53%\n",
            "iter 740: loss 2.1549, time 337.26ms, mfu 1.54%\n",
            "iter 750: loss 2.1658, time 336.78ms, mfu 1.55%\n",
            "iter 760: loss 2.1152, time 339.16ms, mfu 1.55%\n",
            "iter 770: loss 2.1298, time 345.14ms, mfu 1.55%\n",
            "iter 780: loss 2.1095, time 341.37ms, mfu 1.55%\n",
            "iter 790: loss 2.1598, time 335.53ms, mfu 1.56%\n",
            "step 800: train loss 1.9745, val loss 2.0553\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 800: loss 2.1346, time 1818.53ms, mfu 1.43%\n",
            "iter 810: loss 2.0644, time 335.20ms, mfu 1.45%\n",
            "iter 820: loss 2.0981, time 336.52ms, mfu 1.47%\n",
            "iter 830: loss 2.0393, time 334.80ms, mfu 1.48%\n",
            "iter 840: loss 2.0682, time 350.70ms, mfu 1.49%\n",
            "iter 850: loss 2.0382, time 342.15ms, mfu 1.50%\n",
            "iter 860: loss 2.0697, time 344.81ms, mfu 1.50%\n",
            "iter 870: loss 2.0176, time 353.59ms, mfu 1.51%\n",
            "iter 880: loss 2.0624, time 338.42ms, mfu 1.51%\n",
            "iter 890: loss 2.0035, time 346.75ms, mfu 1.52%\n",
            "iter 900: loss 2.0346, time 338.38ms, mfu 1.53%\n",
            "iter 910: loss 2.0358, time 338.77ms, mfu 1.53%\n",
            "iter 920: loss 1.9763, time 339.31ms, mfu 1.54%\n",
            "iter 930: loss 2.0269, time 332.75ms, mfu 1.55%\n",
            "iter 940: loss 1.9712, time 354.68ms, mfu 1.54%\n",
            "iter 950: loss 1.9227, time 338.44ms, mfu 1.55%\n",
            "iter 960: loss 1.9939, time 348.17ms, mfu 1.55%\n",
            "iter 970: loss 1.9744, time 348.74ms, mfu 1.55%\n",
            "iter 980: loss 1.8710, time 341.40ms, mfu 1.55%\n",
            "iter 990: loss 1.9030, time 333.92ms, mfu 1.56%\n",
            "step 1000: train loss 1.7979, val loss 1.9337\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1000: loss 1.8894, time 1804.34ms, mfu 1.43%\n",
            "iter 1010: loss 2.0143, time 343.16ms, mfu 1.45%\n",
            "iter 1020: loss 1.9254, time 334.50ms, mfu 1.46%\n",
            "iter 1030: loss 1.9282, time 337.50ms, mfu 1.48%\n",
            "iter 1040: loss 1.8261, time 345.50ms, mfu 1.49%\n",
            "iter 1050: loss 1.9744, time 333.33ms, mfu 1.50%\n",
            "iter 1060: loss 1.8482, time 338.62ms, mfu 1.51%\n",
            "iter 1070: loss 1.9533, time 348.14ms, mfu 1.51%\n",
            "iter 1080: loss 1.8801, time 341.29ms, mfu 1.52%\n",
            "iter 1090: loss 1.8339, time 348.14ms, mfu 1.52%\n",
            "iter 1100: loss 1.9295, time 342.27ms, mfu 1.53%\n",
            "iter 1110: loss 1.8435, time 357.80ms, mfu 1.53%\n",
            "iter 1120: loss 1.7769, time 336.45ms, mfu 1.53%\n",
            "iter 1130: loss 1.8608, time 342.32ms, mfu 1.54%\n",
            "iter 1140: loss 1.8102, time 337.13ms, mfu 1.54%\n",
            "iter 1150: loss 1.8709, time 336.71ms, mfu 1.55%\n",
            "iter 1160: loss 1.7939, time 340.71ms, mfu 1.55%\n",
            "iter 1170: loss 1.7880, time 333.11ms, mfu 1.56%\n",
            "iter 1180: loss 1.8231, time 335.28ms, mfu 1.57%\n",
            "iter 1190: loss 1.8713, time 345.15ms, mfu 1.57%\n",
            "step 1200: train loss 1.6521, val loss 1.8274\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1200: loss 1.7392, time 1785.72ms, mfu 1.44%\n",
            "iter 1210: loss 1.8011, time 353.21ms, mfu 1.45%\n",
            "iter 1220: loss 1.7826, time 343.69ms, mfu 1.46%\n",
            "iter 1230: loss 1.8585, time 340.71ms, mfu 1.47%\n",
            "iter 1240: loss 1.7317, time 349.65ms, mfu 1.48%\n",
            "iter 1250: loss 1.8239, time 360.00ms, mfu 1.48%\n",
            "iter 1260: loss 1.6952, time 339.27ms, mfu 1.49%\n",
            "iter 1270: loss 1.6920, time 351.31ms, mfu 1.50%\n",
            "iter 1280: loss 1.6855, time 356.01ms, mfu 1.50%\n",
            "iter 1290: loss 1.6678, time 356.39ms, mfu 1.50%\n",
            "iter 1300: loss 1.7546, time 336.92ms, mfu 1.51%\n",
            "iter 1310: loss 1.6955, time 346.79ms, mfu 1.52%\n",
            "iter 1320: loss 1.7414, time 363.27ms, mfu 1.51%\n",
            "iter 1330: loss 1.7321, time 340.62ms, mfu 1.52%\n",
            "iter 1340: loss 1.6799, time 341.99ms, mfu 1.53%\n",
            "iter 1350: loss 1.7341, time 353.83ms, mfu 1.53%\n",
            "iter 1360: loss 1.7035, time 337.29ms, mfu 1.53%\n",
            "iter 1370: loss 1.6418, time 335.98ms, mfu 1.54%\n",
            "iter 1380: loss 1.6979, time 336.21ms, mfu 1.55%\n",
            "iter 1390: loss 1.6644, time 349.32ms, mfu 1.55%\n",
            "step 1400: train loss 1.5553, val loss 1.7388\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1400: loss 1.6306, time 1823.51ms, mfu 1.42%\n",
            "iter 1410: loss 1.5901, time 344.52ms, mfu 1.44%\n",
            "iter 1420: loss 1.7667, time 338.15ms, mfu 1.45%\n",
            "iter 1430: loss 1.7092, time 342.13ms, mfu 1.46%\n",
            "iter 1440: loss 1.6515, time 356.38ms, mfu 1.47%\n",
            "iter 1450: loss 1.6500, time 353.92ms, mfu 1.48%\n",
            "iter 1460: loss 1.6455, time 355.07ms, mfu 1.48%\n",
            "iter 1470: loss 1.6234, time 343.14ms, mfu 1.49%\n",
            "iter 1480: loss 1.6102, time 354.88ms, mfu 1.49%\n",
            "iter 1490: loss 1.6846, time 349.25ms, mfu 1.50%\n",
            "iter 1500: loss 1.6938, time 336.43ms, mfu 1.51%\n",
            "iter 1510: loss 1.6219, time 334.62ms, mfu 1.52%\n",
            "iter 1520: loss 1.6411, time 341.12ms, mfu 1.53%\n",
            "iter 1530: loss 1.5783, time 335.56ms, mfu 1.53%\n",
            "iter 1540: loss 1.6068, time 335.90ms, mfu 1.54%\n",
            "iter 1550: loss 1.6897, time 338.66ms, mfu 1.55%\n",
            "iter 1560: loss 1.5311, time 343.31ms, mfu 1.55%\n",
            "iter 1570: loss 1.6521, time 338.67ms, mfu 1.55%\n",
            "iter 1580: loss 1.6263, time 339.16ms, mfu 1.56%\n",
            "iter 1590: loss 1.5210, time 334.57ms, mfu 1.56%\n",
            "step 1600: train loss 1.4877, val loss 1.6792\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1600: loss 1.7010, time 1825.99ms, mfu 1.44%\n",
            "iter 1610: loss 1.6023, time 335.32ms, mfu 1.45%\n",
            "iter 1620: loss 1.5861, time 338.66ms, mfu 1.47%\n",
            "iter 1630: loss 1.5745, time 346.47ms, mfu 1.48%\n",
            "iter 1640: loss 1.5970, time 348.52ms, mfu 1.48%\n",
            "iter 1650: loss 1.5110, time 339.07ms, mfu 1.49%\n",
            "iter 1660: loss 1.5199, time 341.75ms, mfu 1.50%\n",
            "iter 1670: loss 1.5747, time 341.03ms, mfu 1.51%\n",
            "iter 1680: loss 1.5435, time 352.75ms, mfu 1.51%\n",
            "iter 1690: loss 1.4989, time 338.56ms, mfu 1.52%\n",
            "iter 1700: loss 1.5772, time 347.07ms, mfu 1.52%\n",
            "iter 1710: loss 1.6268, time 334.12ms, mfu 1.53%\n",
            "iter 1720: loss 1.5977, time 340.96ms, mfu 1.54%\n",
            "iter 1730: loss 1.5237, time 353.36ms, mfu 1.54%\n",
            "iter 1740: loss 1.5793, time 345.55ms, mfu 1.54%\n",
            "iter 1750: loss 1.5437, time 339.90ms, mfu 1.54%\n",
            "iter 1760: loss 1.6550, time 335.95ms, mfu 1.55%\n",
            "iter 1770: loss 1.5570, time 347.36ms, mfu 1.55%\n",
            "iter 1780: loss 1.4880, time 343.78ms, mfu 1.55%\n",
            "iter 1790: loss 1.5778, time 348.83ms, mfu 1.55%\n",
            "step 1800: train loss 1.4295, val loss 1.6264\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1800: loss 1.5779, time 1815.84ms, mfu 1.43%\n",
            "iter 1810: loss 1.4735, time 348.97ms, mfu 1.44%\n",
            "iter 1820: loss 1.5528, time 344.18ms, mfu 1.45%\n",
            "iter 1830: loss 1.6074, time 354.98ms, mfu 1.46%\n",
            "iter 1840: loss 1.6012, time 345.71ms, mfu 1.47%\n",
            "iter 1850: loss 1.5624, time 338.78ms, mfu 1.48%\n",
            "iter 1860: loss 1.5070, time 335.42ms, mfu 1.49%\n",
            "iter 1870: loss 1.4518, time 354.83ms, mfu 1.50%\n",
            "iter 1880: loss 1.4073, time 343.04ms, mfu 1.50%\n",
            "iter 1890: loss 1.5622, time 347.08ms, mfu 1.51%\n",
            "iter 1900: loss 1.5357, time 337.85ms, mfu 1.52%\n",
            "iter 1910: loss 1.4800, time 334.21ms, mfu 1.53%\n",
            "iter 1920: loss 1.4828, time 335.64ms, mfu 1.54%\n",
            "iter 1930: loss 1.5826, time 339.23ms, mfu 1.54%\n",
            "iter 1940: loss 1.5089, time 349.85ms, mfu 1.54%\n",
            "iter 1950: loss 1.4706, time 345.52ms, mfu 1.54%\n",
            "iter 1960: loss 1.5022, time 340.88ms, mfu 1.55%\n",
            "iter 1970: loss 1.4723, time 358.45ms, mfu 1.54%\n",
            "iter 1980: loss 1.5892, time 347.37ms, mfu 1.54%\n",
            "iter 1990: loss 1.4655, time 339.40ms, mfu 1.55%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 17/32: b128_L4_H8_E128_BS8_MI1000_D10_s17 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS8_MI1000_D10_s17.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 17\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1833, val loss 4.1802\n",
            "iter 0: loss 4.1814, time 1986.15ms, mfu -100.00%\n",
            "iter 10: loss 4.1696, time 332.19ms, mfu 0.22%\n",
            "iter 20: loss 4.1361, time 332.39ms, mfu 0.22%\n",
            "iter 30: loss 4.0849, time 333.18ms, mfu 0.22%\n",
            "iter 40: loss 4.0282, time 332.80ms, mfu 0.22%\n",
            "iter 50: loss 3.9423, time 326.86ms, mfu 0.22%\n",
            "iter 60: loss 3.8305, time 325.68ms, mfu 0.22%\n",
            "iter 70: loss 3.8102, time 341.23ms, mfu 0.22%\n",
            "iter 80: loss 3.6988, time 322.97ms, mfu 0.22%\n",
            "iter 90: loss 3.7168, time 318.88ms, mfu 0.22%\n",
            "iter 100: loss 3.6727, time 317.96ms, mfu 0.22%\n",
            "iter 110: loss 3.6311, time 322.43ms, mfu 0.22%\n",
            "iter 120: loss 3.5492, time 327.37ms, mfu 0.22%\n",
            "iter 130: loss 3.4961, time 322.39ms, mfu 0.22%\n",
            "iter 140: loss 3.4652, time 327.80ms, mfu 0.22%\n",
            "iter 150: loss 3.4243, time 326.58ms, mfu 0.22%\n",
            "iter 160: loss 3.3873, time 331.13ms, mfu 0.22%\n",
            "iter 170: loss 3.3719, time 327.27ms, mfu 0.22%\n",
            "iter 180: loss 3.3721, time 324.22ms, mfu 0.22%\n",
            "iter 190: loss 3.3441, time 318.07ms, mfu 0.22%\n",
            "step 200: train loss 3.2392, val loss 3.2504\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 200: loss 3.2967, time 3464.29ms, mfu 0.20%\n",
            "iter 210: loss 3.2613, time 336.62ms, mfu 0.20%\n",
            "iter 220: loss 3.2153, time 327.35ms, mfu 0.21%\n",
            "iter 230: loss 3.2120, time 335.16ms, mfu 0.21%\n",
            "iter 240: loss 3.1308, time 328.74ms, mfu 0.21%\n",
            "iter 250: loss 3.1297, time 317.21ms, mfu 0.21%\n",
            "iter 260: loss 3.0884, time 323.90ms, mfu 0.21%\n",
            "iter 270: loss 3.0909, time 320.72ms, mfu 0.21%\n",
            "iter 280: loss 3.0531, time 324.64ms, mfu 0.22%\n",
            "iter 290: loss 3.0180, time 326.68ms, mfu 0.22%\n",
            "iter 300: loss 3.0232, time 318.46ms, mfu 0.22%\n",
            "iter 310: loss 2.9744, time 322.19ms, mfu 0.22%\n",
            "iter 320: loss 2.9604, time 330.96ms, mfu 0.22%\n",
            "iter 330: loss 2.9164, time 317.69ms, mfu 0.22%\n",
            "iter 340: loss 2.9175, time 327.88ms, mfu 0.22%\n",
            "iter 350: loss 2.8568, time 326.79ms, mfu 0.22%\n",
            "iter 360: loss 2.8955, time 319.12ms, mfu 0.22%\n",
            "iter 370: loss 2.8658, time 325.63ms, mfu 0.22%\n",
            "iter 380: loss 2.7669, time 327.90ms, mfu 0.22%\n",
            "iter 390: loss 2.7486, time 341.35ms, mfu 0.22%\n",
            "step 400: train loss 2.7661, val loss 2.7807\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 400: loss 2.8199, time 1593.25ms, mfu 0.20%\n",
            "iter 410: loss 2.7879, time 322.51ms, mfu 0.21%\n",
            "iter 420: loss 2.7213, time 321.88ms, mfu 0.21%\n",
            "iter 430: loss 2.6984, time 331.92ms, mfu 0.21%\n",
            "iter 440: loss 2.7863, time 320.53ms, mfu 0.21%\n",
            "iter 450: loss 2.7120, time 323.85ms, mfu 0.21%\n",
            "iter 460: loss 2.7517, time 330.63ms, mfu 0.21%\n",
            "iter 470: loss 2.7282, time 320.34ms, mfu 0.21%\n",
            "iter 480: loss 2.6784, time 325.41ms, mfu 0.22%\n",
            "iter 490: loss 2.6496, time 327.45ms, mfu 0.22%\n",
            "iter 500: loss 2.6443, time 327.00ms, mfu 0.22%\n",
            "iter 510: loss 2.6478, time 321.92ms, mfu 0.22%\n",
            "iter 520: loss 2.5763, time 323.97ms, mfu 0.22%\n",
            "iter 530: loss 2.6044, time 323.23ms, mfu 0.22%\n",
            "iter 540: loss 2.5247, time 343.42ms, mfu 0.22%\n",
            "iter 550: loss 2.5749, time 319.06ms, mfu 0.22%\n",
            "iter 560: loss 2.6017, time 336.39ms, mfu 0.22%\n",
            "iter 570: loss 2.5447, time 326.10ms, mfu 0.22%\n",
            "iter 580: loss 2.5902, time 327.86ms, mfu 0.22%\n",
            "iter 590: loss 2.4809, time 321.89ms, mfu 0.22%\n",
            "step 600: train loss 2.4834, val loss 2.4907\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 600: loss 2.5468, time 1598.86ms, mfu 0.20%\n",
            "iter 610: loss 2.5284, time 346.84ms, mfu 0.20%\n",
            "iter 620: loss 2.5873, time 328.31ms, mfu 0.21%\n",
            "iter 630: loss 2.5984, time 340.05ms, mfu 0.21%\n",
            "iter 640: loss 2.5151, time 330.89ms, mfu 0.21%\n",
            "iter 650: loss 2.5505, time 336.93ms, mfu 0.21%\n",
            "iter 660: loss 2.4905, time 334.26ms, mfu 0.21%\n",
            "iter 670: loss 2.4231, time 328.14ms, mfu 0.21%\n",
            "iter 680: loss 2.4287, time 336.09ms, mfu 0.21%\n",
            "iter 690: loss 2.4500, time 329.42ms, mfu 0.21%\n",
            "iter 700: loss 2.4816, time 324.65ms, mfu 0.21%\n",
            "iter 710: loss 2.4438, time 332.12ms, mfu 0.21%\n",
            "iter 720: loss 2.4347, time 333.64ms, mfu 0.22%\n",
            "iter 730: loss 2.3327, time 325.54ms, mfu 0.22%\n",
            "iter 740: loss 2.3849, time 332.11ms, mfu 0.22%\n",
            "iter 750: loss 2.3864, time 334.39ms, mfu 0.22%\n",
            "iter 760: loss 2.4524, time 332.36ms, mfu 0.22%\n",
            "iter 770: loss 2.4235, time 325.94ms, mfu 0.22%\n",
            "iter 780: loss 2.3625, time 320.42ms, mfu 0.22%\n",
            "iter 790: loss 2.4157, time 336.02ms, mfu 0.22%\n",
            "step 800: train loss 2.3354, val loss 2.3485\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 800: loss 2.3581, time 1683.34ms, mfu 0.20%\n",
            "iter 810: loss 2.3737, time 323.42ms, mfu 0.20%\n",
            "iter 820: loss 2.3735, time 327.02ms, mfu 0.21%\n",
            "iter 830: loss 2.3809, time 324.86ms, mfu 0.21%\n",
            "iter 840: loss 2.3341, time 325.77ms, mfu 0.21%\n",
            "iter 850: loss 2.3474, time 317.79ms, mfu 0.21%\n",
            "iter 860: loss 2.4073, time 330.05ms, mfu 0.21%\n",
            "iter 870: loss 2.3813, time 326.13ms, mfu 0.21%\n",
            "iter 880: loss 2.2786, time 325.11ms, mfu 0.21%\n",
            "iter 890: loss 2.3454, time 327.32ms, mfu 0.22%\n",
            "iter 900: loss 2.3675, time 330.02ms, mfu 0.22%\n",
            "iter 910: loss 2.2543, time 327.54ms, mfu 0.22%\n",
            "iter 920: loss 2.3442, time 320.63ms, mfu 0.22%\n",
            "iter 930: loss 2.3103, time 326.19ms, mfu 0.22%\n",
            "iter 940: loss 2.2301, time 322.31ms, mfu 0.22%\n",
            "iter 950: loss 2.2493, time 323.30ms, mfu 0.22%\n",
            "iter 960: loss 2.2602, time 324.95ms, mfu 0.22%\n",
            "iter 970: loss 2.3272, time 329.40ms, mfu 0.22%\n",
            "iter 980: loss 2.3623, time 318.00ms, mfu 0.22%\n",
            "iter 990: loss 2.2284, time 334.31ms, mfu 0.22%\n",
            "step 1000: train loss 2.1974, val loss 2.2253\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 1000: loss 2.2618, time 1596.87ms, mfu 0.20%\n",
            "\n",
            "=== Experiment 18/32: b128_L4_H8_E128_BS8_MI1000_D20_s18 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS8_MI1000_D20_s18.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 18\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1833, val loss 4.1802\n",
            "iter 0: loss 4.1863, time 1931.63ms, mfu -100.00%\n",
            "iter 10: loss 4.1738, time 332.17ms, mfu 0.22%\n",
            "iter 20: loss 4.1435, time 342.98ms, mfu 0.22%\n",
            "iter 30: loss 4.1004, time 335.97ms, mfu 0.22%\n",
            "iter 40: loss 4.0476, time 338.72ms, mfu 0.22%\n",
            "iter 50: loss 3.9747, time 319.52ms, mfu 0.22%\n",
            "iter 60: loss 3.8642, time 335.08ms, mfu 0.22%\n",
            "iter 70: loss 3.8356, time 335.66ms, mfu 0.22%\n",
            "iter 80: loss 3.7216, time 329.69ms, mfu 0.22%\n",
            "iter 90: loss 3.7368, time 330.97ms, mfu 0.22%\n",
            "iter 100: loss 3.6929, time 363.01ms, mfu 0.22%\n",
            "iter 110: loss 3.6608, time 330.04ms, mfu 0.22%\n",
            "iter 120: loss 3.5819, time 329.55ms, mfu 0.22%\n",
            "iter 130: loss 3.5414, time 340.29ms, mfu 0.22%\n",
            "iter 140: loss 3.5076, time 325.33ms, mfu 0.22%\n",
            "iter 150: loss 3.4690, time 329.86ms, mfu 0.22%\n",
            "iter 160: loss 3.4278, time 349.63ms, mfu 0.22%\n",
            "iter 170: loss 3.4053, time 338.54ms, mfu 0.22%\n",
            "iter 180: loss 3.4169, time 330.11ms, mfu 0.22%\n",
            "iter 190: loss 3.3780, time 339.28ms, mfu 0.22%\n",
            "step 200: train loss 3.2564, val loss 3.2697\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 200: loss 3.3322, time 3083.72ms, mfu 0.20%\n",
            "iter 210: loss 3.2974, time 322.83ms, mfu 0.20%\n",
            "iter 220: loss 3.2519, time 321.31ms, mfu 0.20%\n",
            "iter 230: loss 3.2543, time 321.85ms, mfu 0.21%\n",
            "iter 240: loss 3.1629, time 328.65ms, mfu 0.21%\n",
            "iter 250: loss 3.1676, time 329.49ms, mfu 0.21%\n",
            "iter 260: loss 3.1218, time 331.65ms, mfu 0.21%\n",
            "iter 270: loss 3.1188, time 338.36ms, mfu 0.21%\n",
            "iter 280: loss 3.0865, time 326.98ms, mfu 0.21%\n",
            "iter 290: loss 3.0432, time 327.09ms, mfu 0.21%\n",
            "iter 300: loss 3.0556, time 327.28ms, mfu 0.21%\n",
            "iter 310: loss 3.0001, time 326.80ms, mfu 0.22%\n",
            "iter 320: loss 2.9908, time 320.24ms, mfu 0.22%\n",
            "iter 330: loss 2.9498, time 321.47ms, mfu 0.22%\n",
            "iter 340: loss 2.9495, time 324.46ms, mfu 0.22%\n",
            "iter 350: loss 2.8858, time 335.24ms, mfu 0.22%\n",
            "iter 360: loss 2.9270, time 329.84ms, mfu 0.22%\n",
            "iter 370: loss 2.8895, time 343.00ms, mfu 0.22%\n",
            "iter 380: loss 2.7904, time 342.82ms, mfu 0.22%\n",
            "iter 390: loss 2.7743, time 323.77ms, mfu 0.22%\n",
            "step 400: train loss 2.7809, val loss 2.7925\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 400: loss 2.8500, time 1618.11ms, mfu 0.20%\n",
            "iter 410: loss 2.8140, time 320.90ms, mfu 0.20%\n",
            "iter 420: loss 2.7493, time 322.24ms, mfu 0.21%\n",
            "iter 430: loss 2.7189, time 326.26ms, mfu 0.21%\n",
            "iter 440: loss 2.8113, time 324.57ms, mfu 0.21%\n",
            "iter 450: loss 2.7501, time 333.93ms, mfu 0.21%\n",
            "iter 460: loss 2.7857, time 322.63ms, mfu 0.21%\n",
            "iter 470: loss 2.7570, time 331.48ms, mfu 0.21%\n",
            "iter 480: loss 2.7059, time 321.98ms, mfu 0.21%\n",
            "iter 490: loss 2.6785, time 322.60ms, mfu 0.22%\n",
            "iter 500: loss 2.6726, time 320.38ms, mfu 0.22%\n",
            "iter 510: loss 2.6848, time 325.80ms, mfu 0.22%\n",
            "iter 520: loss 2.6057, time 321.85ms, mfu 0.22%\n",
            "iter 530: loss 2.6299, time 333.32ms, mfu 0.22%\n",
            "iter 540: loss 2.5557, time 321.57ms, mfu 0.22%\n",
            "iter 550: loss 2.6116, time 323.70ms, mfu 0.22%\n",
            "iter 560: loss 2.6287, time 323.93ms, mfu 0.22%\n",
            "iter 570: loss 2.5875, time 321.36ms, mfu 0.22%\n",
            "iter 580: loss 2.6353, time 328.28ms, mfu 0.22%\n",
            "iter 590: loss 2.5138, time 329.92ms, mfu 0.22%\n",
            "step 600: train loss 2.5101, val loss 2.5174\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 600: loss 2.5749, time 1609.67ms, mfu 0.20%\n",
            "iter 610: loss 2.5502, time 326.37ms, mfu 0.21%\n",
            "iter 620: loss 2.6193, time 320.96ms, mfu 0.21%\n",
            "iter 630: loss 2.6145, time 327.48ms, mfu 0.21%\n",
            "iter 640: loss 2.5342, time 321.82ms, mfu 0.21%\n",
            "iter 650: loss 2.5939, time 348.01ms, mfu 0.21%\n",
            "iter 660: loss 2.5290, time 326.80ms, mfu 0.21%\n",
            "iter 670: loss 2.4509, time 322.61ms, mfu 0.21%\n",
            "iter 680: loss 2.4706, time 327.75ms, mfu 0.21%\n",
            "iter 690: loss 2.4731, time 329.22ms, mfu 0.22%\n",
            "iter 700: loss 2.5138, time 321.25ms, mfu 0.22%\n",
            "iter 710: loss 2.4674, time 329.92ms, mfu 0.22%\n",
            "iter 720: loss 2.4657, time 329.48ms, mfu 0.22%\n",
            "iter 730: loss 2.3656, time 322.02ms, mfu 0.22%\n",
            "iter 740: loss 2.4332, time 323.37ms, mfu 0.22%\n",
            "iter 750: loss 2.4182, time 327.38ms, mfu 0.22%\n",
            "iter 760: loss 2.4875, time 327.94ms, mfu 0.22%\n",
            "iter 770: loss 2.4662, time 324.07ms, mfu 0.22%\n",
            "iter 780: loss 2.4093, time 337.14ms, mfu 0.22%\n",
            "iter 790: loss 2.4704, time 329.13ms, mfu 0.22%\n",
            "step 800: train loss 2.3740, val loss 2.3842\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 800: loss 2.4142, time 1628.07ms, mfu 0.20%\n",
            "iter 810: loss 2.4008, time 323.86ms, mfu 0.20%\n",
            "iter 820: loss 2.4154, time 329.89ms, mfu 0.21%\n",
            "iter 830: loss 2.4301, time 341.38ms, mfu 0.21%\n",
            "iter 840: loss 2.3720, time 341.37ms, mfu 0.21%\n",
            "iter 850: loss 2.3983, time 332.45ms, mfu 0.21%\n",
            "iter 860: loss 2.4151, time 340.47ms, mfu 0.21%\n",
            "iter 870: loss 2.4233, time 329.94ms, mfu 0.21%\n",
            "iter 880: loss 2.3387, time 331.85ms, mfu 0.21%\n",
            "iter 890: loss 2.3862, time 324.24ms, mfu 0.21%\n",
            "iter 900: loss 2.4129, time 326.32ms, mfu 0.21%\n",
            "iter 910: loss 2.3308, time 322.09ms, mfu 0.22%\n",
            "iter 920: loss 2.3991, time 327.49ms, mfu 0.22%\n",
            "iter 930: loss 2.3925, time 335.76ms, mfu 0.22%\n",
            "iter 940: loss 2.2873, time 339.93ms, mfu 0.22%\n",
            "iter 950: loss 2.3113, time 338.43ms, mfu 0.22%\n",
            "iter 960: loss 2.3389, time 335.33ms, mfu 0.22%\n",
            "iter 970: loss 2.3761, time 337.84ms, mfu 0.22%\n",
            "iter 980: loss 2.4038, time 331.00ms, mfu 0.22%\n",
            "iter 990: loss 2.3023, time 325.06ms, mfu 0.22%\n",
            "step 1000: train loss 2.2510, val loss 2.2734\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 1000: loss 2.3305, time 1599.58ms, mfu 0.20%\n",
            "\n",
            "=== Experiment 19/32: b128_L4_H8_E128_BS8_MI2000_D10_s19 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS8_MI2000_D10_s19.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 19\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1833, val loss 4.1802\n",
            "iter 0: loss 4.1814, time 1944.91ms, mfu -100.00%\n",
            "iter 10: loss 4.1696, time 321.95ms, mfu 0.23%\n",
            "iter 20: loss 4.1361, time 320.17ms, mfu 0.23%\n",
            "iter 30: loss 4.0849, time 333.51ms, mfu 0.23%\n",
            "iter 40: loss 4.0282, time 321.85ms, mfu 0.23%\n",
            "iter 50: loss 3.9423, time 329.00ms, mfu 0.23%\n",
            "iter 60: loss 3.8305, time 341.91ms, mfu 0.22%\n",
            "iter 70: loss 3.8102, time 324.96ms, mfu 0.22%\n",
            "iter 80: loss 3.6988, time 326.27ms, mfu 0.22%\n",
            "iter 90: loss 3.7168, time 344.18ms, mfu 0.22%\n",
            "iter 100: loss 3.6727, time 327.98ms, mfu 0.22%\n",
            "iter 110: loss 3.6311, time 330.22ms, mfu 0.22%\n",
            "iter 120: loss 3.5492, time 329.29ms, mfu 0.22%\n",
            "iter 130: loss 3.4961, time 323.58ms, mfu 0.22%\n",
            "iter 140: loss 3.4652, time 317.27ms, mfu 0.22%\n",
            "iter 150: loss 3.4243, time 326.75ms, mfu 0.22%\n",
            "iter 160: loss 3.3873, time 336.21ms, mfu 0.22%\n",
            "iter 170: loss 3.3719, time 322.18ms, mfu 0.22%\n",
            "iter 180: loss 3.3721, time 323.47ms, mfu 0.22%\n",
            "iter 190: loss 3.3441, time 323.28ms, mfu 0.22%\n",
            "step 200: train loss 3.2392, val loss 3.2504\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 200: loss 3.2967, time 3297.07ms, mfu 0.20%\n",
            "iter 210: loss 3.2613, time 325.40ms, mfu 0.21%\n",
            "iter 220: loss 3.2153, time 323.96ms, mfu 0.21%\n",
            "iter 230: loss 3.2120, time 331.03ms, mfu 0.21%\n",
            "iter 240: loss 3.1308, time 337.78ms, mfu 0.21%\n",
            "iter 250: loss 3.1297, time 328.62ms, mfu 0.21%\n",
            "iter 260: loss 3.0884, time 327.36ms, mfu 0.21%\n",
            "iter 270: loss 3.0909, time 327.51ms, mfu 0.21%\n",
            "iter 280: loss 3.0531, time 324.87ms, mfu 0.21%\n",
            "iter 290: loss 3.0180, time 324.14ms, mfu 0.22%\n",
            "iter 300: loss 3.0232, time 338.34ms, mfu 0.22%\n",
            "iter 310: loss 2.9744, time 326.34ms, mfu 0.22%\n",
            "iter 320: loss 2.9604, time 325.57ms, mfu 0.22%\n",
            "iter 330: loss 2.9164, time 323.31ms, mfu 0.22%\n",
            "iter 340: loss 2.9175, time 342.28ms, mfu 0.22%\n",
            "iter 350: loss 2.8568, time 323.58ms, mfu 0.22%\n",
            "iter 360: loss 2.8955, time 325.73ms, mfu 0.22%\n",
            "iter 370: loss 2.8658, time 329.11ms, mfu 0.22%\n",
            "iter 380: loss 2.7669, time 336.11ms, mfu 0.22%\n",
            "iter 390: loss 2.7486, time 339.68ms, mfu 0.22%\n",
            "step 400: train loss 2.7661, val loss 2.7807\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 400: loss 2.8199, time 1629.66ms, mfu 0.20%\n",
            "iter 410: loss 2.7879, time 333.63ms, mfu 0.20%\n",
            "iter 420: loss 2.7213, time 335.86ms, mfu 0.20%\n",
            "iter 430: loss 2.6984, time 327.06ms, mfu 0.21%\n",
            "iter 440: loss 2.7863, time 324.31ms, mfu 0.21%\n",
            "iter 450: loss 2.7120, time 326.93ms, mfu 0.21%\n",
            "iter 460: loss 2.7517, time 331.84ms, mfu 0.21%\n",
            "iter 470: loss 2.7282, time 323.56ms, mfu 0.21%\n",
            "iter 480: loss 2.6784, time 331.52ms, mfu 0.21%\n",
            "iter 490: loss 2.6496, time 324.61ms, mfu 0.21%\n",
            "iter 500: loss 2.6443, time 328.52ms, mfu 0.22%\n",
            "iter 510: loss 2.6478, time 321.15ms, mfu 0.22%\n",
            "iter 520: loss 2.5763, time 328.88ms, mfu 0.22%\n",
            "iter 530: loss 2.6044, time 326.78ms, mfu 0.22%\n",
            "iter 540: loss 2.5247, time 328.58ms, mfu 0.22%\n",
            "iter 550: loss 2.5749, time 323.15ms, mfu 0.22%\n",
            "iter 560: loss 2.6017, time 328.84ms, mfu 0.22%\n",
            "iter 570: loss 2.5447, time 327.29ms, mfu 0.22%\n",
            "iter 580: loss 2.5902, time 324.85ms, mfu 0.22%\n",
            "iter 590: loss 2.4809, time 327.66ms, mfu 0.22%\n",
            "step 600: train loss 2.4834, val loss 2.4907\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 600: loss 2.5468, time 1605.71ms, mfu 0.20%\n",
            "iter 610: loss 2.5284, time 315.40ms, mfu 0.21%\n",
            "iter 620: loss 2.5873, time 316.44ms, mfu 0.21%\n",
            "iter 630: loss 2.5984, time 347.06ms, mfu 0.21%\n",
            "iter 640: loss 2.5151, time 333.29ms, mfu 0.21%\n",
            "iter 650: loss 2.5505, time 332.20ms, mfu 0.21%\n",
            "iter 660: loss 2.4905, time 324.38ms, mfu 0.21%\n",
            "iter 670: loss 2.4231, time 330.76ms, mfu 0.21%\n",
            "iter 680: loss 2.4287, time 332.44ms, mfu 0.21%\n",
            "iter 690: loss 2.4500, time 326.40ms, mfu 0.21%\n",
            "iter 700: loss 2.4816, time 321.93ms, mfu 0.22%\n",
            "iter 710: loss 2.4438, time 326.09ms, mfu 0.22%\n",
            "iter 720: loss 2.4347, time 332.79ms, mfu 0.22%\n",
            "iter 730: loss 2.3327, time 329.19ms, mfu 0.22%\n",
            "iter 740: loss 2.3849, time 337.78ms, mfu 0.22%\n",
            "iter 750: loss 2.3864, time 324.60ms, mfu 0.22%\n",
            "iter 760: loss 2.4524, time 326.96ms, mfu 0.22%\n",
            "iter 770: loss 2.4235, time 339.20ms, mfu 0.22%\n",
            "iter 780: loss 2.3625, time 321.53ms, mfu 0.22%\n",
            "iter 790: loss 2.4157, time 323.71ms, mfu 0.22%\n",
            "step 800: train loss 2.3354, val loss 2.3485\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 800: loss 2.3581, time 1622.31ms, mfu 0.20%\n",
            "iter 810: loss 2.3737, time 326.33ms, mfu 0.20%\n",
            "iter 820: loss 2.3735, time 328.58ms, mfu 0.21%\n",
            "iter 830: loss 2.3809, time 335.47ms, mfu 0.21%\n",
            "iter 840: loss 2.3341, time 327.14ms, mfu 0.21%\n",
            "iter 850: loss 2.3474, time 325.83ms, mfu 0.21%\n",
            "iter 860: loss 2.4073, time 324.07ms, mfu 0.21%\n",
            "iter 870: loss 2.3813, time 330.13ms, mfu 0.21%\n",
            "iter 880: loss 2.2786, time 329.88ms, mfu 0.21%\n",
            "iter 890: loss 2.3454, time 344.69ms, mfu 0.21%\n",
            "iter 900: loss 2.3675, time 338.18ms, mfu 0.21%\n",
            "iter 910: loss 2.2543, time 330.29ms, mfu 0.21%\n",
            "iter 920: loss 2.3442, time 336.86ms, mfu 0.21%\n",
            "iter 930: loss 2.3103, time 324.00ms, mfu 0.22%\n",
            "iter 940: loss 2.2301, time 326.33ms, mfu 0.22%\n",
            "iter 950: loss 2.2493, time 343.63ms, mfu 0.22%\n",
            "iter 960: loss 2.2602, time 330.17ms, mfu 0.22%\n",
            "iter 970: loss 2.3272, time 321.93ms, mfu 0.22%\n",
            "iter 980: loss 2.3623, time 342.33ms, mfu 0.22%\n",
            "iter 990: loss 2.2284, time 329.21ms, mfu 0.22%\n",
            "step 1000: train loss 2.1974, val loss 2.2253\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1000: loss 2.2618, time 1606.62ms, mfu 0.20%\n",
            "iter 1010: loss 2.1966, time 323.30ms, mfu 0.20%\n",
            "iter 1020: loss 2.2768, time 334.70ms, mfu 0.20%\n",
            "iter 1030: loss 2.2789, time 333.72ms, mfu 0.21%\n",
            "iter 1040: loss 2.3419, time 326.12ms, mfu 0.21%\n",
            "iter 1050: loss 2.2352, time 327.02ms, mfu 0.21%\n",
            "iter 1060: loss 2.1776, time 332.38ms, mfu 0.21%\n",
            "iter 1070: loss 2.2486, time 321.06ms, mfu 0.21%\n",
            "iter 1080: loss 2.1990, time 328.53ms, mfu 0.21%\n",
            "iter 1090: loss 2.2206, time 348.65ms, mfu 0.21%\n",
            "iter 1100: loss 2.1097, time 327.67ms, mfu 0.21%\n",
            "iter 1110: loss 2.1195, time 337.87ms, mfu 0.21%\n",
            "iter 1120: loss 2.1933, time 330.78ms, mfu 0.21%\n",
            "iter 1130: loss 2.1619, time 338.17ms, mfu 0.21%\n",
            "iter 1140: loss 2.1752, time 332.49ms, mfu 0.22%\n",
            "iter 1150: loss 2.2308, time 326.30ms, mfu 0.22%\n",
            "iter 1160: loss 2.2257, time 325.10ms, mfu 0.22%\n",
            "iter 1170: loss 2.1573, time 333.57ms, mfu 0.22%\n",
            "iter 1180: loss 2.1675, time 328.90ms, mfu 0.22%\n",
            "iter 1190: loss 2.1325, time 321.36ms, mfu 0.22%\n",
            "step 1200: train loss 2.0561, val loss 2.1059\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1200: loss 2.1165, time 1601.62ms, mfu 0.20%\n",
            "iter 1210: loss 2.2186, time 329.84ms, mfu 0.20%\n",
            "iter 1220: loss 2.1086, time 329.99ms, mfu 0.21%\n",
            "iter 1230: loss 2.1063, time 339.97ms, mfu 0.21%\n",
            "iter 1240: loss 2.1081, time 335.20ms, mfu 0.21%\n",
            "iter 1250: loss 2.1150, time 328.62ms, mfu 0.21%\n",
            "iter 1260: loss 2.0876, time 325.14ms, mfu 0.21%\n",
            "iter 1270: loss 2.1788, time 328.13ms, mfu 0.21%\n",
            "iter 1280: loss 2.1516, time 331.21ms, mfu 0.21%\n",
            "iter 1290: loss 2.0328, time 328.73ms, mfu 0.21%\n",
            "iter 1300: loss 2.1750, time 330.12ms, mfu 0.21%\n",
            "iter 1310: loss 2.0881, time 335.40ms, mfu 0.21%\n",
            "iter 1320: loss 2.0758, time 321.24ms, mfu 0.22%\n",
            "iter 1330: loss 2.0401, time 326.99ms, mfu 0.22%\n",
            "iter 1340: loss 2.0244, time 338.65ms, mfu 0.22%\n",
            "iter 1350: loss 2.0241, time 336.99ms, mfu 0.22%\n",
            "iter 1360: loss 2.0044, time 320.91ms, mfu 0.22%\n",
            "iter 1370: loss 2.0419, time 320.68ms, mfu 0.22%\n",
            "iter 1380: loss 2.1043, time 320.86ms, mfu 0.22%\n",
            "iter 1390: loss 1.9916, time 322.89ms, mfu 0.22%\n",
            "step 1400: train loss 1.9346, val loss 2.0196\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1400: loss 2.0426, time 1618.40ms, mfu 0.20%\n",
            "iter 1410: loss 2.0110, time 326.83ms, mfu 0.20%\n",
            "iter 1420: loss 1.9808, time 331.63ms, mfu 0.21%\n",
            "iter 1430: loss 1.9627, time 339.03ms, mfu 0.21%\n",
            "iter 1440: loss 1.9674, time 328.79ms, mfu 0.21%\n",
            "iter 1450: loss 1.9693, time 336.42ms, mfu 0.21%\n",
            "iter 1460: loss 1.9804, time 334.21ms, mfu 0.21%\n",
            "iter 1470: loss 2.0370, time 332.67ms, mfu 0.21%\n",
            "iter 1480: loss 2.0769, time 322.52ms, mfu 0.21%\n",
            "iter 1490: loss 1.9498, time 334.62ms, mfu 0.21%\n",
            "iter 1500: loss 2.0220, time 352.44ms, mfu 0.21%\n",
            "iter 1510: loss 1.9267, time 321.55ms, mfu 0.21%\n",
            "iter 1520: loss 1.9729, time 324.76ms, mfu 0.22%\n",
            "iter 1530: loss 1.9651, time 336.14ms, mfu 0.22%\n",
            "iter 1540: loss 1.9082, time 329.25ms, mfu 0.22%\n",
            "iter 1550: loss 2.0031, time 333.04ms, mfu 0.22%\n",
            "iter 1560: loss 1.9643, time 332.41ms, mfu 0.22%\n",
            "iter 1570: loss 1.9631, time 330.90ms, mfu 0.22%\n",
            "iter 1580: loss 1.9259, time 333.61ms, mfu 0.22%\n",
            "iter 1590: loss 1.9011, time 325.00ms, mfu 0.22%\n",
            "step 1600: train loss 1.8187, val loss 1.9282\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1600: loss 1.9228, time 1635.79ms, mfu 0.20%\n",
            "iter 1610: loss 1.9223, time 333.31ms, mfu 0.20%\n",
            "iter 1620: loss 1.8643, time 326.74ms, mfu 0.20%\n",
            "iter 1630: loss 1.9223, time 331.04ms, mfu 0.21%\n",
            "iter 1640: loss 1.8884, time 323.29ms, mfu 0.21%\n",
            "iter 1650: loss 1.9134, time 337.76ms, mfu 0.21%\n",
            "iter 1660: loss 1.9256, time 328.99ms, mfu 0.21%\n",
            "iter 1670: loss 1.8610, time 332.34ms, mfu 0.21%\n",
            "iter 1680: loss 1.8804, time 330.70ms, mfu 0.21%\n",
            "iter 1690: loss 1.8365, time 329.84ms, mfu 0.21%\n",
            "iter 1700: loss 1.8270, time 328.63ms, mfu 0.21%\n",
            "iter 1710: loss 1.7844, time 331.43ms, mfu 0.21%\n",
            "iter 1720: loss 1.8397, time 329.79ms, mfu 0.22%\n",
            "iter 1730: loss 1.9111, time 322.71ms, mfu 0.22%\n",
            "iter 1740: loss 1.8396, time 326.34ms, mfu 0.22%\n",
            "iter 1750: loss 1.8369, time 329.55ms, mfu 0.22%\n",
            "iter 1760: loss 1.8604, time 323.60ms, mfu 0.22%\n",
            "iter 1770: loss 1.8020, time 324.23ms, mfu 0.22%\n",
            "iter 1780: loss 1.8711, time 326.77ms, mfu 0.22%\n",
            "iter 1790: loss 1.8459, time 318.11ms, mfu 0.22%\n",
            "step 1800: train loss 1.7146, val loss 1.8729\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1800: loss 1.7648, time 1622.37ms, mfu 0.20%\n",
            "iter 1810: loss 1.7183, time 338.63ms, mfu 0.20%\n",
            "iter 1820: loss 1.7021, time 324.56ms, mfu 0.21%\n",
            "iter 1830: loss 1.7838, time 338.75ms, mfu 0.21%\n",
            "iter 1840: loss 1.7775, time 335.10ms, mfu 0.21%\n",
            "iter 1850: loss 1.7344, time 352.37ms, mfu 0.21%\n",
            "iter 1860: loss 1.8716, time 335.03ms, mfu 0.21%\n",
            "iter 1870: loss 1.6882, time 329.95ms, mfu 0.21%\n",
            "iter 1880: loss 1.7185, time 334.81ms, mfu 0.21%\n",
            "iter 1890: loss 1.7154, time 346.97ms, mfu 0.21%\n",
            "iter 1900: loss 1.7351, time 336.29ms, mfu 0.21%\n",
            "iter 1910: loss 1.8614, time 336.66ms, mfu 0.21%\n",
            "iter 1920: loss 1.8647, time 347.97ms, mfu 0.21%\n",
            "iter 1930: loss 1.7279, time 337.89ms, mfu 0.21%\n",
            "iter 1940: loss 1.8292, time 358.65ms, mfu 0.21%\n",
            "iter 1950: loss 1.9124, time 342.50ms, mfu 0.21%\n",
            "iter 1960: loss 1.7473, time 354.22ms, mfu 0.21%\n",
            "iter 1970: loss 1.7394, time 351.95ms, mfu 0.21%\n",
            "iter 1980: loss 1.8131, time 350.74ms, mfu 0.21%\n",
            "iter 1990: loss 1.8722, time 337.90ms, mfu 0.21%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 20/32: b128_L4_H8_E128_BS8_MI2000_D20_s20 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS8_MI2000_D20_s20.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 20\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1833, val loss 4.1802\n",
            "iter 0: loss 4.1863, time 1953.50ms, mfu -100.00%\n",
            "iter 10: loss 4.1738, time 327.82ms, mfu 0.22%\n",
            "iter 20: loss 4.1435, time 357.39ms, mfu 0.22%\n",
            "iter 30: loss 4.1004, time 342.64ms, mfu 0.22%\n",
            "iter 40: loss 4.0476, time 329.07ms, mfu 0.22%\n",
            "iter 50: loss 3.9747, time 349.26ms, mfu 0.22%\n",
            "iter 60: loss 3.8642, time 328.02ms, mfu 0.22%\n",
            "iter 70: loss 3.8356, time 324.65ms, mfu 0.22%\n",
            "iter 80: loss 3.7216, time 325.41ms, mfu 0.22%\n",
            "iter 90: loss 3.7368, time 333.21ms, mfu 0.22%\n",
            "iter 100: loss 3.6929, time 320.11ms, mfu 0.22%\n",
            "iter 110: loss 3.6608, time 323.31ms, mfu 0.22%\n",
            "iter 120: loss 3.5819, time 325.43ms, mfu 0.22%\n",
            "iter 130: loss 3.5414, time 318.04ms, mfu 0.22%\n",
            "iter 140: loss 3.5076, time 326.25ms, mfu 0.22%\n",
            "iter 150: loss 3.4690, time 325.86ms, mfu 0.22%\n",
            "iter 160: loss 3.4278, time 337.16ms, mfu 0.22%\n",
            "iter 170: loss 3.4053, time 332.17ms, mfu 0.22%\n",
            "iter 180: loss 3.4169, time 326.15ms, mfu 0.22%\n",
            "iter 190: loss 3.3780, time 321.60ms, mfu 0.22%\n",
            "step 200: train loss 3.2564, val loss 3.2697\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 200: loss 3.3322, time 3447.81ms, mfu 0.20%\n",
            "iter 210: loss 3.2974, time 329.74ms, mfu 0.20%\n",
            "iter 220: loss 3.2519, time 321.57ms, mfu 0.21%\n",
            "iter 230: loss 3.2543, time 319.30ms, mfu 0.21%\n",
            "iter 240: loss 3.1629, time 333.39ms, mfu 0.21%\n",
            "iter 250: loss 3.1676, time 330.69ms, mfu 0.21%\n",
            "iter 260: loss 3.1218, time 332.94ms, mfu 0.21%\n",
            "iter 270: loss 3.1188, time 338.16ms, mfu 0.21%\n",
            "iter 280: loss 3.0865, time 323.43ms, mfu 0.21%\n",
            "iter 290: loss 3.0432, time 315.18ms, mfu 0.22%\n",
            "iter 300: loss 3.0556, time 326.58ms, mfu 0.22%\n",
            "iter 310: loss 3.0001, time 335.08ms, mfu 0.22%\n",
            "iter 320: loss 2.9908, time 326.69ms, mfu 0.22%\n",
            "iter 330: loss 2.9498, time 328.66ms, mfu 0.22%\n",
            "iter 340: loss 2.9495, time 330.73ms, mfu 0.22%\n",
            "iter 350: loss 2.8858, time 326.20ms, mfu 0.22%\n",
            "iter 360: loss 2.9270, time 327.79ms, mfu 0.22%\n",
            "iter 370: loss 2.8895, time 332.55ms, mfu 0.22%\n",
            "iter 380: loss 2.7904, time 323.77ms, mfu 0.22%\n",
            "iter 390: loss 2.7743, time 322.24ms, mfu 0.22%\n",
            "step 400: train loss 2.7809, val loss 2.7925\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 400: loss 2.8500, time 1633.49ms, mfu 0.20%\n",
            "iter 410: loss 2.8140, time 325.76ms, mfu 0.20%\n",
            "iter 420: loss 2.7493, time 325.16ms, mfu 0.21%\n",
            "iter 430: loss 2.7189, time 335.64ms, mfu 0.21%\n",
            "iter 440: loss 2.8113, time 331.86ms, mfu 0.21%\n",
            "iter 450: loss 2.7501, time 323.94ms, mfu 0.21%\n",
            "iter 460: loss 2.7857, time 323.18ms, mfu 0.21%\n",
            "iter 470: loss 2.7570, time 323.34ms, mfu 0.21%\n",
            "iter 480: loss 2.7059, time 326.51ms, mfu 0.21%\n",
            "iter 490: loss 2.6785, time 320.30ms, mfu 0.22%\n",
            "iter 500: loss 2.6726, time 318.43ms, mfu 0.22%\n",
            "iter 510: loss 2.6848, time 320.61ms, mfu 0.22%\n",
            "iter 520: loss 2.6057, time 329.01ms, mfu 0.22%\n",
            "iter 530: loss 2.6299, time 325.91ms, mfu 0.22%\n",
            "iter 540: loss 2.5557, time 332.93ms, mfu 0.22%\n",
            "iter 550: loss 2.6116, time 321.59ms, mfu 0.22%\n",
            "iter 560: loss 2.6287, time 325.21ms, mfu 0.22%\n",
            "iter 570: loss 2.5875, time 322.40ms, mfu 0.22%\n",
            "iter 580: loss 2.6353, time 320.47ms, mfu 0.22%\n",
            "iter 590: loss 2.5138, time 324.50ms, mfu 0.22%\n",
            "step 600: train loss 2.5101, val loss 2.5174\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 600: loss 2.5749, time 1595.59ms, mfu 0.20%\n",
            "iter 610: loss 2.5502, time 327.86ms, mfu 0.21%\n",
            "iter 620: loss 2.6193, time 334.26ms, mfu 0.21%\n",
            "iter 630: loss 2.6145, time 329.91ms, mfu 0.21%\n",
            "iter 640: loss 2.5342, time 331.79ms, mfu 0.21%\n",
            "iter 650: loss 2.5939, time 328.21ms, mfu 0.21%\n",
            "iter 660: loss 2.5290, time 337.35ms, mfu 0.21%\n",
            "iter 670: loss 2.4509, time 326.94ms, mfu 0.21%\n",
            "iter 680: loss 2.4706, time 329.75ms, mfu 0.21%\n",
            "iter 690: loss 2.4731, time 333.86ms, mfu 0.21%\n",
            "iter 700: loss 2.5138, time 342.56ms, mfu 0.21%\n",
            "iter 710: loss 2.4674, time 321.00ms, mfu 0.22%\n",
            "iter 720: loss 2.4657, time 323.63ms, mfu 0.22%\n",
            "iter 730: loss 2.3656, time 336.45ms, mfu 0.22%\n",
            "iter 740: loss 2.4332, time 320.47ms, mfu 0.22%\n",
            "iter 750: loss 2.4182, time 328.60ms, mfu 0.22%\n",
            "iter 760: loss 2.4875, time 328.18ms, mfu 0.22%\n",
            "iter 770: loss 2.4662, time 338.35ms, mfu 0.22%\n",
            "iter 780: loss 2.4093, time 325.42ms, mfu 0.22%\n",
            "iter 790: loss 2.4704, time 328.64ms, mfu 0.22%\n",
            "step 800: train loss 2.3740, val loss 2.3842\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 800: loss 2.4142, time 1603.34ms, mfu 0.20%\n",
            "iter 810: loss 2.4008, time 317.56ms, mfu 0.20%\n",
            "iter 820: loss 2.4154, time 325.26ms, mfu 0.21%\n",
            "iter 830: loss 2.4301, time 329.83ms, mfu 0.21%\n",
            "iter 840: loss 2.3720, time 325.37ms, mfu 0.21%\n",
            "iter 850: loss 2.3983, time 319.72ms, mfu 0.21%\n",
            "iter 860: loss 2.4151, time 323.17ms, mfu 0.21%\n",
            "iter 870: loss 2.4233, time 335.31ms, mfu 0.21%\n",
            "iter 880: loss 2.3387, time 327.31ms, mfu 0.21%\n",
            "iter 890: loss 2.3862, time 332.35ms, mfu 0.21%\n",
            "iter 900: loss 2.4129, time 343.21ms, mfu 0.21%\n",
            "iter 910: loss 2.3308, time 326.24ms, mfu 0.22%\n",
            "iter 920: loss 2.3991, time 338.60ms, mfu 0.22%\n",
            "iter 930: loss 2.3925, time 324.65ms, mfu 0.22%\n",
            "iter 940: loss 2.2873, time 325.40ms, mfu 0.22%\n",
            "iter 950: loss 2.3113, time 339.08ms, mfu 0.22%\n",
            "iter 960: loss 2.3389, time 349.91ms, mfu 0.22%\n",
            "iter 970: loss 2.3761, time 327.21ms, mfu 0.22%\n",
            "iter 980: loss 2.4038, time 323.97ms, mfu 0.22%\n",
            "iter 990: loss 2.3023, time 327.76ms, mfu 0.22%\n",
            "step 1000: train loss 2.2510, val loss 2.2734\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1000: loss 2.3305, time 1610.51ms, mfu 0.20%\n",
            "iter 1010: loss 2.2596, time 323.26ms, mfu 0.20%\n",
            "iter 1020: loss 2.3347, time 333.98ms, mfu 0.21%\n",
            "iter 1030: loss 2.3360, time 328.96ms, mfu 0.21%\n",
            "iter 1040: loss 2.4208, time 323.84ms, mfu 0.21%\n",
            "iter 1050: loss 2.3197, time 317.61ms, mfu 0.21%\n",
            "iter 1060: loss 2.2558, time 363.08ms, mfu 0.21%\n",
            "iter 1070: loss 2.3112, time 328.86ms, mfu 0.21%\n",
            "iter 1080: loss 2.2726, time 332.48ms, mfu 0.21%\n",
            "iter 1090: loss 2.3054, time 321.18ms, mfu 0.21%\n",
            "iter 1100: loss 2.1763, time 333.40ms, mfu 0.21%\n",
            "iter 1110: loss 2.1800, time 315.88ms, mfu 0.22%\n",
            "iter 1120: loss 2.2762, time 322.90ms, mfu 0.22%\n",
            "iter 1130: loss 2.2530, time 336.66ms, mfu 0.22%\n",
            "iter 1140: loss 2.2460, time 332.09ms, mfu 0.22%\n",
            "iter 1150: loss 2.3202, time 325.38ms, mfu 0.22%\n",
            "iter 1160: loss 2.3056, time 325.53ms, mfu 0.22%\n",
            "iter 1170: loss 2.2624, time 322.13ms, mfu 0.22%\n",
            "iter 1180: loss 2.2275, time 332.42ms, mfu 0.22%\n",
            "iter 1190: loss 2.2144, time 326.28ms, mfu 0.22%\n",
            "step 1200: train loss 2.1374, val loss 2.1714\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1200: loss 2.2295, time 1620.40ms, mfu 0.20%\n",
            "iter 1210: loss 2.3108, time 319.49ms, mfu 0.20%\n",
            "iter 1220: loss 2.2087, time 325.75ms, mfu 0.21%\n",
            "iter 1230: loss 2.2024, time 336.51ms, mfu 0.21%\n",
            "iter 1240: loss 2.1969, time 324.45ms, mfu 0.21%\n",
            "iter 1250: loss 2.2382, time 322.28ms, mfu 0.21%\n",
            "iter 1260: loss 2.1965, time 324.40ms, mfu 0.21%\n",
            "iter 1270: loss 2.2811, time 328.62ms, mfu 0.21%\n",
            "iter 1280: loss 2.2417, time 332.94ms, mfu 0.21%\n",
            "iter 1290: loss 2.1439, time 331.96ms, mfu 0.21%\n",
            "iter 1300: loss 2.2891, time 319.32ms, mfu 0.22%\n",
            "iter 1310: loss 2.1743, time 327.46ms, mfu 0.22%\n",
            "iter 1320: loss 2.1822, time 319.64ms, mfu 0.22%\n",
            "iter 1330: loss 2.1526, time 321.92ms, mfu 0.22%\n",
            "iter 1340: loss 2.1198, time 319.07ms, mfu 0.22%\n",
            "iter 1350: loss 2.1416, time 331.18ms, mfu 0.22%\n",
            "iter 1360: loss 2.1164, time 321.53ms, mfu 0.22%\n",
            "iter 1370: loss 2.1511, time 322.00ms, mfu 0.22%\n",
            "iter 1380: loss 2.2221, time 335.60ms, mfu 0.22%\n",
            "iter 1390: loss 2.1105, time 347.06ms, mfu 0.22%\n",
            "step 1400: train loss 2.0274, val loss 2.0859\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1400: loss 2.1254, time 1608.06ms, mfu 0.20%\n",
            "iter 1410: loss 2.1132, time 330.44ms, mfu 0.20%\n",
            "iter 1420: loss 2.0948, time 343.17ms, mfu 0.21%\n",
            "iter 1430: loss 2.0793, time 343.63ms, mfu 0.21%\n",
            "iter 1440: loss 2.0751, time 322.65ms, mfu 0.21%\n",
            "iter 1450: loss 2.0717, time 326.04ms, mfu 0.21%\n",
            "iter 1460: loss 2.0944, time 323.74ms, mfu 0.21%\n",
            "iter 1470: loss 2.1403, time 323.13ms, mfu 0.21%\n",
            "iter 1480: loss 2.1809, time 331.32ms, mfu 0.21%\n",
            "iter 1490: loss 2.0702, time 320.22ms, mfu 0.21%\n",
            "iter 1500: loss 2.1262, time 320.07ms, mfu 0.22%\n",
            "iter 1510: loss 2.0627, time 327.32ms, mfu 0.22%\n",
            "iter 1520: loss 2.1002, time 330.10ms, mfu 0.22%\n",
            "iter 1530: loss 2.0968, time 333.98ms, mfu 0.22%\n",
            "iter 1540: loss 2.0480, time 331.74ms, mfu 0.22%\n",
            "iter 1550: loss 2.1232, time 324.20ms, mfu 0.22%\n",
            "iter 1560: loss 2.1158, time 325.26ms, mfu 0.22%\n",
            "iter 1570: loss 2.1001, time 331.52ms, mfu 0.22%\n",
            "iter 1580: loss 2.0431, time 325.57ms, mfu 0.22%\n",
            "iter 1590: loss 2.0638, time 323.12ms, mfu 0.22%\n",
            "step 1600: train loss 1.9308, val loss 2.0041\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1600: loss 2.0686, time 1591.50ms, mfu 0.20%\n",
            "iter 1610: loss 2.0472, time 327.09ms, mfu 0.20%\n",
            "iter 1620: loss 2.0099, time 326.89ms, mfu 0.21%\n",
            "iter 1630: loss 2.0559, time 338.55ms, mfu 0.21%\n",
            "iter 1640: loss 1.9862, time 319.89ms, mfu 0.21%\n",
            "iter 1650: loss 2.0470, time 322.10ms, mfu 0.21%\n",
            "iter 1660: loss 2.0398, time 320.01ms, mfu 0.21%\n",
            "iter 1670: loss 1.9812, time 322.35ms, mfu 0.21%\n",
            "iter 1680: loss 2.0075, time 327.39ms, mfu 0.22%\n",
            "iter 1690: loss 2.0042, time 321.83ms, mfu 0.22%\n",
            "iter 1700: loss 1.9723, time 321.61ms, mfu 0.22%\n",
            "iter 1710: loss 1.9487, time 331.33ms, mfu 0.22%\n",
            "iter 1720: loss 1.9466, time 318.43ms, mfu 0.22%\n",
            "iter 1730: loss 2.0587, time 325.54ms, mfu 0.22%\n",
            "iter 1740: loss 1.9705, time 319.53ms, mfu 0.22%\n",
            "iter 1750: loss 1.9625, time 322.87ms, mfu 0.22%\n",
            "iter 1760: loss 1.9900, time 319.65ms, mfu 0.22%\n",
            "iter 1770: loss 1.9466, time 325.20ms, mfu 0.22%\n",
            "iter 1780: loss 2.0317, time 323.68ms, mfu 0.22%\n",
            "iter 1790: loss 1.9805, time 328.51ms, mfu 0.22%\n",
            "step 1800: train loss 1.8296, val loss 1.9540\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1800: loss 1.9179, time 1575.78ms, mfu 0.20%\n",
            "iter 1810: loss 1.8644, time 323.71ms, mfu 0.21%\n",
            "iter 1820: loss 1.8637, time 318.22ms, mfu 0.21%\n",
            "iter 1830: loss 1.9090, time 337.95ms, mfu 0.21%\n",
            "iter 1840: loss 1.9217, time 335.88ms, mfu 0.21%\n",
            "iter 1850: loss 1.8589, time 330.34ms, mfu 0.21%\n",
            "iter 1860: loss 1.9976, time 327.92ms, mfu 0.21%\n",
            "iter 1870: loss 1.8361, time 315.10ms, mfu 0.21%\n",
            "iter 1880: loss 1.8832, time 330.16ms, mfu 0.22%\n",
            "iter 1890: loss 1.8439, time 328.68ms, mfu 0.22%\n",
            "iter 1900: loss 1.8547, time 333.37ms, mfu 0.22%\n",
            "iter 1910: loss 1.9694, time 333.48ms, mfu 0.22%\n",
            "iter 1920: loss 2.0045, time 318.58ms, mfu 0.22%\n",
            "iter 1930: loss 1.8697, time 330.74ms, mfu 0.22%\n",
            "iter 1940: loss 1.9723, time 319.01ms, mfu 0.22%\n",
            "iter 1950: loss 2.0302, time 316.98ms, mfu 0.22%\n",
            "iter 1960: loss 1.8813, time 319.87ms, mfu 0.22%\n",
            "iter 1970: loss 1.8544, time 325.81ms, mfu 0.22%\n",
            "iter 1980: loss 1.8966, time 314.55ms, mfu 0.22%\n",
            "iter 1990: loss 1.9760, time 318.45ms, mfu 0.22%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 21/32: b128_L4_H8_E128_BS16_MI1000_D10_s21 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS16_MI1000_D10_s21.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 21\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1832, val loss 4.1800\n",
            "iter 0: loss 4.1917, time 2053.39ms, mfu -100.00%\n",
            "iter 10: loss 4.1783, time 330.38ms, mfu 0.44%\n",
            "iter 20: loss 4.1359, time 328.21ms, mfu 0.44%\n",
            "iter 30: loss 4.0807, time 344.11ms, mfu 0.44%\n",
            "iter 40: loss 4.0095, time 334.55ms, mfu 0.44%\n",
            "iter 50: loss 3.9336, time 336.59ms, mfu 0.44%\n",
            "iter 60: loss 3.8485, time 336.38ms, mfu 0.44%\n",
            "iter 70: loss 3.7757, time 328.16ms, mfu 0.44%\n",
            "iter 80: loss 3.7195, time 331.75ms, mfu 0.44%\n",
            "iter 90: loss 3.6819, time 326.32ms, mfu 0.44%\n",
            "iter 100: loss 3.6576, time 332.72ms, mfu 0.44%\n",
            "iter 110: loss 3.6108, time 328.94ms, mfu 0.44%\n",
            "iter 120: loss 3.5865, time 344.27ms, mfu 0.44%\n",
            "iter 130: loss 3.5197, time 328.49ms, mfu 0.44%\n",
            "iter 140: loss 3.4672, time 356.62ms, mfu 0.44%\n",
            "iter 150: loss 3.4455, time 333.99ms, mfu 0.44%\n",
            "iter 160: loss 3.3857, time 347.33ms, mfu 0.43%\n",
            "iter 170: loss 3.3415, time 356.04ms, mfu 0.43%\n",
            "iter 180: loss 3.3117, time 334.09ms, mfu 0.43%\n",
            "iter 190: loss 3.3499, time 331.24ms, mfu 0.43%\n",
            "step 200: train loss 3.2394, val loss 3.2453\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 200: loss 3.2442, time 3155.96ms, mfu 0.40%\n",
            "iter 210: loss 3.2413, time 336.72ms, mfu 0.40%\n",
            "iter 220: loss 3.2414, time 335.75ms, mfu 0.40%\n",
            "iter 230: loss 3.1599, time 335.79ms, mfu 0.41%\n",
            "iter 240: loss 3.1236, time 330.50ms, mfu 0.41%\n",
            "iter 250: loss 3.1008, time 331.29ms, mfu 0.41%\n",
            "iter 260: loss 3.0922, time 339.53ms, mfu 0.41%\n",
            "iter 270: loss 3.0894, time 359.47ms, mfu 0.41%\n",
            "iter 280: loss 3.0786, time 331.38ms, mfu 0.42%\n",
            "iter 290: loss 3.0063, time 337.83ms, mfu 0.42%\n",
            "iter 300: loss 2.9816, time 339.72ms, mfu 0.42%\n",
            "iter 310: loss 2.9878, time 342.47ms, mfu 0.42%\n",
            "iter 320: loss 2.9612, time 342.14ms, mfu 0.42%\n",
            "iter 330: loss 2.9196, time 330.02ms, mfu 0.42%\n",
            "iter 340: loss 2.8982, time 330.98ms, mfu 0.42%\n",
            "iter 350: loss 2.8937, time 332.44ms, mfu 0.43%\n",
            "iter 360: loss 2.8756, time 328.05ms, mfu 0.43%\n",
            "iter 370: loss 2.8215, time 332.04ms, mfu 0.43%\n",
            "iter 380: loss 2.8194, time 349.24ms, mfu 0.43%\n",
            "iter 390: loss 2.7714, time 334.76ms, mfu 0.43%\n",
            "step 400: train loss 2.7590, val loss 2.7668\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 400: loss 2.7574, time 1703.08ms, mfu 0.39%\n",
            "iter 410: loss 2.8227, time 340.09ms, mfu 0.40%\n",
            "iter 420: loss 2.7566, time 335.67ms, mfu 0.40%\n",
            "iter 430: loss 2.7486, time 332.83ms, mfu 0.41%\n",
            "iter 440: loss 2.7235, time 336.48ms, mfu 0.41%\n",
            "iter 450: loss 2.6711, time 352.65ms, mfu 0.41%\n",
            "iter 460: loss 2.7008, time 330.94ms, mfu 0.41%\n",
            "iter 470: loss 2.6420, time 332.59ms, mfu 0.41%\n",
            "iter 480: loss 2.6584, time 333.28ms, mfu 0.42%\n",
            "iter 490: loss 2.6362, time 331.33ms, mfu 0.42%\n",
            "iter 500: loss 2.6128, time 332.74ms, mfu 0.42%\n",
            "iter 510: loss 2.5820, time 331.64ms, mfu 0.42%\n",
            "iter 520: loss 2.5633, time 331.45ms, mfu 0.43%\n",
            "iter 530: loss 2.5443, time 342.36ms, mfu 0.43%\n",
            "iter 540: loss 2.5606, time 341.25ms, mfu 0.43%\n",
            "iter 550: loss 2.5509, time 332.83ms, mfu 0.43%\n",
            "iter 560: loss 2.5193, time 350.78ms, mfu 0.43%\n",
            "iter 570: loss 2.4954, time 331.47ms, mfu 0.43%\n",
            "iter 580: loss 2.4870, time 332.57ms, mfu 0.43%\n",
            "iter 590: loss 2.4525, time 338.59ms, mfu 0.43%\n",
            "step 600: train loss 2.4639, val loss 2.4706\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 600: loss 2.5556, time 1695.72ms, mfu 0.39%\n",
            "iter 610: loss 2.5147, time 331.76ms, mfu 0.40%\n",
            "iter 620: loss 2.4285, time 332.24ms, mfu 0.40%\n",
            "iter 630: loss 2.4909, time 333.45ms, mfu 0.41%\n",
            "iter 640: loss 2.4837, time 334.31ms, mfu 0.41%\n",
            "iter 650: loss 2.4930, time 334.95ms, mfu 0.41%\n",
            "iter 660: loss 2.4323, time 335.93ms, mfu 0.41%\n",
            "iter 670: loss 2.4571, time 327.88ms, mfu 0.42%\n",
            "iter 680: loss 2.3804, time 328.10ms, mfu 0.42%\n",
            "iter 690: loss 2.4408, time 329.93ms, mfu 0.42%\n",
            "iter 700: loss 2.3930, time 336.69ms, mfu 0.42%\n",
            "iter 710: loss 2.3690, time 336.62ms, mfu 0.42%\n",
            "iter 720: loss 2.3982, time 341.92ms, mfu 0.42%\n",
            "iter 730: loss 2.4596, time 338.55ms, mfu 0.43%\n",
            "iter 740: loss 2.3882, time 328.57ms, mfu 0.43%\n",
            "iter 750: loss 2.3749, time 328.74ms, mfu 0.43%\n",
            "iter 760: loss 2.3430, time 327.11ms, mfu 0.43%\n",
            "iter 770: loss 2.3873, time 331.43ms, mfu 0.43%\n",
            "iter 780: loss 2.3144, time 332.00ms, mfu 0.43%\n",
            "iter 790: loss 2.4300, time 327.24ms, mfu 0.43%\n",
            "step 800: train loss 2.3049, val loss 2.3157\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 800: loss 2.3629, time 1740.14ms, mfu 0.40%\n",
            "iter 810: loss 2.3735, time 336.48ms, mfu 0.40%\n",
            "iter 820: loss 2.3422, time 330.07ms, mfu 0.41%\n",
            "iter 830: loss 2.3722, time 333.66ms, mfu 0.41%\n",
            "iter 840: loss 2.3373, time 339.09ms, mfu 0.41%\n",
            "iter 850: loss 2.3417, time 338.80ms, mfu 0.41%\n",
            "iter 860: loss 2.3001, time 338.08ms, mfu 0.42%\n",
            "iter 870: loss 2.3106, time 345.00ms, mfu 0.42%\n",
            "iter 880: loss 2.3048, time 336.42ms, mfu 0.42%\n",
            "iter 890: loss 2.3257, time 337.66ms, mfu 0.42%\n",
            "iter 900: loss 2.2811, time 350.89ms, mfu 0.42%\n",
            "iter 910: loss 2.2818, time 338.12ms, mfu 0.42%\n",
            "iter 920: loss 2.2908, time 340.73ms, mfu 0.42%\n",
            "iter 930: loss 2.3117, time 328.16ms, mfu 0.42%\n",
            "iter 940: loss 2.2583, time 332.65ms, mfu 0.43%\n",
            "iter 950: loss 2.3359, time 332.89ms, mfu 0.43%\n",
            "iter 960: loss 2.2695, time 332.54ms, mfu 0.43%\n",
            "iter 970: loss 2.2857, time 328.17ms, mfu 0.43%\n",
            "iter 980: loss 2.2764, time 333.46ms, mfu 0.43%\n",
            "iter 990: loss 2.2200, time 332.95ms, mfu 0.43%\n",
            "step 1000: train loss 2.1643, val loss 2.1914\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 1000: loss 2.1396, time 1740.22ms, mfu 0.40%\n",
            "\n",
            "=== Experiment 22/32: b128_L4_H8_E128_BS16_MI1000_D20_s22 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS16_MI1000_D20_s22.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 22\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1832, val loss 4.1800\n",
            "iter 0: loss 4.1964, time 2054.67ms, mfu -100.00%\n",
            "iter 10: loss 4.1808, time 336.87ms, mfu 0.43%\n",
            "iter 20: loss 4.1457, time 334.16ms, mfu 0.43%\n",
            "iter 30: loss 4.0987, time 338.71ms, mfu 0.43%\n",
            "iter 40: loss 4.0319, time 342.12ms, mfu 0.43%\n",
            "iter 50: loss 3.9660, time 331.30ms, mfu 0.43%\n",
            "iter 60: loss 3.8750, time 332.39ms, mfu 0.43%\n",
            "iter 70: loss 3.8016, time 356.76ms, mfu 0.43%\n",
            "iter 80: loss 3.7412, time 340.58ms, mfu 0.43%\n",
            "iter 90: loss 3.7045, time 338.46ms, mfu 0.43%\n",
            "iter 100: loss 3.6806, time 334.15ms, mfu 0.43%\n",
            "iter 110: loss 3.6401, time 341.48ms, mfu 0.43%\n",
            "iter 120: loss 3.6240, time 342.35ms, mfu 0.43%\n",
            "iter 130: loss 3.5648, time 342.35ms, mfu 0.43%\n",
            "iter 140: loss 3.5084, time 342.65ms, mfu 0.43%\n",
            "iter 150: loss 3.4833, time 334.14ms, mfu 0.43%\n",
            "iter 160: loss 3.4281, time 331.20ms, mfu 0.43%\n",
            "iter 170: loss 3.3837, time 335.74ms, mfu 0.43%\n",
            "iter 180: loss 3.3516, time 330.38ms, mfu 0.43%\n",
            "iter 190: loss 3.3885, time 335.11ms, mfu 0.43%\n",
            "step 200: train loss 3.2564, val loss 3.2641\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 200: loss 3.2861, time 3322.78ms, mfu 0.39%\n",
            "iter 210: loss 3.2822, time 357.35ms, mfu 0.40%\n",
            "iter 220: loss 3.2806, time 336.88ms, mfu 0.40%\n",
            "iter 230: loss 3.1912, time 334.28ms, mfu 0.40%\n",
            "iter 240: loss 3.1634, time 343.17ms, mfu 0.41%\n",
            "iter 250: loss 3.1302, time 331.08ms, mfu 0.41%\n",
            "iter 260: loss 3.1249, time 339.25ms, mfu 0.41%\n",
            "iter 270: loss 3.1257, time 345.07ms, mfu 0.41%\n",
            "iter 280: loss 3.1083, time 349.37ms, mfu 0.41%\n",
            "iter 290: loss 3.0424, time 347.19ms, mfu 0.41%\n",
            "iter 300: loss 3.0142, time 339.20ms, mfu 0.42%\n",
            "iter 310: loss 3.0187, time 350.65ms, mfu 0.42%\n",
            "iter 320: loss 2.9830, time 337.37ms, mfu 0.42%\n",
            "iter 330: loss 2.9473, time 345.30ms, mfu 0.42%\n",
            "iter 340: loss 2.9227, time 339.48ms, mfu 0.42%\n",
            "iter 350: loss 2.9310, time 344.92ms, mfu 0.42%\n",
            "iter 360: loss 2.9064, time 332.94ms, mfu 0.42%\n",
            "iter 370: loss 2.8467, time 337.95ms, mfu 0.42%\n",
            "iter 380: loss 2.8445, time 343.20ms, mfu 0.42%\n",
            "iter 390: loss 2.7961, time 330.13ms, mfu 0.42%\n",
            "step 400: train loss 2.7746, val loss 2.7810\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 400: loss 2.7872, time 1694.43ms, mfu 0.39%\n",
            "iter 410: loss 2.8519, time 343.44ms, mfu 0.39%\n",
            "iter 420: loss 2.7714, time 343.11ms, mfu 0.40%\n",
            "iter 430: loss 2.7744, time 338.52ms, mfu 0.40%\n",
            "iter 440: loss 2.7597, time 345.77ms, mfu 0.40%\n",
            "iter 450: loss 2.6946, time 346.28ms, mfu 0.40%\n",
            "iter 460: loss 2.7255, time 337.30ms, mfu 0.41%\n",
            "iter 470: loss 2.6733, time 341.94ms, mfu 0.41%\n",
            "iter 480: loss 2.6866, time 346.67ms, mfu 0.41%\n",
            "iter 490: loss 2.6666, time 331.70ms, mfu 0.41%\n",
            "iter 500: loss 2.6332, time 348.15ms, mfu 0.41%\n",
            "iter 510: loss 2.6061, time 334.38ms, mfu 0.42%\n",
            "iter 520: loss 2.5906, time 348.27ms, mfu 0.42%\n",
            "iter 530: loss 2.5768, time 337.50ms, mfu 0.42%\n",
            "iter 540: loss 2.5855, time 333.44ms, mfu 0.42%\n",
            "iter 550: loss 2.5877, time 345.66ms, mfu 0.42%\n",
            "iter 560: loss 2.5531, time 340.89ms, mfu 0.42%\n",
            "iter 570: loss 2.5257, time 352.72ms, mfu 0.42%\n",
            "iter 580: loss 2.5235, time 350.76ms, mfu 0.42%\n",
            "iter 590: loss 2.4811, time 341.16ms, mfu 0.42%\n",
            "step 600: train loss 2.4911, val loss 2.4934\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 600: loss 2.5875, time 1724.54ms, mfu 0.39%\n",
            "iter 610: loss 2.5522, time 337.25ms, mfu 0.39%\n",
            "iter 620: loss 2.4539, time 347.14ms, mfu 0.39%\n",
            "iter 630: loss 2.5286, time 343.07ms, mfu 0.40%\n",
            "iter 640: loss 2.4958, time 340.30ms, mfu 0.40%\n",
            "iter 650: loss 2.5250, time 341.08ms, mfu 0.40%\n",
            "iter 660: loss 2.4676, time 331.41ms, mfu 0.41%\n",
            "iter 670: loss 2.4964, time 334.20ms, mfu 0.41%\n",
            "iter 680: loss 2.4090, time 335.33ms, mfu 0.41%\n",
            "iter 690: loss 2.4855, time 335.21ms, mfu 0.42%\n",
            "iter 700: loss 2.4371, time 332.05ms, mfu 0.42%\n",
            "iter 710: loss 2.3966, time 340.07ms, mfu 0.42%\n",
            "iter 720: loss 2.4476, time 344.01ms, mfu 0.42%\n",
            "iter 730: loss 2.5038, time 352.41ms, mfu 0.42%\n",
            "iter 740: loss 2.4160, time 335.15ms, mfu 0.42%\n",
            "iter 750: loss 2.4082, time 334.53ms, mfu 0.42%\n",
            "iter 760: loss 2.3747, time 336.02ms, mfu 0.42%\n",
            "iter 770: loss 2.4285, time 334.24ms, mfu 0.42%\n",
            "iter 780: loss 2.3582, time 332.34ms, mfu 0.43%\n",
            "iter 790: loss 2.4713, time 332.24ms, mfu 0.43%\n",
            "step 800: train loss 2.3372, val loss 2.3456\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 800: loss 2.4006, time 1740.39ms, mfu 0.39%\n",
            "iter 810: loss 2.4092, time 335.91ms, mfu 0.40%\n",
            "iter 820: loss 2.3930, time 338.66ms, mfu 0.40%\n",
            "iter 830: loss 2.4161, time 340.09ms, mfu 0.40%\n",
            "iter 840: loss 2.3832, time 335.96ms, mfu 0.41%\n",
            "iter 850: loss 2.3962, time 344.12ms, mfu 0.41%\n",
            "iter 860: loss 2.3496, time 346.00ms, mfu 0.41%\n",
            "iter 870: loss 2.3699, time 377.11ms, mfu 0.41%\n",
            "iter 880: loss 2.3465, time 337.98ms, mfu 0.41%\n",
            "iter 890: loss 2.3659, time 346.42ms, mfu 0.41%\n",
            "iter 900: loss 2.3287, time 342.32ms, mfu 0.41%\n",
            "iter 910: loss 2.3398, time 344.51ms, mfu 0.41%\n",
            "iter 920: loss 2.3528, time 340.73ms, mfu 0.42%\n",
            "iter 930: loss 2.3690, time 342.27ms, mfu 0.42%\n",
            "iter 940: loss 2.3045, time 349.52ms, mfu 0.42%\n",
            "iter 950: loss 2.3882, time 347.10ms, mfu 0.42%\n",
            "iter 960: loss 2.3284, time 338.72ms, mfu 0.42%\n",
            "iter 970: loss 2.3446, time 332.27ms, mfu 0.42%\n",
            "iter 980: loss 2.3542, time 348.35ms, mfu 0.42%\n",
            "iter 990: loss 2.2987, time 332.76ms, mfu 0.42%\n",
            "step 1000: train loss 2.2198, val loss 2.2400\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 1000: loss 2.2077, time 1708.84ms, mfu 0.39%\n",
            "\n",
            "=== Experiment 23/32: b128_L4_H8_E128_BS16_MI2000_D10_s23 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS16_MI2000_D10_s23.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 23\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1832, val loss 4.1800\n",
            "iter 0: loss 4.1917, time 2021.74ms, mfu -100.00%\n",
            "iter 10: loss 4.1783, time 337.96ms, mfu 0.43%\n",
            "iter 20: loss 4.1359, time 332.30ms, mfu 0.43%\n",
            "iter 30: loss 4.0807, time 329.60ms, mfu 0.43%\n",
            "iter 40: loss 4.0095, time 330.07ms, mfu 0.43%\n",
            "iter 50: loss 3.9336, time 328.45ms, mfu 0.44%\n",
            "iter 60: loss 3.8485, time 330.08ms, mfu 0.44%\n",
            "iter 70: loss 3.7757, time 332.45ms, mfu 0.44%\n",
            "iter 80: loss 3.7195, time 334.13ms, mfu 0.44%\n",
            "iter 90: loss 3.6819, time 335.04ms, mfu 0.44%\n",
            "iter 100: loss 3.6576, time 342.30ms, mfu 0.44%\n",
            "iter 110: loss 3.6108, time 335.72ms, mfu 0.44%\n",
            "iter 120: loss 3.5865, time 331.85ms, mfu 0.44%\n",
            "iter 130: loss 3.5197, time 333.68ms, mfu 0.44%\n",
            "iter 140: loss 3.4672, time 331.60ms, mfu 0.44%\n",
            "iter 150: loss 3.4455, time 332.41ms, mfu 0.44%\n",
            "iter 160: loss 3.3857, time 336.36ms, mfu 0.44%\n",
            "iter 170: loss 3.3415, time 368.19ms, mfu 0.43%\n",
            "iter 180: loss 3.3117, time 344.05ms, mfu 0.43%\n",
            "iter 190: loss 3.3499, time 341.36ms, mfu 0.43%\n",
            "step 200: train loss 3.2394, val loss 3.2453\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 200: loss 3.2442, time 3211.48ms, mfu 0.39%\n",
            "iter 210: loss 3.2413, time 356.84ms, mfu 0.39%\n",
            "iter 220: loss 3.2414, time 331.57ms, mfu 0.40%\n",
            "iter 230: loss 3.1599, time 353.83ms, mfu 0.40%\n",
            "iter 240: loss 3.1236, time 341.91ms, mfu 0.40%\n",
            "iter 250: loss 3.1008, time 345.63ms, mfu 0.41%\n",
            "iter 260: loss 3.0922, time 361.55ms, mfu 0.40%\n",
            "iter 270: loss 3.0894, time 349.73ms, mfu 0.41%\n",
            "iter 280: loss 3.0786, time 333.00ms, mfu 0.41%\n",
            "iter 290: loss 3.0063, time 341.14ms, mfu 0.41%\n",
            "iter 300: loss 2.9816, time 343.99ms, mfu 0.41%\n",
            "iter 310: loss 2.9878, time 337.71ms, mfu 0.41%\n",
            "iter 320: loss 2.9612, time 336.24ms, mfu 0.42%\n",
            "iter 330: loss 2.9196, time 336.64ms, mfu 0.42%\n",
            "iter 340: loss 2.8982, time 334.75ms, mfu 0.42%\n",
            "iter 350: loss 2.8937, time 331.98ms, mfu 0.42%\n",
            "iter 360: loss 2.8756, time 345.98ms, mfu 0.42%\n",
            "iter 370: loss 2.8215, time 336.56ms, mfu 0.42%\n",
            "iter 380: loss 2.8194, time 340.36ms, mfu 0.42%\n",
            "iter 390: loss 2.7714, time 333.76ms, mfu 0.43%\n",
            "step 400: train loss 2.7590, val loss 2.7668\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 400: loss 2.7574, time 1715.02ms, mfu 0.39%\n",
            "iter 410: loss 2.8227, time 336.02ms, mfu 0.40%\n",
            "iter 420: loss 2.7566, time 332.67ms, mfu 0.40%\n",
            "iter 430: loss 2.7486, time 344.88ms, mfu 0.40%\n",
            "iter 440: loss 2.7235, time 343.67ms, mfu 0.40%\n",
            "iter 450: loss 2.6711, time 339.07ms, mfu 0.41%\n",
            "iter 460: loss 2.7008, time 342.42ms, mfu 0.41%\n",
            "iter 470: loss 2.6420, time 335.33ms, mfu 0.41%\n",
            "iter 480: loss 2.6584, time 343.39ms, mfu 0.41%\n",
            "iter 490: loss 2.6362, time 332.48ms, mfu 0.42%\n",
            "iter 500: loss 2.6128, time 350.85ms, mfu 0.42%\n",
            "iter 510: loss 2.5820, time 336.41ms, mfu 0.42%\n",
            "iter 520: loss 2.5633, time 339.75ms, mfu 0.42%\n",
            "iter 530: loss 2.5443, time 336.33ms, mfu 0.42%\n",
            "iter 540: loss 2.5606, time 331.96ms, mfu 0.42%\n",
            "iter 550: loss 2.5509, time 340.05ms, mfu 0.42%\n",
            "iter 560: loss 2.5193, time 348.88ms, mfu 0.42%\n",
            "iter 570: loss 2.4954, time 334.74ms, mfu 0.42%\n",
            "iter 580: loss 2.4870, time 347.80ms, mfu 0.42%\n",
            "iter 590: loss 2.4525, time 338.61ms, mfu 0.42%\n",
            "step 600: train loss 2.4639, val loss 2.4706\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 600: loss 2.5556, time 1693.02ms, mfu 0.39%\n",
            "iter 610: loss 2.5147, time 328.84ms, mfu 0.40%\n",
            "iter 620: loss 2.4285, time 341.11ms, mfu 0.40%\n",
            "iter 630: loss 2.4909, time 341.19ms, mfu 0.40%\n",
            "iter 640: loss 2.4837, time 339.54ms, mfu 0.40%\n",
            "iter 650: loss 2.4930, time 345.41ms, mfu 0.41%\n",
            "iter 660: loss 2.4323, time 338.43ms, mfu 0.41%\n",
            "iter 670: loss 2.4571, time 334.56ms, mfu 0.41%\n",
            "iter 680: loss 2.3804, time 350.25ms, mfu 0.41%\n",
            "iter 690: loss 2.4408, time 343.61ms, mfu 0.41%\n",
            "iter 700: loss 2.3930, time 339.18ms, mfu 0.42%\n",
            "iter 710: loss 2.3690, time 330.45ms, mfu 0.42%\n",
            "iter 720: loss 2.3982, time 332.60ms, mfu 0.42%\n",
            "iter 730: loss 2.4596, time 332.24ms, mfu 0.42%\n",
            "iter 740: loss 2.3882, time 330.89ms, mfu 0.42%\n",
            "iter 750: loss 2.3749, time 330.81ms, mfu 0.43%\n",
            "iter 760: loss 2.3430, time 343.55ms, mfu 0.43%\n",
            "iter 770: loss 2.3873, time 330.20ms, mfu 0.43%\n",
            "iter 780: loss 2.3144, time 327.89ms, mfu 0.43%\n",
            "iter 790: loss 2.4300, time 333.55ms, mfu 0.43%\n",
            "step 800: train loss 2.3049, val loss 2.3157\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 800: loss 2.3629, time 1706.77ms, mfu 0.40%\n",
            "iter 810: loss 2.3735, time 333.25ms, mfu 0.40%\n",
            "iter 820: loss 2.3422, time 328.99ms, mfu 0.40%\n",
            "iter 830: loss 2.3722, time 345.77ms, mfu 0.41%\n",
            "iter 840: loss 2.3373, time 341.43ms, mfu 0.41%\n",
            "iter 850: loss 2.3417, time 331.83ms, mfu 0.41%\n",
            "iter 860: loss 2.3001, time 331.19ms, mfu 0.41%\n",
            "iter 870: loss 2.3106, time 333.86ms, mfu 0.42%\n",
            "iter 880: loss 2.3048, time 329.42ms, mfu 0.42%\n",
            "iter 890: loss 2.3257, time 330.42ms, mfu 0.42%\n",
            "iter 900: loss 2.2811, time 332.88ms, mfu 0.42%\n",
            "iter 910: loss 2.2818, time 331.18ms, mfu 0.43%\n",
            "iter 920: loss 2.2908, time 331.82ms, mfu 0.43%\n",
            "iter 930: loss 2.3117, time 336.48ms, mfu 0.43%\n",
            "iter 940: loss 2.2583, time 335.57ms, mfu 0.43%\n",
            "iter 950: loss 2.3359, time 330.74ms, mfu 0.43%\n",
            "iter 960: loss 2.2695, time 339.62ms, mfu 0.43%\n",
            "iter 970: loss 2.2857, time 333.34ms, mfu 0.43%\n",
            "iter 980: loss 2.2764, time 337.44ms, mfu 0.43%\n",
            "iter 990: loss 2.2200, time 334.20ms, mfu 0.43%\n",
            "step 1000: train loss 2.1643, val loss 2.1914\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1000: loss 2.1396, time 1696.16ms, mfu 0.40%\n",
            "iter 1010: loss 2.2687, time 343.17ms, mfu 0.40%\n",
            "iter 1020: loss 2.1977, time 339.62ms, mfu 0.40%\n",
            "iter 1030: loss 2.2151, time 344.02ms, mfu 0.40%\n",
            "iter 1040: loss 2.1665, time 331.21ms, mfu 0.41%\n",
            "iter 1050: loss 2.2190, time 332.97ms, mfu 0.41%\n",
            "iter 1060: loss 2.1583, time 330.70ms, mfu 0.41%\n",
            "iter 1070: loss 2.1822, time 327.53ms, mfu 0.42%\n",
            "iter 1080: loss 2.1934, time 344.74ms, mfu 0.42%\n",
            "iter 1090: loss 2.1363, time 339.79ms, mfu 0.42%\n",
            "iter 1100: loss 2.1675, time 338.67ms, mfu 0.42%\n",
            "iter 1110: loss 2.1450, time 340.37ms, mfu 0.42%\n",
            "iter 1120: loss 2.2075, time 328.82ms, mfu 0.42%\n",
            "iter 1130: loss 2.1030, time 330.75ms, mfu 0.43%\n",
            "iter 1140: loss 2.1241, time 330.46ms, mfu 0.43%\n",
            "iter 1150: loss 2.0896, time 339.22ms, mfu 0.43%\n",
            "iter 1160: loss 2.1119, time 336.82ms, mfu 0.43%\n",
            "iter 1170: loss 2.1792, time 330.16ms, mfu 0.43%\n",
            "iter 1180: loss 2.1200, time 329.58ms, mfu 0.43%\n",
            "iter 1190: loss 2.0781, time 328.71ms, mfu 0.43%\n",
            "step 1200: train loss 1.9918, val loss 2.0636\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1200: loss 2.1884, time 1713.31ms, mfu 0.40%\n",
            "iter 1210: loss 2.0745, time 331.84ms, mfu 0.40%\n",
            "iter 1220: loss 2.0978, time 347.41ms, mfu 0.40%\n",
            "iter 1230: loss 2.0069, time 331.57ms, mfu 0.41%\n",
            "iter 1240: loss 2.0395, time 330.13ms, mfu 0.41%\n",
            "iter 1250: loss 2.1011, time 326.71ms, mfu 0.41%\n",
            "iter 1260: loss 2.1134, time 338.60ms, mfu 0.42%\n",
            "iter 1270: loss 2.0620, time 339.59ms, mfu 0.42%\n",
            "iter 1280: loss 2.0519, time 338.48ms, mfu 0.42%\n",
            "iter 1290: loss 2.0310, time 342.91ms, mfu 0.42%\n",
            "iter 1300: loss 2.0593, time 339.01ms, mfu 0.42%\n",
            "iter 1310: loss 2.0442, time 339.39ms, mfu 0.42%\n",
            "iter 1320: loss 2.0823, time 333.60ms, mfu 0.42%\n",
            "iter 1330: loss 2.0461, time 337.36ms, mfu 0.42%\n",
            "iter 1340: loss 2.0007, time 330.50ms, mfu 0.43%\n",
            "iter 1350: loss 1.9624, time 354.41ms, mfu 0.42%\n",
            "iter 1360: loss 1.9535, time 331.80ms, mfu 0.43%\n",
            "iter 1370: loss 1.9672, time 333.14ms, mfu 0.43%\n",
            "iter 1380: loss 1.9568, time 336.95ms, mfu 0.43%\n",
            "iter 1390: loss 1.9884, time 332.99ms, mfu 0.43%\n",
            "step 1400: train loss 1.8635, val loss 1.9725\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1400: loss 2.0177, time 1700.16ms, mfu 0.39%\n",
            "iter 1410: loss 1.9025, time 338.58ms, mfu 0.40%\n",
            "iter 1420: loss 1.9286, time 340.09ms, mfu 0.40%\n",
            "iter 1430: loss 1.9926, time 337.08ms, mfu 0.40%\n",
            "iter 1440: loss 1.9449, time 332.09ms, mfu 0.41%\n",
            "iter 1450: loss 1.9721, time 331.97ms, mfu 0.41%\n",
            "iter 1460: loss 1.8992, time 329.16ms, mfu 0.41%\n",
            "iter 1470: loss 1.9109, time 330.25ms, mfu 0.42%\n",
            "iter 1480: loss 1.9245, time 335.33ms, mfu 0.42%\n",
            "iter 1490: loss 1.9422, time 326.88ms, mfu 0.42%\n",
            "iter 1500: loss 1.8993, time 359.19ms, mfu 0.42%\n",
            "iter 1510: loss 1.9166, time 353.71ms, mfu 0.42%\n",
            "iter 1520: loss 1.9257, time 326.91ms, mfu 0.42%\n",
            "iter 1530: loss 1.9055, time 328.47ms, mfu 0.42%\n",
            "iter 1540: loss 1.8923, time 350.88ms, mfu 0.42%\n",
            "iter 1550: loss 1.8063, time 327.18ms, mfu 0.43%\n",
            "iter 1560: loss 1.8302, time 332.06ms, mfu 0.43%\n",
            "iter 1570: loss 1.9377, time 328.16ms, mfu 0.43%\n",
            "iter 1580: loss 1.9124, time 334.22ms, mfu 0.43%\n",
            "iter 1590: loss 1.8022, time 334.14ms, mfu 0.43%\n",
            "step 1600: train loss 1.7487, val loss 1.8829\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1600: loss 1.8675, time 1714.19ms, mfu 0.40%\n",
            "iter 1610: loss 1.8795, time 346.27ms, mfu 0.40%\n",
            "iter 1620: loss 1.9092, time 343.48ms, mfu 0.40%\n",
            "iter 1630: loss 1.8908, time 342.37ms, mfu 0.40%\n",
            "iter 1640: loss 1.8226, time 342.94ms, mfu 0.41%\n",
            "iter 1650: loss 1.8255, time 341.24ms, mfu 0.41%\n",
            "iter 1660: loss 1.8541, time 332.48ms, mfu 0.41%\n",
            "iter 1670: loss 1.8674, time 331.05ms, mfu 0.41%\n",
            "iter 1680: loss 1.7958, time 332.68ms, mfu 0.42%\n",
            "iter 1690: loss 1.7999, time 335.51ms, mfu 0.42%\n",
            "iter 1700: loss 1.7330, time 327.27ms, mfu 0.42%\n",
            "iter 1710: loss 1.8516, time 334.66ms, mfu 0.42%\n",
            "iter 1720: loss 1.7636, time 328.50ms, mfu 0.43%\n",
            "iter 1730: loss 1.7561, time 335.80ms, mfu 0.43%\n",
            "iter 1740: loss 1.8570, time 329.82ms, mfu 0.43%\n",
            "iter 1750: loss 1.7389, time 331.01ms, mfu 0.43%\n",
            "iter 1760: loss 1.8264, time 331.80ms, mfu 0.43%\n",
            "iter 1770: loss 1.7579, time 344.68ms, mfu 0.43%\n",
            "iter 1780: loss 1.8083, time 329.88ms, mfu 0.43%\n",
            "iter 1790: loss 1.7073, time 330.76ms, mfu 0.43%\n",
            "step 1800: train loss 1.6547, val loss 1.8261\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1800: loss 1.7849, time 1707.20ms, mfu 0.40%\n",
            "iter 1810: loss 1.7644, time 329.04ms, mfu 0.40%\n",
            "iter 1820: loss 1.6537, time 335.99ms, mfu 0.41%\n",
            "iter 1830: loss 1.7740, time 331.17ms, mfu 0.41%\n",
            "iter 1840: loss 1.7837, time 329.28ms, mfu 0.41%\n",
            "iter 1850: loss 1.7530, time 340.51ms, mfu 0.41%\n",
            "iter 1860: loss 1.8161, time 329.79ms, mfu 0.42%\n",
            "iter 1870: loss 1.7336, time 342.04ms, mfu 0.42%\n",
            "iter 1880: loss 1.7914, time 332.44ms, mfu 0.42%\n",
            "iter 1890: loss 1.6731, time 330.69ms, mfu 0.42%\n",
            "iter 1900: loss 1.7814, time 331.73ms, mfu 0.42%\n",
            "iter 1910: loss 1.6786, time 331.23ms, mfu 0.43%\n",
            "iter 1920: loss 1.7166, time 327.09ms, mfu 0.43%\n",
            "iter 1930: loss 1.6083, time 332.00ms, mfu 0.43%\n",
            "iter 1940: loss 1.6739, time 329.33ms, mfu 0.43%\n",
            "iter 1950: loss 1.7192, time 329.84ms, mfu 0.43%\n",
            "iter 1960: loss 1.7333, time 330.00ms, mfu 0.43%\n",
            "iter 1970: loss 1.6936, time 331.89ms, mfu 0.43%\n",
            "iter 1980: loss 1.7049, time 334.37ms, mfu 0.43%\n",
            "iter 1990: loss 1.7689, time 329.15ms, mfu 0.43%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 24/32: b128_L4_H8_E128_BS16_MI2000_D20_s24 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E128_BS16_MI2000_D20_s24.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 24\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "num decayed parameter tensors: 18, with 811,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1832, val loss 4.1800\n",
            "iter 0: loss 4.1964, time 2037.91ms, mfu -100.00%\n",
            "iter 10: loss 4.1808, time 334.23ms, mfu 0.44%\n",
            "iter 20: loss 4.1457, time 341.50ms, mfu 0.44%\n",
            "iter 30: loss 4.0987, time 334.97ms, mfu 0.44%\n",
            "iter 40: loss 4.0319, time 332.66ms, mfu 0.44%\n",
            "iter 50: loss 3.9660, time 333.97ms, mfu 0.44%\n",
            "iter 60: loss 3.8750, time 328.00ms, mfu 0.44%\n",
            "iter 70: loss 3.8016, time 340.17ms, mfu 0.44%\n",
            "iter 80: loss 3.7412, time 327.44ms, mfu 0.44%\n",
            "iter 90: loss 3.7045, time 337.89ms, mfu 0.44%\n",
            "iter 100: loss 3.6806, time 330.39ms, mfu 0.44%\n",
            "iter 110: loss 3.6401, time 338.66ms, mfu 0.44%\n",
            "iter 120: loss 3.6240, time 330.88ms, mfu 0.44%\n",
            "iter 130: loss 3.5648, time 345.46ms, mfu 0.44%\n",
            "iter 140: loss 3.5084, time 341.28ms, mfu 0.44%\n",
            "iter 150: loss 3.4833, time 335.25ms, mfu 0.44%\n",
            "iter 160: loss 3.4281, time 351.68ms, mfu 0.43%\n",
            "iter 170: loss 3.3837, time 342.38ms, mfu 0.43%\n",
            "iter 180: loss 3.3516, time 341.95ms, mfu 0.43%\n",
            "iter 190: loss 3.3885, time 331.80ms, mfu 0.43%\n",
            "step 200: train loss 3.2564, val loss 3.2641\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 200: loss 3.2861, time 1709.02ms, mfu 0.40%\n",
            "iter 210: loss 3.2822, time 332.92ms, mfu 0.40%\n",
            "iter 220: loss 3.2806, time 340.55ms, mfu 0.40%\n",
            "iter 230: loss 3.1912, time 341.25ms, mfu 0.41%\n",
            "iter 240: loss 3.1634, time 328.50ms, mfu 0.41%\n",
            "iter 250: loss 3.1302, time 334.96ms, mfu 0.41%\n",
            "iter 260: loss 3.1249, time 334.73ms, mfu 0.42%\n",
            "iter 270: loss 3.1257, time 340.24ms, mfu 0.42%\n",
            "iter 280: loss 3.1083, time 340.26ms, mfu 0.42%\n",
            "iter 290: loss 3.0424, time 339.62ms, mfu 0.42%\n",
            "iter 300: loss 3.0142, time 337.30ms, mfu 0.42%\n",
            "iter 310: loss 3.0187, time 340.46ms, mfu 0.42%\n",
            "iter 320: loss 2.9830, time 342.70ms, mfu 0.42%\n",
            "iter 330: loss 2.9473, time 329.95ms, mfu 0.42%\n",
            "iter 340: loss 2.9227, time 356.16ms, mfu 0.42%\n",
            "iter 350: loss 2.9310, time 341.08ms, mfu 0.42%\n",
            "iter 360: loss 2.9064, time 351.79ms, mfu 0.42%\n",
            "iter 370: loss 2.8467, time 343.50ms, mfu 0.42%\n",
            "iter 380: loss 2.8445, time 333.19ms, mfu 0.42%\n",
            "iter 390: loss 2.7961, time 332.64ms, mfu 0.43%\n",
            "step 400: train loss 2.7746, val loss 2.7810\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 400: loss 2.7872, time 1714.68ms, mfu 0.39%\n",
            "iter 410: loss 2.8519, time 330.77ms, mfu 0.40%\n",
            "iter 420: loss 2.7714, time 340.21ms, mfu 0.40%\n",
            "iter 430: loss 2.7744, time 334.50ms, mfu 0.40%\n",
            "iter 440: loss 2.7597, time 336.44ms, mfu 0.41%\n",
            "iter 450: loss 2.6946, time 333.99ms, mfu 0.41%\n",
            "iter 460: loss 2.7255, time 334.05ms, mfu 0.41%\n",
            "iter 470: loss 2.6733, time 333.01ms, mfu 0.42%\n",
            "iter 480: loss 2.6866, time 333.20ms, mfu 0.42%\n",
            "iter 490: loss 2.6666, time 334.01ms, mfu 0.42%\n",
            "iter 500: loss 2.6332, time 341.75ms, mfu 0.42%\n",
            "iter 510: loss 2.6061, time 341.56ms, mfu 0.42%\n",
            "iter 520: loss 2.5906, time 342.70ms, mfu 0.42%\n",
            "iter 530: loss 2.5768, time 332.98ms, mfu 0.42%\n",
            "iter 540: loss 2.5855, time 334.94ms, mfu 0.42%\n",
            "iter 550: loss 2.5877, time 365.07ms, mfu 0.42%\n",
            "iter 560: loss 2.5531, time 332.99ms, mfu 0.42%\n",
            "iter 570: loss 2.5257, time 333.50ms, mfu 0.43%\n",
            "iter 580: loss 2.5235, time 348.34ms, mfu 0.42%\n",
            "iter 590: loss 2.4811, time 331.83ms, mfu 0.43%\n",
            "step 600: train loss 2.4911, val loss 2.4934\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 600: loss 2.5875, time 1710.89ms, mfu 0.39%\n",
            "iter 610: loss 2.5522, time 334.45ms, mfu 0.40%\n",
            "iter 620: loss 2.4539, time 334.58ms, mfu 0.40%\n",
            "iter 630: loss 2.5286, time 350.82ms, mfu 0.40%\n",
            "iter 640: loss 2.4958, time 347.62ms, mfu 0.40%\n",
            "iter 650: loss 2.5250, time 354.10ms, mfu 0.40%\n",
            "iter 660: loss 2.4676, time 343.40ms, mfu 0.41%\n",
            "iter 670: loss 2.4964, time 340.87ms, mfu 0.41%\n",
            "iter 680: loss 2.4090, time 336.60ms, mfu 0.41%\n",
            "iter 690: loss 2.4855, time 341.30ms, mfu 0.41%\n",
            "iter 700: loss 2.4371, time 332.13ms, mfu 0.42%\n",
            "iter 710: loss 2.3966, time 334.85ms, mfu 0.42%\n",
            "iter 720: loss 2.4476, time 343.65ms, mfu 0.42%\n",
            "iter 730: loss 2.5038, time 342.09ms, mfu 0.42%\n",
            "iter 740: loss 2.4160, time 333.78ms, mfu 0.42%\n",
            "iter 750: loss 2.4082, time 335.43ms, mfu 0.42%\n",
            "iter 760: loss 2.3747, time 354.09ms, mfu 0.42%\n",
            "iter 770: loss 2.4285, time 345.29ms, mfu 0.42%\n",
            "iter 780: loss 2.3582, time 337.50ms, mfu 0.42%\n",
            "iter 790: loss 2.4713, time 346.38ms, mfu 0.42%\n",
            "step 800: train loss 2.3372, val loss 2.3456\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 800: loss 2.4006, time 1701.96ms, mfu 0.39%\n",
            "iter 810: loss 2.4092, time 335.23ms, mfu 0.39%\n",
            "iter 820: loss 2.3930, time 345.32ms, mfu 0.40%\n",
            "iter 830: loss 2.4161, time 338.44ms, mfu 0.40%\n",
            "iter 840: loss 2.3832, time 333.59ms, mfu 0.40%\n",
            "iter 850: loss 2.3962, time 334.53ms, mfu 0.41%\n",
            "iter 860: loss 2.3496, time 333.27ms, mfu 0.41%\n",
            "iter 870: loss 2.3699, time 331.10ms, mfu 0.41%\n",
            "iter 880: loss 2.3465, time 342.57ms, mfu 0.41%\n",
            "iter 890: loss 2.3659, time 336.20ms, mfu 0.42%\n",
            "iter 900: loss 2.3287, time 339.62ms, mfu 0.42%\n",
            "iter 910: loss 2.3398, time 342.59ms, mfu 0.42%\n",
            "iter 920: loss 2.3528, time 340.42ms, mfu 0.42%\n",
            "iter 930: loss 2.3690, time 339.67ms, mfu 0.42%\n",
            "iter 940: loss 2.3045, time 336.94ms, mfu 0.42%\n",
            "iter 950: loss 2.3882, time 336.02ms, mfu 0.42%\n",
            "iter 960: loss 2.3284, time 330.33ms, mfu 0.43%\n",
            "iter 970: loss 2.3446, time 337.37ms, mfu 0.43%\n",
            "iter 980: loss 2.3542, time 340.58ms, mfu 0.43%\n",
            "iter 990: loss 2.2987, time 340.47ms, mfu 0.43%\n",
            "step 1000: train loss 2.2198, val loss 2.2400\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1000: loss 2.2077, time 1741.99ms, mfu 0.39%\n",
            "iter 1010: loss 2.3201, time 336.87ms, mfu 0.40%\n",
            "iter 1020: loss 2.2806, time 332.18ms, mfu 0.40%\n",
            "iter 1030: loss 2.2750, time 338.80ms, mfu 0.40%\n",
            "iter 1040: loss 2.2381, time 338.01ms, mfu 0.41%\n",
            "iter 1050: loss 2.2870, time 334.91ms, mfu 0.41%\n",
            "iter 1060: loss 2.2380, time 335.51ms, mfu 0.41%\n",
            "iter 1070: loss 2.2612, time 342.71ms, mfu 0.41%\n",
            "iter 1080: loss 2.2791, time 336.25ms, mfu 0.42%\n",
            "iter 1090: loss 2.1977, time 336.81ms, mfu 0.42%\n",
            "iter 1100: loss 2.2450, time 338.68ms, mfu 0.42%\n",
            "iter 1110: loss 2.2297, time 338.28ms, mfu 0.42%\n",
            "iter 1120: loss 2.2917, time 338.62ms, mfu 0.42%\n",
            "iter 1130: loss 2.1805, time 335.58ms, mfu 0.42%\n",
            "iter 1140: loss 2.2103, time 338.82ms, mfu 0.42%\n",
            "iter 1150: loss 2.1889, time 344.07ms, mfu 0.42%\n",
            "iter 1160: loss 2.2297, time 337.57ms, mfu 0.42%\n",
            "iter 1170: loss 2.2819, time 339.38ms, mfu 0.43%\n",
            "iter 1180: loss 2.2195, time 338.20ms, mfu 0.43%\n",
            "iter 1190: loss 2.1832, time 335.04ms, mfu 0.43%\n",
            "step 1200: train loss 2.0768, val loss 2.1282\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1200: loss 2.2711, time 1704.76ms, mfu 0.39%\n",
            "iter 1210: loss 2.1664, time 332.68ms, mfu 0.40%\n",
            "iter 1220: loss 2.2093, time 330.60ms, mfu 0.40%\n",
            "iter 1230: loss 2.1043, time 335.34ms, mfu 0.41%\n",
            "iter 1240: loss 2.1375, time 334.05ms, mfu 0.41%\n",
            "iter 1250: loss 2.1824, time 338.89ms, mfu 0.41%\n",
            "iter 1260: loss 2.2113, time 334.80ms, mfu 0.41%\n",
            "iter 1270: loss 2.1675, time 336.16ms, mfu 0.42%\n",
            "iter 1280: loss 2.1477, time 341.78ms, mfu 0.42%\n",
            "iter 1290: loss 2.1493, time 338.98ms, mfu 0.42%\n",
            "iter 1300: loss 2.1722, time 342.16ms, mfu 0.42%\n",
            "iter 1310: loss 2.1556, time 341.15ms, mfu 0.42%\n",
            "iter 1320: loss 2.2018, time 343.83ms, mfu 0.42%\n",
            "iter 1330: loss 2.1502, time 341.12ms, mfu 0.42%\n",
            "iter 1340: loss 2.1158, time 344.60ms, mfu 0.42%\n",
            "iter 1350: loss 2.0633, time 342.29ms, mfu 0.42%\n",
            "iter 1360: loss 2.0821, time 344.78ms, mfu 0.42%\n",
            "iter 1370: loss 2.0825, time 330.63ms, mfu 0.42%\n",
            "iter 1380: loss 2.0932, time 330.46ms, mfu 0.43%\n",
            "iter 1390: loss 2.1125, time 347.04ms, mfu 0.43%\n",
            "step 1400: train loss 1.9594, val loss 2.0350\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1400: loss 2.1239, time 1734.22ms, mfu 0.39%\n",
            "iter 1410: loss 2.0431, time 347.09ms, mfu 0.39%\n",
            "iter 1420: loss 2.0585, time 345.67ms, mfu 0.40%\n",
            "iter 1430: loss 2.1321, time 331.83ms, mfu 0.40%\n",
            "iter 1440: loss 2.0526, time 330.96ms, mfu 0.41%\n",
            "iter 1450: loss 2.0924, time 333.51ms, mfu 0.41%\n",
            "iter 1460: loss 2.0186, time 349.83ms, mfu 0.41%\n",
            "iter 1470: loss 2.0382, time 340.64ms, mfu 0.41%\n",
            "iter 1480: loss 2.0502, time 345.24ms, mfu 0.41%\n",
            "iter 1490: loss 2.0783, time 341.99ms, mfu 0.41%\n",
            "iter 1500: loss 2.0500, time 328.41ms, mfu 0.42%\n",
            "iter 1510: loss 2.0440, time 329.54ms, mfu 0.42%\n",
            "iter 1520: loss 2.0525, time 331.39ms, mfu 0.42%\n",
            "iter 1530: loss 2.0323, time 335.37ms, mfu 0.42%\n",
            "iter 1540: loss 2.0136, time 330.54ms, mfu 0.42%\n",
            "iter 1550: loss 1.9466, time 332.03ms, mfu 0.43%\n",
            "iter 1560: loss 1.9610, time 338.44ms, mfu 0.43%\n",
            "iter 1570: loss 2.0814, time 329.70ms, mfu 0.43%\n",
            "iter 1580: loss 2.0328, time 329.76ms, mfu 0.43%\n",
            "iter 1590: loss 1.9678, time 338.68ms, mfu 0.43%\n",
            "step 1600: train loss 1.8611, val loss 1.9620\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1600: loss 2.0023, time 1719.02ms, mfu 0.40%\n",
            "iter 1610: loss 2.0349, time 340.97ms, mfu 0.40%\n",
            "iter 1620: loss 2.0336, time 329.52ms, mfu 0.40%\n",
            "iter 1630: loss 1.9986, time 341.78ms, mfu 0.41%\n",
            "iter 1640: loss 1.9863, time 335.15ms, mfu 0.41%\n",
            "iter 1650: loss 1.9591, time 330.99ms, mfu 0.41%\n",
            "iter 1660: loss 1.9985, time 332.94ms, mfu 0.41%\n",
            "iter 1670: loss 2.0021, time 333.83ms, mfu 0.42%\n",
            "iter 1680: loss 1.9183, time 328.80ms, mfu 0.42%\n",
            "iter 1690: loss 1.9143, time 330.44ms, mfu 0.42%\n",
            "iter 1700: loss 1.8701, time 333.88ms, mfu 0.42%\n",
            "iter 1710: loss 1.9709, time 332.48ms, mfu 0.42%\n",
            "iter 1720: loss 1.8920, time 332.62ms, mfu 0.43%\n",
            "iter 1730: loss 1.8936, time 331.88ms, mfu 0.43%\n",
            "iter 1740: loss 1.9959, time 357.33ms, mfu 0.43%\n",
            "iter 1750: loss 1.8798, time 334.82ms, mfu 0.43%\n",
            "iter 1760: loss 1.9619, time 335.78ms, mfu 0.43%\n",
            "iter 1770: loss 1.9043, time 336.87ms, mfu 0.43%\n",
            "iter 1780: loss 1.9493, time 341.78ms, mfu 0.43%\n",
            "iter 1790: loss 1.8418, time 337.55ms, mfu 0.43%\n",
            "step 1800: train loss 1.7752, val loss 1.9180\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1800: loss 1.9000, time 1735.80ms, mfu 0.39%\n",
            "iter 1810: loss 1.8965, time 337.20ms, mfu 0.40%\n",
            "iter 1820: loss 1.8015, time 334.78ms, mfu 0.40%\n",
            "iter 1830: loss 1.8840, time 332.17ms, mfu 0.41%\n",
            "iter 1840: loss 1.9351, time 344.75ms, mfu 0.41%\n",
            "iter 1850: loss 1.9016, time 334.64ms, mfu 0.41%\n",
            "iter 1860: loss 1.9360, time 332.33ms, mfu 0.41%\n",
            "iter 1870: loss 1.8684, time 354.85ms, mfu 0.41%\n",
            "iter 1880: loss 1.9153, time 343.43ms, mfu 0.41%\n",
            "iter 1890: loss 1.8242, time 336.47ms, mfu 0.42%\n",
            "iter 1900: loss 1.8974, time 338.99ms, mfu 0.42%\n",
            "iter 1910: loss 1.8570, time 336.71ms, mfu 0.42%\n",
            "iter 1920: loss 1.8483, time 340.92ms, mfu 0.42%\n",
            "iter 1930: loss 1.7509, time 330.48ms, mfu 0.42%\n",
            "iter 1940: loss 1.8151, time 329.24ms, mfu 0.42%\n",
            "iter 1950: loss 1.8708, time 345.57ms, mfu 0.42%\n",
            "iter 1960: loss 1.8763, time 337.88ms, mfu 0.43%\n",
            "iter 1970: loss 1.8388, time 339.15ms, mfu 0.43%\n",
            "iter 1980: loss 1.8568, time 329.83ms, mfu 0.43%\n",
            "iter 1990: loss 1.8969, time 330.88ms, mfu 0.43%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 25/32: b128_L4_H8_E256_BS8_MI1000_D10_s25 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS8_MI1000_D10_s25.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 25\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1809, val loss 4.1764\n",
            "iter 0: loss 4.1921, time 1904.68ms, mfu -100.00%\n",
            "iter 10: loss 4.1454, time 315.62ms, mfu 0.86%\n",
            "iter 20: loss 3.9989, time 323.50ms, mfu 0.85%\n",
            "iter 30: loss 3.8431, time 319.10ms, mfu 0.85%\n",
            "iter 40: loss 3.6109, time 313.07ms, mfu 0.85%\n",
            "iter 50: loss 3.5773, time 313.59ms, mfu 0.85%\n",
            "iter 60: loss 3.4531, time 321.03ms, mfu 0.85%\n",
            "iter 70: loss 3.3920, time 316.57ms, mfu 0.85%\n",
            "iter 80: loss 3.2975, time 311.82ms, mfu 0.85%\n",
            "iter 90: loss 3.1825, time 313.41ms, mfu 0.85%\n",
            "iter 100: loss 3.1152, time 311.16ms, mfu 0.86%\n",
            "iter 110: loss 3.0962, time 314.02ms, mfu 0.86%\n",
            "iter 120: loss 2.9774, time 321.89ms, mfu 0.85%\n",
            "iter 130: loss 3.0126, time 328.12ms, mfu 0.85%\n",
            "iter 140: loss 2.8497, time 317.80ms, mfu 0.85%\n",
            "iter 150: loss 2.8225, time 313.69ms, mfu 0.85%\n",
            "iter 160: loss 2.8810, time 327.28ms, mfu 0.85%\n",
            "iter 170: loss 2.8216, time 323.48ms, mfu 0.85%\n",
            "iter 180: loss 2.8404, time 348.87ms, mfu 0.84%\n",
            "iter 190: loss 2.7983, time 325.66ms, mfu 0.84%\n",
            "step 200: train loss 2.7029, val loss 2.7123\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 200: loss 2.7353, time 1615.20ms, mfu 0.77%\n",
            "iter 210: loss 2.7009, time 313.31ms, mfu 0.78%\n",
            "iter 220: loss 2.6881, time 311.06ms, mfu 0.79%\n",
            "iter 230: loss 2.7348, time 315.05ms, mfu 0.80%\n",
            "iter 240: loss 2.7209, time 312.65ms, mfu 0.80%\n",
            "iter 250: loss 2.6845, time 310.71ms, mfu 0.81%\n",
            "iter 260: loss 2.5953, time 311.60ms, mfu 0.82%\n",
            "iter 270: loss 2.5985, time 323.26ms, mfu 0.82%\n",
            "iter 280: loss 2.6735, time 321.05ms, mfu 0.82%\n",
            "iter 290: loss 2.5808, time 319.38ms, mfu 0.82%\n",
            "iter 300: loss 2.5828, time 317.44ms, mfu 0.83%\n",
            "iter 310: loss 2.5732, time 319.20ms, mfu 0.83%\n",
            "iter 320: loss 2.5657, time 317.18ms, mfu 0.83%\n",
            "iter 330: loss 2.4961, time 316.62ms, mfu 0.83%\n",
            "iter 340: loss 2.4992, time 346.39ms, mfu 0.83%\n",
            "iter 350: loss 2.5416, time 311.96ms, mfu 0.83%\n",
            "iter 360: loss 2.4795, time 312.09ms, mfu 0.83%\n",
            "iter 370: loss 2.5767, time 313.80ms, mfu 0.84%\n",
            "iter 380: loss 2.4909, time 316.43ms, mfu 0.84%\n",
            "iter 390: loss 2.4267, time 310.35ms, mfu 0.84%\n",
            "step 400: train loss 2.4265, val loss 2.4319\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 400: loss 2.4902, time 1630.77ms, mfu 0.77%\n",
            "iter 410: loss 2.4486, time 315.17ms, mfu 0.78%\n",
            "iter 420: loss 2.4099, time 313.82ms, mfu 0.79%\n",
            "iter 430: loss 2.4275, time 315.24ms, mfu 0.80%\n",
            "iter 440: loss 2.4419, time 316.49ms, mfu 0.80%\n",
            "iter 450: loss 2.4971, time 316.33ms, mfu 0.81%\n",
            "iter 460: loss 2.3105, time 312.54ms, mfu 0.81%\n",
            "iter 470: loss 2.3596, time 313.05ms, mfu 0.82%\n",
            "iter 480: loss 2.3390, time 315.80ms, mfu 0.82%\n",
            "iter 490: loss 2.3278, time 320.68ms, mfu 0.82%\n",
            "iter 500: loss 2.3312, time 338.85ms, mfu 0.82%\n",
            "iter 510: loss 2.3244, time 320.90ms, mfu 0.82%\n",
            "iter 520: loss 2.2716, time 321.18ms, mfu 0.82%\n",
            "iter 530: loss 2.2898, time 322.89ms, mfu 0.83%\n",
            "iter 540: loss 2.3357, time 322.35ms, mfu 0.83%\n",
            "iter 550: loss 2.3125, time 314.01ms, mfu 0.83%\n",
            "iter 560: loss 2.3042, time 312.75ms, mfu 0.83%\n",
            "iter 570: loss 2.3489, time 321.89ms, mfu 0.83%\n",
            "iter 580: loss 2.3959, time 326.11ms, mfu 0.83%\n",
            "iter 590: loss 2.2456, time 315.17ms, mfu 0.84%\n",
            "step 600: train loss 2.2222, val loss 2.2442\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 600: loss 2.2498, time 1650.44ms, mfu 0.77%\n",
            "iter 610: loss 2.2999, time 311.74ms, mfu 0.78%\n",
            "iter 620: loss 2.2084, time 314.76ms, mfu 0.79%\n",
            "iter 630: loss 2.2094, time 315.99ms, mfu 0.79%\n",
            "iter 640: loss 2.2108, time 320.86ms, mfu 0.80%\n",
            "iter 650: loss 2.2715, time 312.62ms, mfu 0.80%\n",
            "iter 660: loss 2.1561, time 314.90ms, mfu 0.81%\n",
            "iter 670: loss 2.2840, time 316.89ms, mfu 0.81%\n",
            "iter 680: loss 2.2616, time 329.34ms, mfu 0.81%\n",
            "iter 690: loss 2.1719, time 313.32ms, mfu 0.82%\n",
            "iter 700: loss 2.1095, time 312.94ms, mfu 0.82%\n",
            "iter 710: loss 2.1950, time 311.68ms, mfu 0.83%\n",
            "iter 720: loss 2.1312, time 314.41ms, mfu 0.83%\n",
            "iter 730: loss 2.2401, time 312.81ms, mfu 0.83%\n",
            "iter 740: loss 2.1864, time 314.04ms, mfu 0.84%\n",
            "iter 750: loss 2.1798, time 311.60ms, mfu 0.84%\n",
            "iter 760: loss 2.1273, time 320.74ms, mfu 0.84%\n",
            "iter 770: loss 2.1021, time 312.44ms, mfu 0.84%\n",
            "iter 780: loss 2.1580, time 311.04ms, mfu 0.84%\n",
            "iter 790: loss 2.0805, time 312.70ms, mfu 0.85%\n",
            "step 800: train loss 1.9992, val loss 2.0462\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 800: loss 2.0547, time 1660.34ms, mfu 0.78%\n",
            "iter 810: loss 2.1584, time 315.74ms, mfu 0.79%\n",
            "iter 820: loss 1.9764, time 316.13ms, mfu 0.79%\n",
            "iter 830: loss 2.0698, time 316.96ms, mfu 0.80%\n",
            "iter 840: loss 2.0073, time 321.93ms, mfu 0.80%\n",
            "iter 850: loss 2.0123, time 315.60ms, mfu 0.81%\n",
            "iter 860: loss 2.0413, time 323.77ms, mfu 0.81%\n",
            "iter 870: loss 2.0186, time 321.43ms, mfu 0.81%\n",
            "iter 880: loss 2.0548, time 327.18ms, mfu 0.81%\n",
            "iter 890: loss 2.0091, time 320.47ms, mfu 0.82%\n",
            "iter 900: loss 2.0358, time 321.90ms, mfu 0.82%\n",
            "iter 910: loss 1.9186, time 321.93ms, mfu 0.82%\n",
            "iter 920: loss 1.8909, time 319.12ms, mfu 0.82%\n",
            "iter 930: loss 1.9502, time 317.51ms, mfu 0.83%\n",
            "iter 940: loss 2.0494, time 323.20ms, mfu 0.83%\n",
            "iter 950: loss 1.8451, time 313.27ms, mfu 0.83%\n",
            "iter 960: loss 1.9334, time 310.45ms, mfu 0.83%\n",
            "iter 970: loss 1.9353, time 313.36ms, mfu 0.84%\n",
            "iter 980: loss 1.8200, time 322.59ms, mfu 0.84%\n",
            "iter 990: loss 1.9774, time 321.22ms, mfu 0.84%\n",
            "step 1000: train loss 1.8023, val loss 1.9158\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 1000: loss 1.9116, time 1645.26ms, mfu 0.77%\n",
            "\n",
            "=== Experiment 26/32: b128_L4_H8_E256_BS8_MI1000_D20_s26 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS8_MI1000_D20_s26.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 26\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1809, val loss 4.1764\n",
            "iter 0: loss 4.1907, time 1920.97ms, mfu -100.00%\n",
            "iter 10: loss 4.1544, time 313.53ms, mfu 0.86%\n",
            "iter 20: loss 4.0289, time 360.48ms, mfu 0.85%\n",
            "iter 30: loss 3.8892, time 318.62ms, mfu 0.85%\n",
            "iter 40: loss 3.6571, time 316.15ms, mfu 0.85%\n",
            "iter 50: loss 3.6126, time 316.55ms, mfu 0.85%\n",
            "iter 60: loss 3.4883, time 314.34ms, mfu 0.85%\n",
            "iter 70: loss 3.4390, time 312.85ms, mfu 0.85%\n",
            "iter 80: loss 3.3543, time 318.24ms, mfu 0.85%\n",
            "iter 90: loss 3.2588, time 316.34ms, mfu 0.85%\n",
            "iter 100: loss 3.1883, time 312.35ms, mfu 0.85%\n",
            "iter 110: loss 3.1589, time 335.41ms, mfu 0.85%\n",
            "iter 120: loss 3.0322, time 315.30ms, mfu 0.85%\n",
            "iter 130: loss 3.0743, time 313.48ms, mfu 0.85%\n",
            "iter 140: loss 2.9008, time 314.07ms, mfu 0.85%\n",
            "iter 150: loss 2.8646, time 314.79ms, mfu 0.85%\n",
            "iter 160: loss 2.9410, time 314.55ms, mfu 0.85%\n",
            "iter 170: loss 2.8606, time 317.47ms, mfu 0.85%\n",
            "iter 180: loss 2.8813, time 314.05ms, mfu 0.85%\n",
            "iter 190: loss 2.8391, time 315.20ms, mfu 0.85%\n",
            "step 200: train loss 2.7184, val loss 2.7247\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 200: loss 2.7765, time 1627.24ms, mfu 0.78%\n",
            "iter 210: loss 2.7418, time 315.08ms, mfu 0.79%\n",
            "iter 220: loss 2.7409, time 319.20ms, mfu 0.80%\n",
            "iter 230: loss 2.7772, time 311.97ms, mfu 0.80%\n",
            "iter 240: loss 2.7624, time 316.68ms, mfu 0.81%\n",
            "iter 250: loss 2.7184, time 318.14ms, mfu 0.81%\n",
            "iter 260: loss 2.6328, time 323.57ms, mfu 0.81%\n",
            "iter 270: loss 2.6257, time 319.43ms, mfu 0.82%\n",
            "iter 280: loss 2.7071, time 319.33ms, mfu 0.82%\n",
            "iter 290: loss 2.6333, time 315.84ms, mfu 0.82%\n",
            "iter 300: loss 2.6016, time 317.89ms, mfu 0.83%\n",
            "iter 310: loss 2.6018, time 322.14ms, mfu 0.83%\n",
            "iter 320: loss 2.5862, time 318.01ms, mfu 0.83%\n",
            "iter 330: loss 2.5259, time 315.73ms, mfu 0.83%\n",
            "iter 340: loss 2.5481, time 312.20ms, mfu 0.84%\n",
            "iter 350: loss 2.5721, time 313.82ms, mfu 0.84%\n",
            "iter 360: loss 2.5185, time 311.90ms, mfu 0.84%\n",
            "iter 370: loss 2.6087, time 318.33ms, mfu 0.84%\n",
            "iter 380: loss 2.5242, time 314.72ms, mfu 0.84%\n",
            "iter 390: loss 2.4735, time 317.92ms, mfu 0.84%\n",
            "step 400: train loss 2.4691, val loss 2.4692\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 400: loss 2.5321, time 1613.16ms, mfu 0.78%\n",
            "iter 410: loss 2.4976, time 312.74ms, mfu 0.78%\n",
            "iter 420: loss 2.4442, time 312.99ms, mfu 0.79%\n",
            "iter 430: loss 2.4577, time 313.00ms, mfu 0.80%\n",
            "iter 440: loss 2.4992, time 340.57ms, mfu 0.80%\n",
            "iter 450: loss 2.5515, time 311.35ms, mfu 0.81%\n",
            "iter 460: loss 2.3506, time 334.27ms, mfu 0.81%\n",
            "iter 470: loss 2.4054, time 315.18ms, mfu 0.81%\n",
            "iter 480: loss 2.3611, time 331.12ms, mfu 0.81%\n",
            "iter 490: loss 2.3846, time 317.45ms, mfu 0.82%\n",
            "iter 500: loss 2.3863, time 311.74ms, mfu 0.82%\n",
            "iter 510: loss 2.3703, time 313.82ms, mfu 0.82%\n",
            "iter 520: loss 2.3146, time 315.17ms, mfu 0.83%\n",
            "iter 530: loss 2.3657, time 317.04ms, mfu 0.83%\n",
            "iter 540: loss 2.4052, time 313.39ms, mfu 0.83%\n",
            "iter 550: loss 2.3612, time 314.73ms, mfu 0.84%\n",
            "iter 560: loss 2.3683, time 338.18ms, mfu 0.83%\n",
            "iter 570: loss 2.3822, time 310.39ms, mfu 0.84%\n",
            "iter 580: loss 2.4428, time 314.19ms, mfu 0.84%\n",
            "iter 590: loss 2.3189, time 312.93ms, mfu 0.84%\n",
            "step 600: train loss 2.2800, val loss 2.2969\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 600: loss 2.3379, time 1637.11ms, mfu 0.77%\n",
            "iter 610: loss 2.3806, time 312.33ms, mfu 0.78%\n",
            "iter 620: loss 2.2835, time 313.51ms, mfu 0.79%\n",
            "iter 630: loss 2.2890, time 318.19ms, mfu 0.80%\n",
            "iter 640: loss 2.2912, time 320.07ms, mfu 0.80%\n",
            "iter 650: loss 2.3440, time 322.74ms, mfu 0.80%\n",
            "iter 660: loss 2.2272, time 320.51ms, mfu 0.81%\n",
            "iter 670: loss 2.3622, time 321.74ms, mfu 0.81%\n",
            "iter 680: loss 2.3542, time 314.71ms, mfu 0.82%\n",
            "iter 690: loss 2.2486, time 314.50ms, mfu 0.82%\n",
            "iter 700: loss 2.2128, time 321.24ms, mfu 0.82%\n",
            "iter 710: loss 2.2702, time 311.36ms, mfu 0.83%\n",
            "iter 720: loss 2.2396, time 311.63ms, mfu 0.83%\n",
            "iter 730: loss 2.3189, time 311.19ms, mfu 0.83%\n",
            "iter 740: loss 2.2864, time 317.71ms, mfu 0.84%\n",
            "iter 750: loss 2.2698, time 321.97ms, mfu 0.84%\n",
            "iter 760: loss 2.2315, time 321.89ms, mfu 0.84%\n",
            "iter 770: loss 2.2126, time 320.99ms, mfu 0.84%\n",
            "iter 780: loss 2.2390, time 328.53ms, mfu 0.84%\n",
            "iter 790: loss 2.1808, time 312.51ms, mfu 0.84%\n",
            "step 800: train loss 2.0910, val loss 2.1235\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 800: loss 2.1837, time 1653.73ms, mfu 0.77%\n",
            "iter 810: loss 2.2458, time 317.11ms, mfu 0.78%\n",
            "iter 820: loss 2.1209, time 313.81ms, mfu 0.79%\n",
            "iter 830: loss 2.1421, time 316.20ms, mfu 0.79%\n",
            "iter 840: loss 2.1258, time 314.71ms, mfu 0.80%\n",
            "iter 850: loss 2.1184, time 321.49ms, mfu 0.80%\n",
            "iter 860: loss 2.1755, time 320.22ms, mfu 0.81%\n",
            "iter 870: loss 2.1592, time 324.42ms, mfu 0.81%\n",
            "iter 880: loss 2.1627, time 322.07ms, mfu 0.81%\n",
            "iter 890: loss 2.1160, time 326.91ms, mfu 0.81%\n",
            "iter 900: loss 2.1262, time 313.48ms, mfu 0.82%\n",
            "iter 910: loss 2.0314, time 317.83ms, mfu 0.82%\n",
            "iter 920: loss 2.0117, time 321.70ms, mfu 0.82%\n",
            "iter 930: loss 2.0699, time 327.13ms, mfu 0.82%\n",
            "iter 940: loss 2.1781, time 317.02ms, mfu 0.83%\n",
            "iter 950: loss 1.9799, time 321.57ms, mfu 0.83%\n",
            "iter 960: loss 2.0599, time 321.27ms, mfu 0.83%\n",
            "iter 970: loss 2.0679, time 318.96ms, mfu 0.83%\n",
            "iter 980: loss 1.9526, time 330.22ms, mfu 0.83%\n",
            "iter 990: loss 2.0828, time 311.41ms, mfu 0.83%\n",
            "step 1000: train loss 1.9059, val loss 1.9934\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 1000: loss 2.0491, time 1680.23ms, mfu 0.77%\n",
            "\n",
            "=== Experiment 27/32: b128_L4_H8_E256_BS8_MI2000_D10_s27 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS8_MI2000_D10_s27.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 27\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1809, val loss 4.1764\n",
            "iter 0: loss 4.1921, time 1908.36ms, mfu -100.00%\n",
            "iter 10: loss 4.1454, time 316.15ms, mfu 0.85%\n",
            "iter 20: loss 3.9989, time 344.41ms, mfu 0.85%\n",
            "iter 30: loss 3.8431, time 314.80ms, mfu 0.85%\n",
            "iter 40: loss 3.6109, time 313.40ms, mfu 0.85%\n",
            "iter 50: loss 3.5773, time 319.91ms, mfu 0.85%\n",
            "iter 60: loss 3.4531, time 314.45ms, mfu 0.85%\n",
            "iter 70: loss 3.3920, time 314.12ms, mfu 0.85%\n",
            "iter 80: loss 3.2975, time 314.32ms, mfu 0.85%\n",
            "iter 90: loss 3.1825, time 314.33ms, mfu 0.85%\n",
            "iter 100: loss 3.1152, time 318.94ms, mfu 0.85%\n",
            "iter 110: loss 3.0962, time 320.12ms, mfu 0.85%\n",
            "iter 120: loss 2.9774, time 318.90ms, mfu 0.85%\n",
            "iter 130: loss 3.0126, time 312.72ms, mfu 0.85%\n",
            "iter 140: loss 2.8497, time 315.51ms, mfu 0.85%\n",
            "iter 150: loss 2.8225, time 313.53ms, mfu 0.85%\n",
            "iter 160: loss 2.8810, time 313.21ms, mfu 0.85%\n",
            "iter 170: loss 2.8216, time 324.98ms, mfu 0.85%\n",
            "iter 180: loss 2.8404, time 315.22ms, mfu 0.85%\n",
            "iter 190: loss 2.7983, time 311.38ms, mfu 0.85%\n",
            "step 200: train loss 2.7029, val loss 2.7123\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 200: loss 2.7353, time 1648.45ms, mfu 0.78%\n",
            "iter 210: loss 2.7009, time 324.37ms, mfu 0.79%\n",
            "iter 220: loss 2.6881, time 314.14ms, mfu 0.80%\n",
            "iter 230: loss 2.7348, time 311.56ms, mfu 0.80%\n",
            "iter 240: loss 2.7209, time 314.07ms, mfu 0.81%\n",
            "iter 250: loss 2.6845, time 322.42ms, mfu 0.81%\n",
            "iter 260: loss 2.5953, time 320.40ms, mfu 0.81%\n",
            "iter 270: loss 2.5985, time 315.69ms, mfu 0.82%\n",
            "iter 280: loss 2.6735, time 323.17ms, mfu 0.82%\n",
            "iter 290: loss 2.5808, time 311.21ms, mfu 0.83%\n",
            "iter 300: loss 2.5828, time 311.51ms, mfu 0.83%\n",
            "iter 310: loss 2.5732, time 314.09ms, mfu 0.83%\n",
            "iter 320: loss 2.5657, time 316.39ms, mfu 0.83%\n",
            "iter 330: loss 2.4961, time 317.19ms, mfu 0.84%\n",
            "iter 340: loss 2.4992, time 317.26ms, mfu 0.84%\n",
            "iter 350: loss 2.5416, time 315.45ms, mfu 0.84%\n",
            "iter 360: loss 2.4795, time 313.04ms, mfu 0.84%\n",
            "iter 370: loss 2.5767, time 313.43ms, mfu 0.84%\n",
            "iter 380: loss 2.4909, time 318.81ms, mfu 0.84%\n",
            "iter 390: loss 2.4267, time 316.99ms, mfu 0.84%\n",
            "step 400: train loss 2.4265, val loss 2.4319\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 400: loss 2.4902, time 1654.76ms, mfu 0.78%\n",
            "iter 410: loss 2.4486, time 321.14ms, mfu 0.78%\n",
            "iter 420: loss 2.4099, time 326.68ms, mfu 0.79%\n",
            "iter 430: loss 2.4275, time 324.84ms, mfu 0.79%\n",
            "iter 440: loss 2.4419, time 321.01ms, mfu 0.80%\n",
            "iter 450: loss 2.4971, time 325.56ms, mfu 0.80%\n",
            "iter 460: loss 2.3105, time 316.52ms, mfu 0.81%\n",
            "iter 470: loss 2.3596, time 340.71ms, mfu 0.80%\n",
            "iter 480: loss 2.3390, time 317.39ms, mfu 0.81%\n",
            "iter 490: loss 2.3278, time 317.73ms, mfu 0.81%\n",
            "iter 500: loss 2.3312, time 322.19ms, mfu 0.82%\n",
            "iter 510: loss 2.3244, time 316.32ms, mfu 0.82%\n",
            "iter 520: loss 2.2716, time 315.95ms, mfu 0.82%\n",
            "iter 530: loss 2.2898, time 312.85ms, mfu 0.83%\n",
            "iter 540: loss 2.3357, time 310.66ms, mfu 0.83%\n",
            "iter 550: loss 2.3125, time 317.38ms, mfu 0.83%\n",
            "iter 560: loss 2.3042, time 324.05ms, mfu 0.83%\n",
            "iter 570: loss 2.3489, time 316.86ms, mfu 0.83%\n",
            "iter 580: loss 2.3959, time 324.49ms, mfu 0.83%\n",
            "iter 590: loss 2.2456, time 317.27ms, mfu 0.84%\n",
            "step 600: train loss 2.2222, val loss 2.2442\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 600: loss 2.2498, time 1647.59ms, mfu 0.77%\n",
            "iter 610: loss 2.2999, time 316.71ms, mfu 0.78%\n",
            "iter 620: loss 2.2084, time 321.65ms, mfu 0.78%\n",
            "iter 630: loss 2.2094, time 321.85ms, mfu 0.79%\n",
            "iter 640: loss 2.2108, time 320.51ms, mfu 0.79%\n",
            "iter 650: loss 2.2715, time 325.89ms, mfu 0.80%\n",
            "iter 660: loss 2.1561, time 323.21ms, mfu 0.80%\n",
            "iter 670: loss 2.2840, time 316.46ms, mfu 0.81%\n",
            "iter 680: loss 2.2616, time 318.44ms, mfu 0.81%\n",
            "iter 690: loss 2.1719, time 321.41ms, mfu 0.81%\n",
            "iter 700: loss 2.1095, time 316.29ms, mfu 0.82%\n",
            "iter 710: loss 2.1950, time 315.83ms, mfu 0.82%\n",
            "iter 720: loss 2.1312, time 316.05ms, mfu 0.82%\n",
            "iter 730: loss 2.2401, time 315.61ms, mfu 0.83%\n",
            "iter 740: loss 2.1864, time 314.38ms, mfu 0.83%\n",
            "iter 750: loss 2.1798, time 317.55ms, mfu 0.83%\n",
            "iter 760: loss 2.1273, time 316.18ms, mfu 0.83%\n",
            "iter 770: loss 2.1021, time 320.34ms, mfu 0.84%\n",
            "iter 780: loss 2.1580, time 317.89ms, mfu 0.84%\n",
            "iter 790: loss 2.0805, time 315.06ms, mfu 0.84%\n",
            "step 800: train loss 1.9992, val loss 2.0462\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 800: loss 2.0547, time 1645.21ms, mfu 0.77%\n",
            "iter 810: loss 2.1584, time 314.67ms, mfu 0.78%\n",
            "iter 820: loss 1.9764, time 310.55ms, mfu 0.79%\n",
            "iter 830: loss 2.0698, time 314.21ms, mfu 0.80%\n",
            "iter 840: loss 2.0073, time 319.79ms, mfu 0.80%\n",
            "iter 850: loss 2.0123, time 316.16ms, mfu 0.81%\n",
            "iter 860: loss 2.0413, time 314.99ms, mfu 0.81%\n",
            "iter 870: loss 2.0186, time 313.13ms, mfu 0.82%\n",
            "iter 880: loss 2.0548, time 346.56ms, mfu 0.81%\n",
            "iter 890: loss 2.0091, time 318.83ms, mfu 0.82%\n",
            "iter 900: loss 2.0358, time 314.98ms, mfu 0.82%\n",
            "iter 910: loss 1.9186, time 310.55ms, mfu 0.82%\n",
            "iter 920: loss 1.8909, time 315.26ms, mfu 0.83%\n",
            "iter 930: loss 1.9502, time 311.69ms, mfu 0.83%\n",
            "iter 940: loss 2.0494, time 315.34ms, mfu 0.83%\n",
            "iter 950: loss 1.8451, time 323.92ms, mfu 0.83%\n",
            "iter 960: loss 1.9334, time 329.40ms, mfu 0.83%\n",
            "iter 970: loss 1.9353, time 320.92ms, mfu 0.83%\n",
            "iter 980: loss 1.8200, time 315.39ms, mfu 0.84%\n",
            "iter 990: loss 1.9774, time 321.87ms, mfu 0.84%\n",
            "step 1000: train loss 1.8023, val loss 1.9158\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1000: loss 1.9116, time 1633.50ms, mfu 0.77%\n",
            "iter 1010: loss 1.9816, time 318.41ms, mfu 0.78%\n",
            "iter 1020: loss 1.9203, time 314.04ms, mfu 0.79%\n",
            "iter 1030: loss 1.8182, time 316.78ms, mfu 0.79%\n",
            "iter 1040: loss 2.0282, time 318.71ms, mfu 0.80%\n",
            "iter 1050: loss 1.8955, time 322.15ms, mfu 0.80%\n",
            "iter 1060: loss 1.9503, time 328.81ms, mfu 0.80%\n",
            "iter 1070: loss 1.8253, time 320.98ms, mfu 0.81%\n",
            "iter 1080: loss 1.7939, time 320.38ms, mfu 0.81%\n",
            "iter 1090: loss 1.7978, time 317.93ms, mfu 0.81%\n",
            "iter 1100: loss 1.9287, time 351.41ms, mfu 0.81%\n",
            "iter 1110: loss 1.8265, time 315.71ms, mfu 0.81%\n",
            "iter 1120: loss 1.6804, time 317.35ms, mfu 0.82%\n",
            "iter 1130: loss 1.7606, time 317.74ms, mfu 0.82%\n",
            "iter 1140: loss 1.7581, time 329.90ms, mfu 0.82%\n",
            "iter 1150: loss 1.9376, time 313.79ms, mfu 0.82%\n",
            "iter 1160: loss 1.7816, time 313.32ms, mfu 0.83%\n",
            "iter 1170: loss 1.7310, time 316.97ms, mfu 0.83%\n",
            "iter 1180: loss 1.7059, time 315.90ms, mfu 0.83%\n",
            "iter 1190: loss 1.7407, time 312.91ms, mfu 0.84%\n",
            "step 1200: train loss 1.6442, val loss 1.8086\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1200: loss 1.6797, time 1681.55ms, mfu 0.77%\n",
            "iter 1210: loss 1.7826, time 329.18ms, mfu 0.77%\n",
            "iter 1220: loss 1.7651, time 319.69ms, mfu 0.78%\n",
            "iter 1230: loss 1.6817, time 315.70ms, mfu 0.79%\n",
            "iter 1240: loss 1.5774, time 327.40ms, mfu 0.79%\n",
            "iter 1250: loss 1.6717, time 318.68ms, mfu 0.80%\n",
            "iter 1260: loss 1.6961, time 314.37ms, mfu 0.80%\n",
            "iter 1270: loss 1.6682, time 313.34ms, mfu 0.81%\n",
            "iter 1280: loss 1.6903, time 312.52ms, mfu 0.81%\n",
            "iter 1290: loss 1.7297, time 318.91ms, mfu 0.82%\n",
            "iter 1300: loss 1.6486, time 313.04ms, mfu 0.82%\n",
            "iter 1310: loss 1.6802, time 311.64ms, mfu 0.83%\n",
            "iter 1320: loss 1.5643, time 313.41ms, mfu 0.83%\n",
            "iter 1330: loss 1.6668, time 321.23ms, mfu 0.83%\n",
            "iter 1340: loss 1.5448, time 325.68ms, mfu 0.83%\n",
            "iter 1350: loss 1.5854, time 321.75ms, mfu 0.83%\n",
            "iter 1360: loss 1.7437, time 331.62ms, mfu 0.83%\n",
            "iter 1370: loss 1.6611, time 310.62ms, mfu 0.83%\n",
            "iter 1380: loss 1.5866, time 313.89ms, mfu 0.84%\n",
            "iter 1390: loss 1.5899, time 313.70ms, mfu 0.84%\n",
            "step 1400: train loss 1.5314, val loss 1.7144\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1400: loss 1.6132, time 1664.95ms, mfu 0.77%\n",
            "iter 1410: loss 1.6117, time 311.57ms, mfu 0.78%\n",
            "iter 1420: loss 1.6562, time 315.45ms, mfu 0.79%\n",
            "iter 1430: loss 1.6365, time 316.24ms, mfu 0.79%\n",
            "iter 1440: loss 1.5595, time 333.47ms, mfu 0.80%\n",
            "iter 1450: loss 1.7146, time 312.81ms, mfu 0.80%\n",
            "iter 1460: loss 1.4358, time 315.36ms, mfu 0.81%\n",
            "iter 1470: loss 1.5496, time 325.79ms, mfu 0.81%\n",
            "iter 1480: loss 1.5958, time 314.05ms, mfu 0.82%\n",
            "iter 1490: loss 1.5578, time 314.51ms, mfu 0.82%\n",
            "iter 1500: loss 1.5358, time 315.08ms, mfu 0.82%\n",
            "iter 1510: loss 1.5168, time 320.25ms, mfu 0.83%\n",
            "iter 1520: loss 1.5533, time 316.43ms, mfu 0.83%\n",
            "iter 1530: loss 1.6197, time 315.06ms, mfu 0.83%\n",
            "iter 1540: loss 1.6455, time 313.08ms, mfu 0.83%\n",
            "iter 1550: loss 1.5935, time 324.41ms, mfu 0.83%\n",
            "iter 1560: loss 1.7694, time 320.44ms, mfu 0.83%\n",
            "iter 1570: loss 1.5634, time 313.77ms, mfu 0.84%\n",
            "iter 1580: loss 1.5654, time 312.70ms, mfu 0.84%\n",
            "iter 1590: loss 1.5377, time 328.59ms, mfu 0.84%\n",
            "step 1600: train loss 1.4524, val loss 1.6569\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1600: loss 1.5596, time 1659.45ms, mfu 0.77%\n",
            "iter 1610: loss 1.5893, time 316.52ms, mfu 0.78%\n",
            "iter 1620: loss 1.5286, time 322.28ms, mfu 0.78%\n",
            "iter 1630: loss 1.5704, time 322.87ms, mfu 0.79%\n",
            "iter 1640: loss 1.6046, time 314.59ms, mfu 0.80%\n",
            "iter 1650: loss 1.5819, time 314.46ms, mfu 0.80%\n",
            "iter 1660: loss 1.5600, time 322.54ms, mfu 0.81%\n",
            "iter 1670: loss 1.4461, time 317.18ms, mfu 0.81%\n",
            "iter 1680: loss 1.6089, time 317.61ms, mfu 0.81%\n",
            "iter 1690: loss 1.7544, time 322.59ms, mfu 0.82%\n",
            "iter 1700: loss 1.3726, time 319.51ms, mfu 0.82%\n",
            "iter 1710: loss 1.6131, time 317.96ms, mfu 0.82%\n",
            "iter 1720: loss 1.5939, time 317.68ms, mfu 0.83%\n",
            "iter 1730: loss 1.4626, time 317.37ms, mfu 0.83%\n",
            "iter 1740: loss 1.4273, time 325.71ms, mfu 0.83%\n",
            "iter 1750: loss 1.4112, time 317.88ms, mfu 0.83%\n",
            "iter 1760: loss 1.5518, time 316.75ms, mfu 0.83%\n",
            "iter 1770: loss 1.4801, time 322.65ms, mfu 0.83%\n",
            "iter 1780: loss 1.5160, time 317.72ms, mfu 0.83%\n",
            "iter 1790: loss 1.4754, time 318.31ms, mfu 0.84%\n",
            "step 1800: train loss 1.3923, val loss 1.6077\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1800: loss 1.5038, time 1659.38ms, mfu 0.77%\n",
            "iter 1810: loss 1.4678, time 324.22ms, mfu 0.77%\n",
            "iter 1820: loss 1.4537, time 315.14ms, mfu 0.78%\n",
            "iter 1830: loss 1.4829, time 311.94ms, mfu 0.79%\n",
            "iter 1840: loss 1.4918, time 326.66ms, mfu 0.79%\n",
            "iter 1850: loss 1.3993, time 317.56ms, mfu 0.80%\n",
            "iter 1860: loss 1.4247, time 322.48ms, mfu 0.80%\n",
            "iter 1870: loss 1.4778, time 312.85ms, mfu 0.81%\n",
            "iter 1880: loss 1.4802, time 315.34ms, mfu 0.81%\n",
            "iter 1890: loss 1.4030, time 324.28ms, mfu 0.82%\n",
            "iter 1900: loss 1.3604, time 311.95ms, mfu 0.82%\n",
            "iter 1910: loss 1.4428, time 314.23ms, mfu 0.82%\n",
            "iter 1920: loss 1.5282, time 317.99ms, mfu 0.83%\n",
            "iter 1930: loss 1.5427, time 313.29ms, mfu 0.83%\n",
            "iter 1940: loss 1.5101, time 326.23ms, mfu 0.83%\n",
            "iter 1950: loss 1.5648, time 318.97ms, mfu 0.83%\n",
            "iter 1960: loss 1.5746, time 314.53ms, mfu 0.83%\n",
            "iter 1970: loss 1.3898, time 320.37ms, mfu 0.84%\n",
            "iter 1980: loss 1.4760, time 316.00ms, mfu 0.84%\n",
            "iter 1990: loss 1.4300, time 322.56ms, mfu 0.84%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 28/32: b128_L4_H8_E256_BS8_MI2000_D20_s28 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS8_MI2000_D20_s28.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 28\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1809, val loss 4.1764\n",
            "iter 0: loss 4.1907, time 1978.69ms, mfu -100.00%\n",
            "iter 10: loss 4.1544, time 320.38ms, mfu 0.84%\n",
            "iter 20: loss 4.0289, time 336.60ms, mfu 0.84%\n",
            "iter 30: loss 3.8892, time 325.50ms, mfu 0.84%\n",
            "iter 40: loss 3.6571, time 317.88ms, mfu 0.84%\n",
            "iter 50: loss 3.6126, time 316.61ms, mfu 0.84%\n",
            "iter 60: loss 3.4883, time 320.73ms, mfu 0.84%\n",
            "iter 70: loss 3.4390, time 325.87ms, mfu 0.84%\n",
            "iter 80: loss 3.3543, time 320.97ms, mfu 0.84%\n",
            "iter 90: loss 3.2588, time 329.39ms, mfu 0.84%\n",
            "iter 100: loss 3.1883, time 324.93ms, mfu 0.84%\n",
            "iter 110: loss 3.1589, time 330.26ms, mfu 0.83%\n",
            "iter 120: loss 3.0322, time 315.51ms, mfu 0.84%\n",
            "iter 130: loss 3.0743, time 337.41ms, mfu 0.83%\n",
            "iter 140: loss 2.9008, time 324.21ms, mfu 0.83%\n",
            "iter 150: loss 2.8646, time 322.81ms, mfu 0.83%\n",
            "iter 160: loss 2.9410, time 326.37ms, mfu 0.83%\n",
            "iter 170: loss 2.8606, time 335.41ms, mfu 0.83%\n",
            "iter 180: loss 2.8813, time 319.77ms, mfu 0.83%\n",
            "iter 190: loss 2.8391, time 314.67ms, mfu 0.83%\n",
            "step 200: train loss 2.7184, val loss 2.7247\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 200: loss 2.7765, time 1653.74ms, mfu 0.77%\n",
            "iter 210: loss 2.7418, time 318.35ms, mfu 0.78%\n",
            "iter 220: loss 2.7409, time 317.22ms, mfu 0.78%\n",
            "iter 230: loss 2.7772, time 320.08ms, mfu 0.79%\n",
            "iter 240: loss 2.7624, time 321.75ms, mfu 0.79%\n",
            "iter 250: loss 2.7184, time 316.03ms, mfu 0.80%\n",
            "iter 260: loss 2.6328, time 318.80ms, mfu 0.80%\n",
            "iter 270: loss 2.6257, time 317.56ms, mfu 0.81%\n",
            "iter 280: loss 2.7071, time 327.08ms, mfu 0.81%\n",
            "iter 290: loss 2.6333, time 313.12ms, mfu 0.82%\n",
            "iter 300: loss 2.6016, time 319.34ms, mfu 0.82%\n",
            "iter 310: loss 2.6018, time 319.73ms, mfu 0.82%\n",
            "iter 320: loss 2.5862, time 322.46ms, mfu 0.82%\n",
            "iter 330: loss 2.5259, time 324.80ms, mfu 0.82%\n",
            "iter 340: loss 2.5481, time 321.74ms, mfu 0.83%\n",
            "iter 350: loss 2.5721, time 322.00ms, mfu 0.83%\n",
            "iter 360: loss 2.5185, time 322.00ms, mfu 0.83%\n",
            "iter 370: loss 2.6087, time 332.93ms, mfu 0.83%\n",
            "iter 380: loss 2.5242, time 323.05ms, mfu 0.83%\n",
            "iter 390: loss 2.4735, time 320.46ms, mfu 0.83%\n",
            "step 400: train loss 2.4691, val loss 2.4692\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 400: loss 2.5321, time 1647.43ms, mfu 0.76%\n",
            "iter 410: loss 2.4976, time 330.14ms, mfu 0.77%\n",
            "iter 420: loss 2.4442, time 324.59ms, mfu 0.77%\n",
            "iter 430: loss 2.4577, time 323.27ms, mfu 0.78%\n",
            "iter 440: loss 2.4992, time 329.29ms, mfu 0.78%\n",
            "iter 450: loss 2.5515, time 320.63ms, mfu 0.79%\n",
            "iter 460: loss 2.3506, time 330.27ms, mfu 0.79%\n",
            "iter 470: loss 2.4054, time 329.99ms, mfu 0.80%\n",
            "iter 480: loss 2.3611, time 336.90ms, mfu 0.80%\n",
            "iter 490: loss 2.3846, time 320.95ms, mfu 0.80%\n",
            "iter 500: loss 2.3863, time 329.80ms, mfu 0.80%\n",
            "iter 510: loss 2.3703, time 320.99ms, mfu 0.81%\n",
            "iter 520: loss 2.3146, time 326.12ms, mfu 0.81%\n",
            "iter 530: loss 2.3657, time 323.28ms, mfu 0.81%\n",
            "iter 540: loss 2.4052, time 322.48ms, mfu 0.81%\n",
            "iter 550: loss 2.3612, time 322.96ms, mfu 0.82%\n",
            "iter 560: loss 2.3683, time 345.05ms, mfu 0.81%\n",
            "iter 570: loss 2.3822, time 345.37ms, mfu 0.81%\n",
            "iter 580: loss 2.4428, time 319.69ms, mfu 0.81%\n",
            "iter 590: loss 2.3189, time 324.60ms, mfu 0.81%\n",
            "step 600: train loss 2.2800, val loss 2.2969\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 600: loss 2.3379, time 1822.60ms, mfu 0.75%\n",
            "iter 610: loss 2.3806, time 325.51ms, mfu 0.76%\n",
            "iter 620: loss 2.2835, time 321.93ms, mfu 0.76%\n",
            "iter 630: loss 2.2890, time 320.22ms, mfu 0.77%\n",
            "iter 640: loss 2.2912, time 382.89ms, mfu 0.77%\n",
            "iter 650: loss 2.3440, time 315.87ms, mfu 0.77%\n",
            "iter 660: loss 2.2272, time 319.98ms, mfu 0.78%\n",
            "iter 670: loss 2.3622, time 317.67ms, mfu 0.79%\n",
            "iter 680: loss 2.3542, time 329.67ms, mfu 0.79%\n",
            "iter 690: loss 2.2486, time 324.35ms, mfu 0.80%\n",
            "iter 700: loss 2.2128, time 324.11ms, mfu 0.80%\n",
            "iter 710: loss 2.2702, time 317.41ms, mfu 0.80%\n",
            "iter 720: loss 2.2396, time 325.32ms, mfu 0.81%\n",
            "iter 730: loss 2.3189, time 318.61ms, mfu 0.81%\n",
            "iter 740: loss 2.2864, time 330.18ms, mfu 0.81%\n",
            "iter 750: loss 2.2698, time 334.62ms, mfu 0.81%\n",
            "iter 760: loss 2.2315, time 326.34ms, mfu 0.81%\n",
            "iter 770: loss 2.2126, time 319.81ms, mfu 0.82%\n",
            "iter 780: loss 2.2390, time 317.10ms, mfu 0.82%\n",
            "iter 790: loss 2.1808, time 321.35ms, mfu 0.82%\n",
            "step 800: train loss 2.0910, val loss 2.1235\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 800: loss 2.1837, time 1645.18ms, mfu 0.76%\n",
            "iter 810: loss 2.2458, time 315.40ms, mfu 0.77%\n",
            "iter 820: loss 2.1209, time 317.10ms, mfu 0.77%\n",
            "iter 830: loss 2.1421, time 323.45ms, mfu 0.78%\n",
            "iter 840: loss 2.1258, time 322.99ms, mfu 0.79%\n",
            "iter 850: loss 2.1184, time 329.53ms, mfu 0.79%\n",
            "iter 860: loss 2.1755, time 320.17ms, mfu 0.79%\n",
            "iter 870: loss 2.1592, time 317.22ms, mfu 0.80%\n",
            "iter 880: loss 2.1627, time 320.11ms, mfu 0.80%\n",
            "iter 890: loss 2.1160, time 317.97ms, mfu 0.81%\n",
            "iter 900: loss 2.1262, time 316.85ms, mfu 0.81%\n",
            "iter 910: loss 2.0314, time 343.45ms, mfu 0.81%\n",
            "iter 920: loss 2.0117, time 324.62ms, mfu 0.81%\n",
            "iter 930: loss 2.0699, time 315.23ms, mfu 0.82%\n",
            "iter 940: loss 2.1781, time 330.73ms, mfu 0.82%\n",
            "iter 950: loss 1.9799, time 311.61ms, mfu 0.82%\n",
            "iter 960: loss 2.0599, time 313.03ms, mfu 0.83%\n",
            "iter 970: loss 2.0679, time 312.10ms, mfu 0.83%\n",
            "iter 980: loss 1.9526, time 319.14ms, mfu 0.83%\n",
            "iter 990: loss 2.0828, time 318.84ms, mfu 0.83%\n",
            "step 1000: train loss 1.9059, val loss 1.9934\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1000: loss 2.0491, time 1650.19ms, mfu 0.77%\n",
            "iter 1010: loss 2.1316, time 326.57ms, mfu 0.77%\n",
            "iter 1020: loss 2.0412, time 317.51ms, mfu 0.78%\n",
            "iter 1030: loss 1.9435, time 314.07ms, mfu 0.79%\n",
            "iter 1040: loss 2.1281, time 313.75ms, mfu 0.80%\n",
            "iter 1050: loss 2.0197, time 332.88ms, mfu 0.80%\n",
            "iter 1060: loss 2.0835, time 327.39ms, mfu 0.80%\n",
            "iter 1070: loss 1.9635, time 316.78ms, mfu 0.80%\n",
            "iter 1080: loss 1.9144, time 314.27ms, mfu 0.81%\n",
            "iter 1090: loss 1.9115, time 317.88ms, mfu 0.81%\n",
            "iter 1100: loss 2.0451, time 312.81ms, mfu 0.82%\n",
            "iter 1110: loss 1.9462, time 330.12ms, mfu 0.82%\n",
            "iter 1120: loss 1.8441, time 324.84ms, mfu 0.82%\n",
            "iter 1130: loss 1.9200, time 316.48ms, mfu 0.82%\n",
            "iter 1140: loss 1.8777, time 313.65ms, mfu 0.83%\n",
            "iter 1150: loss 2.0524, time 313.00ms, mfu 0.83%\n",
            "iter 1160: loss 1.9112, time 324.34ms, mfu 0.83%\n",
            "iter 1170: loss 1.8531, time 314.82ms, mfu 0.83%\n",
            "iter 1180: loss 1.8381, time 316.09ms, mfu 0.84%\n",
            "iter 1190: loss 1.8984, time 316.84ms, mfu 0.84%\n",
            "step 1200: train loss 1.7565, val loss 1.8998\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1200: loss 1.8347, time 1650.27ms, mfu 0.77%\n",
            "iter 1210: loss 1.8937, time 315.08ms, mfu 0.78%\n",
            "iter 1220: loss 1.8838, time 314.48ms, mfu 0.79%\n",
            "iter 1230: loss 1.8188, time 322.75ms, mfu 0.79%\n",
            "iter 1240: loss 1.7124, time 331.27ms, mfu 0.79%\n",
            "iter 1250: loss 1.8052, time 315.53ms, mfu 0.80%\n",
            "iter 1260: loss 1.8466, time 312.22ms, mfu 0.81%\n",
            "iter 1270: loss 1.8195, time 320.58ms, mfu 0.81%\n",
            "iter 1280: loss 1.8432, time 315.55ms, mfu 0.81%\n",
            "iter 1290: loss 1.8373, time 314.93ms, mfu 0.82%\n",
            "iter 1300: loss 1.7881, time 318.19ms, mfu 0.82%\n",
            "iter 1310: loss 1.8082, time 317.69ms, mfu 0.82%\n",
            "iter 1320: loss 1.7027, time 316.07ms, mfu 0.83%\n",
            "iter 1330: loss 1.8037, time 313.28ms, mfu 0.83%\n",
            "iter 1340: loss 1.6748, time 315.97ms, mfu 0.83%\n",
            "iter 1350: loss 1.7121, time 328.24ms, mfu 0.83%\n",
            "iter 1360: loss 1.8505, time 315.75ms, mfu 0.83%\n",
            "iter 1370: loss 1.8334, time 311.43ms, mfu 0.84%\n",
            "iter 1380: loss 1.7544, time 311.47ms, mfu 0.84%\n",
            "iter 1390: loss 1.7238, time 317.04ms, mfu 0.84%\n",
            "step 1400: train loss 1.6365, val loss 1.8100\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1400: loss 1.7487, time 1676.40ms, mfu 0.77%\n",
            "iter 1410: loss 1.7287, time 316.93ms, mfu 0.78%\n",
            "iter 1420: loss 1.8094, time 324.15ms, mfu 0.79%\n",
            "iter 1430: loss 1.7729, time 317.84ms, mfu 0.79%\n",
            "iter 1440: loss 1.6901, time 317.75ms, mfu 0.80%\n",
            "iter 1450: loss 1.8448, time 314.41ms, mfu 0.80%\n",
            "iter 1460: loss 1.5417, time 329.57ms, mfu 0.81%\n",
            "iter 1470: loss 1.6710, time 314.78ms, mfu 0.81%\n",
            "iter 1480: loss 1.6898, time 327.81ms, mfu 0.81%\n",
            "iter 1490: loss 1.6907, time 319.41ms, mfu 0.82%\n",
            "iter 1500: loss 1.7033, time 320.96ms, mfu 0.82%\n",
            "iter 1510: loss 1.6427, time 320.49ms, mfu 0.82%\n",
            "iter 1520: loss 1.7000, time 323.81ms, mfu 0.82%\n",
            "iter 1530: loss 1.7434, time 331.25ms, mfu 0.82%\n",
            "iter 1540: loss 1.7890, time 320.06ms, mfu 0.82%\n",
            "iter 1550: loss 1.7359, time 316.14ms, mfu 0.83%\n",
            "iter 1560: loss 1.8679, time 318.31ms, mfu 0.83%\n",
            "iter 1570: loss 1.6912, time 327.91ms, mfu 0.83%\n",
            "iter 1580: loss 1.6729, time 320.84ms, mfu 0.83%\n",
            "iter 1590: loss 1.6557, time 327.55ms, mfu 0.83%\n",
            "step 1600: train loss 1.5502, val loss 1.7496\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1600: loss 1.6814, time 1690.51ms, mfu 0.76%\n",
            "iter 1610: loss 1.7137, time 322.89ms, mfu 0.77%\n",
            "iter 1620: loss 1.6491, time 321.97ms, mfu 0.78%\n",
            "iter 1630: loss 1.6946, time 319.28ms, mfu 0.78%\n",
            "iter 1640: loss 1.7391, time 329.66ms, mfu 0.79%\n",
            "iter 1650: loss 1.7158, time 317.47ms, mfu 0.79%\n",
            "iter 1660: loss 1.6908, time 317.23ms, mfu 0.80%\n",
            "iter 1670: loss 1.5626, time 315.41ms, mfu 0.80%\n",
            "iter 1680: loss 1.7320, time 316.57ms, mfu 0.81%\n",
            "iter 1690: loss 1.8505, time 324.14ms, mfu 0.81%\n",
            "iter 1700: loss 1.4808, time 320.83ms, mfu 0.81%\n",
            "iter 1710: loss 1.7187, time 326.49ms, mfu 0.82%\n",
            "iter 1720: loss 1.6934, time 324.58ms, mfu 0.82%\n",
            "iter 1730: loss 1.5858, time 324.57ms, mfu 0.82%\n",
            "iter 1740: loss 1.5281, time 318.68ms, mfu 0.82%\n",
            "iter 1750: loss 1.5285, time 317.13ms, mfu 0.82%\n",
            "iter 1760: loss 1.6590, time 314.32ms, mfu 0.83%\n",
            "iter 1770: loss 1.5904, time 312.40ms, mfu 0.83%\n",
            "iter 1780: loss 1.6422, time 313.92ms, mfu 0.83%\n",
            "iter 1790: loss 1.5799, time 330.19ms, mfu 0.83%\n",
            "step 1800: train loss 1.4875, val loss 1.6954\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1800: loss 1.6326, time 1644.30ms, mfu 0.77%\n",
            "iter 1810: loss 1.6075, time 312.66ms, mfu 0.78%\n",
            "iter 1820: loss 1.5598, time 312.74ms, mfu 0.78%\n",
            "iter 1830: loss 1.6113, time 353.29ms, mfu 0.78%\n",
            "iter 1840: loss 1.5937, time 347.37ms, mfu 0.78%\n",
            "iter 1850: loss 1.5208, time 318.21ms, mfu 0.79%\n",
            "iter 1860: loss 1.5607, time 318.45ms, mfu 0.79%\n",
            "iter 1870: loss 1.5961, time 322.94ms, mfu 0.80%\n",
            "iter 1880: loss 1.6134, time 319.11ms, mfu 0.80%\n",
            "iter 1890: loss 1.5494, time 314.19ms, mfu 0.81%\n",
            "iter 1900: loss 1.4776, time 321.88ms, mfu 0.81%\n",
            "iter 1910: loss 1.5385, time 317.10ms, mfu 0.82%\n",
            "iter 1920: loss 1.6557, time 314.39ms, mfu 0.82%\n",
            "iter 1930: loss 1.6956, time 321.03ms, mfu 0.82%\n",
            "iter 1940: loss 1.6238, time 325.30ms, mfu 0.82%\n",
            "iter 1950: loss 1.6784, time 317.17ms, mfu 0.83%\n",
            "iter 1960: loss 1.6972, time 321.03ms, mfu 0.83%\n",
            "iter 1970: loss 1.5164, time 314.25ms, mfu 0.83%\n",
            "iter 1980: loss 1.5674, time 316.39ms, mfu 0.83%\n",
            "iter 1990: loss 1.5308, time 316.19ms, mfu 0.83%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 29/32: b128_L4_H8_E256_BS16_MI1000_D10_s29 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS16_MI1000_D10_s29.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 29\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1813, val loss 4.1755\n",
            "iter 0: loss 4.1866, time 2078.35ms, mfu -100.00%\n",
            "iter 10: loss 4.1224, time 332.86ms, mfu 1.62%\n",
            "iter 20: loss 4.0095, time 329.36ms, mfu 1.62%\n",
            "iter 30: loss 3.8216, time 329.52ms, mfu 1.63%\n",
            "iter 40: loss 3.6491, time 340.44ms, mfu 1.62%\n",
            "iter 50: loss 3.5379, time 330.34ms, mfu 1.62%\n",
            "iter 60: loss 3.4725, time 330.51ms, mfu 1.62%\n",
            "iter 70: loss 3.3781, time 326.92ms, mfu 1.63%\n",
            "iter 80: loss 3.2951, time 337.45ms, mfu 1.62%\n",
            "iter 90: loss 3.2158, time 360.02ms, mfu 1.61%\n",
            "iter 100: loss 3.1068, time 336.55ms, mfu 1.61%\n",
            "iter 110: loss 3.0044, time 336.02ms, mfu 1.61%\n",
            "iter 120: loss 3.0242, time 338.61ms, mfu 1.61%\n",
            "iter 130: loss 2.9543, time 330.38ms, mfu 1.61%\n",
            "iter 140: loss 2.8637, time 328.33ms, mfu 1.61%\n",
            "iter 150: loss 2.8090, time 332.22ms, mfu 1.62%\n",
            "iter 160: loss 2.8632, time 324.28ms, mfu 1.62%\n",
            "iter 170: loss 2.8259, time 325.58ms, mfu 1.62%\n",
            "iter 180: loss 2.7204, time 335.05ms, mfu 1.62%\n",
            "iter 190: loss 2.7303, time 334.74ms, mfu 1.62%\n",
            "step 200: train loss 2.7024, val loss 2.7085\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 200: loss 2.7244, time 1737.28ms, mfu 1.49%\n",
            "iter 210: loss 2.7465, time 327.11ms, mfu 1.51%\n",
            "iter 220: loss 2.7101, time 343.08ms, mfu 1.51%\n",
            "iter 230: loss 2.6575, time 337.03ms, mfu 1.52%\n",
            "iter 240: loss 2.6306, time 324.19ms, mfu 1.54%\n",
            "iter 250: loss 2.5990, time 329.73ms, mfu 1.55%\n",
            "iter 260: loss 2.6418, time 325.56ms, mfu 1.56%\n",
            "iter 270: loss 2.5931, time 330.14ms, mfu 1.57%\n",
            "iter 280: loss 2.5667, time 327.37ms, mfu 1.57%\n",
            "iter 290: loss 2.5368, time 345.12ms, mfu 1.57%\n",
            "iter 300: loss 2.5140, time 323.65ms, mfu 1.58%\n",
            "iter 310: loss 2.5515, time 325.34ms, mfu 1.59%\n",
            "iter 320: loss 2.5254, time 330.49ms, mfu 1.59%\n",
            "iter 330: loss 2.4790, time 336.55ms, mfu 1.60%\n",
            "iter 340: loss 2.5311, time 323.61ms, mfu 1.60%\n",
            "iter 350: loss 2.5248, time 332.35ms, mfu 1.60%\n",
            "iter 360: loss 2.4942, time 332.75ms, mfu 1.61%\n",
            "iter 370: loss 2.4549, time 327.03ms, mfu 1.61%\n",
            "iter 380: loss 2.4728, time 328.27ms, mfu 1.61%\n",
            "iter 390: loss 2.4549, time 331.29ms, mfu 1.62%\n",
            "step 400: train loss 2.3947, val loss 2.4102\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 400: loss 2.4000, time 1763.36ms, mfu 1.48%\n",
            "iter 410: loss 2.4496, time 347.85ms, mfu 1.49%\n",
            "iter 420: loss 2.3869, time 328.90ms, mfu 1.51%\n",
            "iter 430: loss 2.3717, time 347.00ms, mfu 1.51%\n",
            "iter 440: loss 2.4146, time 338.18ms, mfu 1.52%\n",
            "iter 450: loss 2.3372, time 334.56ms, mfu 1.53%\n",
            "iter 460: loss 2.3876, time 335.59ms, mfu 1.54%\n",
            "iter 470: loss 2.3668, time 335.35ms, mfu 1.54%\n",
            "iter 480: loss 2.3529, time 339.29ms, mfu 1.55%\n",
            "iter 490: loss 2.3271, time 335.87ms, mfu 1.56%\n",
            "iter 500: loss 2.3120, time 337.19ms, mfu 1.56%\n",
            "iter 510: loss 2.2996, time 335.77ms, mfu 1.56%\n",
            "iter 520: loss 2.3433, time 335.75ms, mfu 1.57%\n",
            "iter 530: loss 2.3658, time 334.39ms, mfu 1.57%\n",
            "iter 540: loss 2.2676, time 340.14ms, mfu 1.57%\n",
            "iter 550: loss 2.2774, time 335.00ms, mfu 1.58%\n",
            "iter 560: loss 2.2305, time 335.07ms, mfu 1.58%\n",
            "iter 570: loss 2.2633, time 336.10ms, mfu 1.58%\n",
            "iter 580: loss 2.2008, time 331.93ms, mfu 1.59%\n",
            "iter 590: loss 2.2060, time 326.58ms, mfu 1.59%\n",
            "step 600: train loss 2.1702, val loss 2.1970\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 600: loss 2.3135, time 1772.84ms, mfu 1.47%\n",
            "iter 610: loss 2.2757, time 340.25ms, mfu 1.48%\n",
            "iter 620: loss 2.2680, time 334.18ms, mfu 1.49%\n",
            "iter 630: loss 2.1594, time 335.35ms, mfu 1.50%\n",
            "iter 640: loss 2.2046, time 342.35ms, mfu 1.51%\n",
            "iter 650: loss 2.1712, time 333.24ms, mfu 1.52%\n",
            "iter 660: loss 2.1636, time 328.83ms, mfu 1.53%\n",
            "iter 670: loss 2.1746, time 331.32ms, mfu 1.54%\n",
            "iter 680: loss 2.2012, time 327.33ms, mfu 1.55%\n",
            "iter 690: loss 2.1575, time 326.63ms, mfu 1.56%\n",
            "iter 700: loss 2.0873, time 332.79ms, mfu 1.57%\n",
            "iter 710: loss 2.0410, time 326.31ms, mfu 1.58%\n",
            "iter 720: loss 2.1266, time 335.53ms, mfu 1.58%\n",
            "iter 730: loss 2.1066, time 333.43ms, mfu 1.58%\n",
            "iter 740: loss 2.0848, time 328.65ms, mfu 1.59%\n",
            "iter 750: loss 2.0904, time 334.12ms, mfu 1.59%\n",
            "iter 760: loss 2.0207, time 334.68ms, mfu 1.60%\n",
            "iter 770: loss 2.0603, time 335.77ms, mfu 1.60%\n",
            "iter 780: loss 2.0307, time 335.10ms, mfu 1.60%\n",
            "iter 790: loss 2.0599, time 343.12ms, mfu 1.60%\n",
            "step 800: train loss 1.9188, val loss 2.0110\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 800: loss 2.0250, time 1744.18ms, mfu 1.47%\n",
            "iter 810: loss 1.9843, time 332.94ms, mfu 1.48%\n",
            "iter 820: loss 2.0185, time 330.83ms, mfu 1.50%\n",
            "iter 830: loss 1.9582, time 335.84ms, mfu 1.51%\n",
            "iter 840: loss 1.9798, time 344.22ms, mfu 1.51%\n",
            "iter 850: loss 1.9621, time 324.26ms, mfu 1.53%\n",
            "iter 860: loss 1.9585, time 331.74ms, mfu 1.54%\n",
            "iter 870: loss 1.9152, time 332.62ms, mfu 1.55%\n",
            "iter 880: loss 1.9620, time 324.34ms, mfu 1.56%\n",
            "iter 890: loss 1.9152, time 327.22ms, mfu 1.57%\n",
            "iter 900: loss 1.9518, time 329.35ms, mfu 1.58%\n",
            "iter 910: loss 1.9284, time 326.47ms, mfu 1.58%\n",
            "iter 920: loss 1.8638, time 325.54ms, mfu 1.59%\n",
            "iter 930: loss 1.9088, time 336.18ms, mfu 1.59%\n",
            "iter 940: loss 1.8587, time 330.67ms, mfu 1.60%\n",
            "iter 950: loss 1.8076, time 325.82ms, mfu 1.60%\n",
            "iter 960: loss 1.8780, time 346.41ms, mfu 1.60%\n",
            "iter 970: loss 1.8881, time 333.54ms, mfu 1.60%\n",
            "iter 980: loss 1.7708, time 327.61ms, mfu 1.60%\n",
            "iter 990: loss 1.8075, time 331.67ms, mfu 1.61%\n",
            "step 1000: train loss 1.7178, val loss 1.8694\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 1000: loss 1.7787, time 1793.41ms, mfu 1.48%\n",
            "\n",
            "=== Experiment 30/32: b128_L4_H8_E256_BS16_MI1000_D20_s30 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS16_MI1000_D20_s30.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 30\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1813, val loss 4.1755\n",
            "iter 0: loss 4.1889, time 2079.39ms, mfu -100.00%\n",
            "iter 10: loss 4.1331, time 327.29ms, mfu 1.65%\n",
            "iter 20: loss 4.0392, time 335.18ms, mfu 1.65%\n",
            "iter 30: loss 3.8710, time 334.94ms, mfu 1.64%\n",
            "iter 40: loss 3.7006, time 328.76ms, mfu 1.64%\n",
            "iter 50: loss 3.5801, time 333.57ms, mfu 1.64%\n",
            "iter 60: loss 3.5169, time 339.58ms, mfu 1.63%\n",
            "iter 70: loss 3.4320, time 328.56ms, mfu 1.64%\n",
            "iter 80: loss 3.3550, time 328.94ms, mfu 1.64%\n",
            "iter 90: loss 3.2848, time 334.35ms, mfu 1.63%\n",
            "iter 100: loss 3.1804, time 328.52ms, mfu 1.63%\n",
            "iter 110: loss 3.0699, time 328.75ms, mfu 1.64%\n",
            "iter 120: loss 3.0937, time 334.21ms, mfu 1.63%\n",
            "iter 130: loss 3.0170, time 345.57ms, mfu 1.63%\n",
            "iter 140: loss 2.9280, time 328.53ms, mfu 1.63%\n",
            "iter 150: loss 2.8584, time 334.78ms, mfu 1.63%\n",
            "iter 160: loss 2.9197, time 333.06ms, mfu 1.63%\n",
            "iter 170: loss 2.8721, time 335.59ms, mfu 1.62%\n",
            "iter 180: loss 2.7698, time 335.78ms, mfu 1.62%\n",
            "iter 190: loss 2.7720, time 330.62ms, mfu 1.62%\n",
            "step 200: train loss 2.7176, val loss 2.7209\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 200: loss 2.7611, time 1830.04ms, mfu 1.49%\n",
            "iter 210: loss 2.7836, time 328.97ms, mfu 1.51%\n",
            "iter 220: loss 2.7402, time 334.81ms, mfu 1.52%\n",
            "iter 230: loss 2.6959, time 353.60ms, mfu 1.52%\n",
            "iter 240: loss 2.6739, time 335.15ms, mfu 1.53%\n",
            "iter 250: loss 2.6372, time 334.39ms, mfu 1.54%\n",
            "iter 260: loss 2.6772, time 329.53ms, mfu 1.55%\n",
            "iter 270: loss 2.6257, time 337.16ms, mfu 1.55%\n",
            "iter 280: loss 2.6003, time 328.50ms, mfu 1.56%\n",
            "iter 290: loss 2.5718, time 327.31ms, mfu 1.57%\n",
            "iter 300: loss 2.5487, time 334.89ms, mfu 1.57%\n",
            "iter 310: loss 2.5847, time 335.43ms, mfu 1.58%\n",
            "iter 320: loss 2.5496, time 326.54ms, mfu 1.58%\n",
            "iter 330: loss 2.5144, time 326.64ms, mfu 1.59%\n",
            "iter 340: loss 2.5615, time 334.87ms, mfu 1.59%\n",
            "iter 350: loss 2.5645, time 328.70ms, mfu 1.60%\n",
            "iter 360: loss 2.5307, time 328.11ms, mfu 1.60%\n",
            "iter 370: loss 2.4932, time 334.12ms, mfu 1.60%\n",
            "iter 380: loss 2.5042, time 338.31ms, mfu 1.60%\n",
            "iter 390: loss 2.4912, time 331.48ms, mfu 1.61%\n",
            "step 400: train loss 2.4352, val loss 2.4452\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 400: loss 2.4417, time 1779.91ms, mfu 1.48%\n",
            "iter 410: loss 2.4829, time 341.82ms, mfu 1.49%\n",
            "iter 420: loss 2.4336, time 339.63ms, mfu 1.50%\n",
            "iter 430: loss 2.4040, time 338.08ms, mfu 1.51%\n",
            "iter 440: loss 2.4504, time 330.83ms, mfu 1.52%\n",
            "iter 450: loss 2.3725, time 334.94ms, mfu 1.53%\n",
            "iter 460: loss 2.4309, time 331.67ms, mfu 1.54%\n",
            "iter 470: loss 2.4124, time 329.22ms, mfu 1.55%\n",
            "iter 480: loss 2.4008, time 336.50ms, mfu 1.55%\n",
            "iter 490: loss 2.3756, time 331.75ms, mfu 1.56%\n",
            "iter 500: loss 2.3675, time 334.21ms, mfu 1.57%\n",
            "iter 510: loss 2.3581, time 329.75ms, mfu 1.57%\n",
            "iter 520: loss 2.3954, time 331.85ms, mfu 1.58%\n",
            "iter 530: loss 2.4107, time 329.03ms, mfu 1.59%\n",
            "iter 540: loss 2.3158, time 328.06ms, mfu 1.59%\n",
            "iter 550: loss 2.3303, time 332.55ms, mfu 1.59%\n",
            "iter 560: loss 2.2852, time 330.84ms, mfu 1.60%\n",
            "iter 570: loss 2.3248, time 328.95ms, mfu 1.60%\n",
            "iter 580: loss 2.2725, time 335.40ms, mfu 1.60%\n",
            "iter 590: loss 2.2671, time 331.18ms, mfu 1.61%\n",
            "step 600: train loss 2.2247, val loss 2.2472\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 600: loss 2.3707, time 1776.46ms, mfu 1.48%\n",
            "iter 610: loss 2.3280, time 327.88ms, mfu 1.49%\n",
            "iter 620: loss 2.3330, time 326.65ms, mfu 1.51%\n",
            "iter 630: loss 2.2437, time 337.31ms, mfu 1.52%\n",
            "iter 640: loss 2.2788, time 337.72ms, mfu 1.53%\n",
            "iter 650: loss 2.2498, time 328.90ms, mfu 1.54%\n",
            "iter 660: loss 2.2351, time 336.74ms, mfu 1.54%\n",
            "iter 670: loss 2.2434, time 335.50ms, mfu 1.55%\n",
            "iter 680: loss 2.2854, time 327.79ms, mfu 1.56%\n",
            "iter 690: loss 2.2390, time 334.53ms, mfu 1.57%\n",
            "iter 700: loss 2.1783, time 341.82ms, mfu 1.57%\n",
            "iter 710: loss 2.1413, time 331.43ms, mfu 1.57%\n",
            "iter 720: loss 2.2145, time 331.55ms, mfu 1.58%\n",
            "iter 730: loss 2.1922, time 372.24ms, mfu 1.57%\n",
            "iter 740: loss 2.1756, time 336.97ms, mfu 1.57%\n",
            "iter 750: loss 2.1850, time 327.76ms, mfu 1.58%\n",
            "iter 760: loss 2.1173, time 343.13ms, mfu 1.58%\n",
            "iter 770: loss 2.1672, time 328.33ms, mfu 1.58%\n",
            "iter 780: loss 2.1383, time 328.25ms, mfu 1.59%\n",
            "iter 790: loss 2.1567, time 335.80ms, mfu 1.59%\n",
            "step 800: train loss 2.0080, val loss 2.0781\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 800: loss 2.1211, time 1772.50ms, mfu 1.46%\n",
            "iter 810: loss 2.0865, time 329.56ms, mfu 1.48%\n",
            "iter 820: loss 2.1315, time 329.64ms, mfu 1.50%\n",
            "iter 830: loss 2.0553, time 328.42ms, mfu 1.51%\n",
            "iter 840: loss 2.0773, time 347.19ms, mfu 1.52%\n",
            "iter 850: loss 2.0729, time 327.86ms, mfu 1.53%\n",
            "iter 860: loss 2.0568, time 327.90ms, mfu 1.54%\n",
            "iter 870: loss 2.0297, time 334.05ms, mfu 1.55%\n",
            "iter 880: loss 2.0644, time 336.02ms, mfu 1.55%\n",
            "iter 890: loss 2.0110, time 325.99ms, mfu 1.56%\n",
            "iter 900: loss 2.0557, time 334.83ms, mfu 1.57%\n",
            "iter 910: loss 2.0323, time 345.53ms, mfu 1.57%\n",
            "iter 920: loss 1.9662, time 330.48ms, mfu 1.57%\n",
            "iter 930: loss 2.0202, time 329.95ms, mfu 1.58%\n",
            "iter 940: loss 1.9854, time 331.30ms, mfu 1.59%\n",
            "iter 950: loss 1.9360, time 345.01ms, mfu 1.58%\n",
            "iter 960: loss 1.9775, time 327.92ms, mfu 1.59%\n",
            "iter 970: loss 1.9981, time 336.64ms, mfu 1.59%\n",
            "iter 980: loss 1.9161, time 329.26ms, mfu 1.60%\n",
            "iter 990: loss 1.9162, time 328.23ms, mfu 1.60%\n",
            "step 1000: train loss 1.8126, val loss 1.9384\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 1000: loss 1.8968, time 1760.19ms, mfu 1.47%\n",
            "\n",
            "=== Experiment 31/32: b128_L4_H8_E256_BS16_MI2000_D10_s31 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS16_MI2000_D10_s31.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 31\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1813, val loss 4.1755\n",
            "iter 0: loss 4.1866, time 2049.27ms, mfu -100.00%\n",
            "iter 10: loss 4.1224, time 329.99ms, mfu 1.64%\n",
            "iter 20: loss 4.0095, time 327.28ms, mfu 1.64%\n",
            "iter 30: loss 3.8216, time 328.88ms, mfu 1.64%\n",
            "iter 40: loss 3.6491, time 345.52ms, mfu 1.63%\n",
            "iter 50: loss 3.5379, time 327.59ms, mfu 1.63%\n",
            "iter 60: loss 3.4725, time 328.31ms, mfu 1.63%\n",
            "iter 70: loss 3.3781, time 340.34ms, mfu 1.63%\n",
            "iter 80: loss 3.2951, time 339.63ms, mfu 1.62%\n",
            "iter 90: loss 3.2158, time 332.16ms, mfu 1.62%\n",
            "iter 100: loss 3.1068, time 344.39ms, mfu 1.62%\n",
            "iter 110: loss 3.0044, time 337.71ms, mfu 1.62%\n",
            "iter 120: loss 3.0242, time 326.30ms, mfu 1.62%\n",
            "iter 130: loss 2.9543, time 333.99ms, mfu 1.62%\n",
            "iter 140: loss 2.8637, time 336.52ms, mfu 1.62%\n",
            "iter 150: loss 2.8090, time 342.42ms, mfu 1.61%\n",
            "iter 160: loss 2.8632, time 330.31ms, mfu 1.62%\n",
            "iter 170: loss 2.8259, time 336.77ms, mfu 1.62%\n",
            "iter 180: loss 2.7204, time 334.28ms, mfu 1.62%\n",
            "iter 190: loss 2.7303, time 336.03ms, mfu 1.61%\n",
            "step 200: train loss 2.7024, val loss 2.7085\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 200: loss 2.7244, time 1767.85ms, mfu 1.48%\n",
            "iter 210: loss 2.7465, time 329.72ms, mfu 1.50%\n",
            "iter 220: loss 2.7101, time 327.50ms, mfu 1.51%\n",
            "iter 230: loss 2.6575, time 329.96ms, mfu 1.53%\n",
            "iter 240: loss 2.6306, time 325.69ms, mfu 1.54%\n",
            "iter 250: loss 2.5990, time 330.11ms, mfu 1.55%\n",
            "iter 260: loss 2.6418, time 342.09ms, mfu 1.55%\n",
            "iter 270: loss 2.5931, time 332.96ms, mfu 1.56%\n",
            "iter 280: loss 2.5667, time 339.69ms, mfu 1.56%\n",
            "iter 290: loss 2.5368, time 326.96ms, mfu 1.57%\n",
            "iter 300: loss 2.5140, time 325.41ms, mfu 1.58%\n",
            "iter 310: loss 2.5515, time 335.06ms, mfu 1.58%\n",
            "iter 320: loss 2.5254, time 333.96ms, mfu 1.59%\n",
            "iter 330: loss 2.4790, time 326.77ms, mfu 1.59%\n",
            "iter 340: loss 2.5311, time 325.90ms, mfu 1.60%\n",
            "iter 350: loss 2.5248, time 343.31ms, mfu 1.60%\n",
            "iter 360: loss 2.4942, time 331.63ms, mfu 1.60%\n",
            "iter 370: loss 2.4549, time 333.58ms, mfu 1.60%\n",
            "iter 380: loss 2.4728, time 330.76ms, mfu 1.60%\n",
            "iter 390: loss 2.4549, time 326.56ms, mfu 1.61%\n",
            "step 400: train loss 2.3947, val loss 2.4102\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 400: loss 2.4000, time 1751.45ms, mfu 1.48%\n",
            "iter 410: loss 2.4496, time 327.72ms, mfu 1.50%\n",
            "iter 420: loss 2.3869, time 338.90ms, mfu 1.51%\n",
            "iter 430: loss 2.3717, time 340.52ms, mfu 1.51%\n",
            "iter 440: loss 2.4146, time 328.12ms, mfu 1.53%\n",
            "iter 450: loss 2.3372, time 328.33ms, mfu 1.54%\n",
            "iter 460: loss 2.3876, time 333.72ms, mfu 1.55%\n",
            "iter 470: loss 2.3668, time 330.44ms, mfu 1.56%\n",
            "iter 480: loss 2.3529, time 330.37ms, mfu 1.56%\n",
            "iter 490: loss 2.3271, time 325.84ms, mfu 1.57%\n",
            "iter 500: loss 2.3120, time 336.58ms, mfu 1.58%\n",
            "iter 510: loss 2.2996, time 331.35ms, mfu 1.58%\n",
            "iter 520: loss 2.3433, time 335.87ms, mfu 1.58%\n",
            "iter 530: loss 2.3658, time 328.10ms, mfu 1.59%\n",
            "iter 540: loss 2.2676, time 345.01ms, mfu 1.59%\n",
            "iter 550: loss 2.2774, time 333.35ms, mfu 1.59%\n",
            "iter 560: loss 2.2305, time 331.80ms, mfu 1.59%\n",
            "iter 570: loss 2.2633, time 341.69ms, mfu 1.59%\n",
            "iter 580: loss 2.2008, time 337.30ms, mfu 1.59%\n",
            "iter 590: loss 2.2060, time 336.17ms, mfu 1.59%\n",
            "step 600: train loss 2.1702, val loss 2.1970\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 600: loss 2.3135, time 1761.96ms, mfu 1.47%\n",
            "iter 610: loss 2.2757, time 336.78ms, mfu 1.48%\n",
            "iter 620: loss 2.2680, time 336.31ms, mfu 1.49%\n",
            "iter 630: loss 2.1594, time 334.78ms, mfu 1.50%\n",
            "iter 640: loss 2.2046, time 339.10ms, mfu 1.51%\n",
            "iter 650: loss 2.1712, time 331.19ms, mfu 1.52%\n",
            "iter 660: loss 2.1636, time 340.39ms, mfu 1.53%\n",
            "iter 670: loss 2.1746, time 336.13ms, mfu 1.54%\n",
            "iter 680: loss 2.2012, time 329.35ms, mfu 1.55%\n",
            "iter 690: loss 2.1575, time 329.54ms, mfu 1.56%\n",
            "iter 700: loss 2.0873, time 330.22ms, mfu 1.57%\n",
            "iter 710: loss 2.0410, time 333.39ms, mfu 1.57%\n",
            "iter 720: loss 2.1266, time 334.93ms, mfu 1.57%\n",
            "iter 730: loss 2.1066, time 333.36ms, mfu 1.58%\n",
            "iter 740: loss 2.0848, time 336.65ms, mfu 1.58%\n",
            "iter 750: loss 2.0904, time 338.86ms, mfu 1.58%\n",
            "iter 760: loss 2.0207, time 331.17ms, mfu 1.59%\n",
            "iter 770: loss 2.0603, time 327.77ms, mfu 1.59%\n",
            "iter 780: loss 2.0307, time 326.17ms, mfu 1.60%\n",
            "iter 790: loss 2.0599, time 333.38ms, mfu 1.60%\n",
            "step 800: train loss 1.9188, val loss 2.0110\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 800: loss 2.0250, time 1765.32ms, mfu 1.47%\n",
            "iter 810: loss 1.9843, time 331.07ms, mfu 1.49%\n",
            "iter 820: loss 2.0185, time 342.90ms, mfu 1.50%\n",
            "iter 830: loss 1.9582, time 337.28ms, mfu 1.51%\n",
            "iter 840: loss 1.9798, time 334.81ms, mfu 1.52%\n",
            "iter 850: loss 1.9621, time 331.68ms, mfu 1.53%\n",
            "iter 860: loss 1.9585, time 335.96ms, mfu 1.54%\n",
            "iter 870: loss 1.9152, time 336.52ms, mfu 1.54%\n",
            "iter 880: loss 1.9620, time 327.12ms, mfu 1.55%\n",
            "iter 890: loss 1.9152, time 338.89ms, mfu 1.56%\n",
            "iter 900: loss 1.9518, time 333.72ms, mfu 1.56%\n",
            "iter 910: loss 1.9284, time 329.30ms, mfu 1.57%\n",
            "iter 920: loss 1.8638, time 341.52ms, mfu 1.57%\n",
            "iter 930: loss 1.9088, time 359.38ms, mfu 1.57%\n",
            "iter 940: loss 1.8587, time 332.19ms, mfu 1.57%\n",
            "iter 950: loss 1.8076, time 341.20ms, mfu 1.57%\n",
            "iter 960: loss 1.8780, time 343.59ms, mfu 1.57%\n",
            "iter 970: loss 1.8881, time 326.78ms, mfu 1.58%\n",
            "iter 980: loss 1.7708, time 331.28ms, mfu 1.59%\n",
            "iter 990: loss 1.8075, time 339.66ms, mfu 1.59%\n",
            "step 1000: train loss 1.7178, val loss 1.8694\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1000: loss 1.7787, time 1751.39ms, mfu 1.46%\n",
            "iter 1010: loss 1.9001, time 326.71ms, mfu 1.48%\n",
            "iter 1020: loss 1.8133, time 328.28ms, mfu 1.49%\n",
            "iter 1030: loss 1.8209, time 333.33ms, mfu 1.51%\n",
            "iter 1040: loss 1.7117, time 345.20ms, mfu 1.51%\n",
            "iter 1050: loss 1.8624, time 329.93ms, mfu 1.52%\n",
            "iter 1060: loss 1.7338, time 331.23ms, mfu 1.54%\n",
            "iter 1070: loss 1.8514, time 336.39ms, mfu 1.54%\n",
            "iter 1080: loss 1.7667, time 327.96ms, mfu 1.55%\n",
            "iter 1090: loss 1.7282, time 326.58ms, mfu 1.56%\n",
            "iter 1100: loss 1.8214, time 328.67ms, mfu 1.57%\n",
            "iter 1110: loss 1.7199, time 333.26ms, mfu 1.58%\n",
            "iter 1120: loss 1.6901, time 335.70ms, mfu 1.58%\n",
            "iter 1130: loss 1.7413, time 326.51ms, mfu 1.59%\n",
            "iter 1140: loss 1.6946, time 331.63ms, mfu 1.59%\n",
            "iter 1150: loss 1.7694, time 329.23ms, mfu 1.60%\n",
            "iter 1160: loss 1.6737, time 332.99ms, mfu 1.60%\n",
            "iter 1170: loss 1.6787, time 334.12ms, mfu 1.60%\n",
            "iter 1180: loss 1.6938, time 332.77ms, mfu 1.60%\n",
            "iter 1190: loss 1.7646, time 331.90ms, mfu 1.60%\n",
            "step 1200: train loss 1.5707, val loss 1.7504\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1200: loss 1.6163, time 1805.21ms, mfu 1.47%\n",
            "iter 1210: loss 1.6736, time 339.92ms, mfu 1.49%\n",
            "iter 1220: loss 1.6726, time 332.71ms, mfu 1.50%\n",
            "iter 1230: loss 1.7243, time 329.01ms, mfu 1.51%\n",
            "iter 1240: loss 1.6216, time 335.72ms, mfu 1.52%\n",
            "iter 1250: loss 1.7285, time 353.09ms, mfu 1.52%\n",
            "iter 1260: loss 1.5676, time 328.30ms, mfu 1.54%\n",
            "iter 1270: loss 1.5858, time 333.26ms, mfu 1.54%\n",
            "iter 1280: loss 1.5932, time 335.57ms, mfu 1.55%\n",
            "iter 1290: loss 1.5500, time 333.58ms, mfu 1.56%\n",
            "iter 1300: loss 1.6418, time 330.46ms, mfu 1.56%\n",
            "iter 1310: loss 1.6034, time 327.80ms, mfu 1.57%\n",
            "iter 1320: loss 1.6465, time 325.38ms, mfu 1.58%\n",
            "iter 1330: loss 1.6003, time 329.51ms, mfu 1.59%\n",
            "iter 1340: loss 1.5743, time 326.48ms, mfu 1.59%\n",
            "iter 1350: loss 1.6198, time 330.00ms, mfu 1.60%\n",
            "iter 1360: loss 1.6067, time 338.63ms, mfu 1.60%\n",
            "iter 1370: loss 1.5409, time 343.84ms, mfu 1.59%\n",
            "iter 1380: loss 1.6038, time 336.68ms, mfu 1.60%\n",
            "iter 1390: loss 1.5542, time 337.83ms, mfu 1.60%\n",
            "step 1400: train loss 1.4778, val loss 1.6592\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1400: loss 1.5519, time 1758.91ms, mfu 1.47%\n",
            "iter 1410: loss 1.4791, time 325.81ms, mfu 1.49%\n",
            "iter 1420: loss 1.6464, time 330.34ms, mfu 1.50%\n",
            "iter 1430: loss 1.6035, time 334.38ms, mfu 1.51%\n",
            "iter 1440: loss 1.5613, time 332.26ms, mfu 1.52%\n",
            "iter 1450: loss 1.5311, time 334.49ms, mfu 1.53%\n",
            "iter 1460: loss 1.5219, time 345.13ms, mfu 1.54%\n",
            "iter 1470: loss 1.5235, time 329.98ms, mfu 1.55%\n",
            "iter 1480: loss 1.5095, time 327.14ms, mfu 1.56%\n",
            "iter 1490: loss 1.5590, time 324.76ms, mfu 1.57%\n",
            "iter 1500: loss 1.5798, time 327.81ms, mfu 1.57%\n",
            "iter 1510: loss 1.4818, time 327.43ms, mfu 1.58%\n",
            "iter 1520: loss 1.5342, time 335.65ms, mfu 1.58%\n",
            "iter 1530: loss 1.4643, time 340.98ms, mfu 1.58%\n",
            "iter 1540: loss 1.4832, time 338.87ms, mfu 1.59%\n",
            "iter 1550: loss 1.5618, time 336.67ms, mfu 1.59%\n",
            "iter 1560: loss 1.3788, time 335.85ms, mfu 1.59%\n",
            "iter 1570: loss 1.5537, time 336.96ms, mfu 1.59%\n",
            "iter 1580: loss 1.5127, time 331.80ms, mfu 1.59%\n",
            "iter 1590: loss 1.3909, time 326.59ms, mfu 1.60%\n",
            "step 1600: train loss 1.4002, val loss 1.6000\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1600: loss 1.5858, time 1803.84ms, mfu 1.47%\n",
            "iter 1610: loss 1.4745, time 338.32ms, mfu 1.48%\n",
            "iter 1620: loss 1.4573, time 337.73ms, mfu 1.49%\n",
            "iter 1630: loss 1.4735, time 338.33ms, mfu 1.50%\n",
            "iter 1640: loss 1.4969, time 336.74ms, mfu 1.51%\n",
            "iter 1650: loss 1.4162, time 328.73ms, mfu 1.53%\n",
            "iter 1660: loss 1.4001, time 327.62ms, mfu 1.54%\n",
            "iter 1670: loss 1.4620, time 345.16ms, mfu 1.54%\n",
            "iter 1680: loss 1.4284, time 329.91ms, mfu 1.55%\n",
            "iter 1690: loss 1.3920, time 329.59ms, mfu 1.56%\n",
            "iter 1700: loss 1.4579, time 326.84ms, mfu 1.57%\n",
            "iter 1710: loss 1.5244, time 334.60ms, mfu 1.57%\n",
            "iter 1720: loss 1.4892, time 335.85ms, mfu 1.58%\n",
            "iter 1730: loss 1.3819, time 329.80ms, mfu 1.58%\n",
            "iter 1740: loss 1.4593, time 334.31ms, mfu 1.59%\n",
            "iter 1750: loss 1.3995, time 331.52ms, mfu 1.59%\n",
            "iter 1760: loss 1.5570, time 336.16ms, mfu 1.59%\n",
            "iter 1770: loss 1.4540, time 340.96ms, mfu 1.59%\n",
            "iter 1780: loss 1.3698, time 328.63ms, mfu 1.60%\n",
            "iter 1790: loss 1.4316, time 334.12ms, mfu 1.60%\n",
            "step 1800: train loss 1.3424, val loss 1.5556\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1800: loss 1.4858, time 1823.78ms, mfu 1.47%\n",
            "iter 1810: loss 1.3638, time 331.51ms, mfu 1.48%\n",
            "iter 1820: loss 1.4409, time 334.11ms, mfu 1.50%\n",
            "iter 1830: loss 1.4816, time 326.80ms, mfu 1.51%\n",
            "iter 1840: loss 1.4751, time 332.11ms, mfu 1.52%\n",
            "iter 1850: loss 1.4425, time 340.90ms, mfu 1.53%\n",
            "iter 1860: loss 1.3817, time 333.18ms, mfu 1.54%\n",
            "iter 1870: loss 1.3400, time 328.56ms, mfu 1.55%\n",
            "iter 1880: loss 1.3160, time 326.42ms, mfu 1.56%\n",
            "iter 1890: loss 1.4126, time 338.41ms, mfu 1.56%\n",
            "iter 1900: loss 1.3945, time 331.27ms, mfu 1.57%\n",
            "iter 1910: loss 1.3707, time 326.95ms, mfu 1.58%\n",
            "iter 1920: loss 1.3650, time 333.98ms, mfu 1.58%\n",
            "iter 1930: loss 1.4406, time 327.00ms, mfu 1.59%\n",
            "iter 1940: loss 1.4245, time 329.20ms, mfu 1.59%\n",
            "iter 1950: loss 1.3525, time 330.90ms, mfu 1.60%\n",
            "iter 1960: loss 1.3680, time 335.76ms, mfu 1.60%\n",
            "iter 1970: loss 1.3683, time 333.14ms, mfu 1.60%\n",
            "iter 1980: loss 1.4600, time 331.49ms, mfu 1.60%\n",
            "iter 1990: loss 1.3450, time 329.13ms, mfu 1.61%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 32/32: b128_L4_H8_E256_BS16_MI2000_D20_s32 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b128_L4_H8_E256_BS16_MI2000_D20_s32.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 128\n",
            "n_layer = 4\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 32\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 81,920\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 3.16M\n",
            "num decayed parameter tensors: 18, with 3,195,136 parameters\n",
            "num non-decayed parameter tensors: 9, with 2,304 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1813, val loss 4.1755\n",
            "iter 0: loss 4.1889, time 2085.88ms, mfu -100.00%\n",
            "iter 10: loss 4.1331, time 331.45ms, mfu 1.63%\n",
            "iter 20: loss 4.0392, time 338.68ms, mfu 1.63%\n",
            "iter 30: loss 3.8710, time 339.73ms, mfu 1.62%\n",
            "iter 40: loss 3.7006, time 339.68ms, mfu 1.62%\n",
            "iter 50: loss 3.5801, time 336.02ms, mfu 1.62%\n",
            "iter 60: loss 3.5169, time 352.50ms, mfu 1.61%\n",
            "iter 70: loss 3.4320, time 331.46ms, mfu 1.61%\n",
            "iter 80: loss 3.3550, time 334.34ms, mfu 1.61%\n",
            "iter 90: loss 3.2848, time 336.47ms, mfu 1.61%\n",
            "iter 100: loss 3.1804, time 329.07ms, mfu 1.61%\n",
            "iter 110: loss 3.0699, time 332.00ms, mfu 1.61%\n",
            "iter 120: loss 3.0937, time 328.33ms, mfu 1.62%\n",
            "iter 130: loss 3.0170, time 340.25ms, mfu 1.61%\n",
            "iter 140: loss 2.9280, time 336.29ms, mfu 1.61%\n",
            "iter 150: loss 2.8584, time 335.28ms, mfu 1.61%\n",
            "iter 160: loss 2.9197, time 333.68ms, mfu 1.61%\n",
            "iter 170: loss 2.8721, time 332.11ms, mfu 1.61%\n",
            "iter 180: loss 2.7698, time 330.59ms, mfu 1.62%\n",
            "iter 190: loss 2.7720, time 331.55ms, mfu 1.62%\n",
            "step 200: train loss 2.7176, val loss 2.7209\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 200: loss 2.7611, time 1835.25ms, mfu 1.49%\n",
            "iter 210: loss 2.7836, time 331.41ms, mfu 1.50%\n",
            "iter 220: loss 2.7402, time 336.06ms, mfu 1.51%\n",
            "iter 230: loss 2.6959, time 345.24ms, mfu 1.52%\n",
            "iter 240: loss 2.6739, time 332.54ms, mfu 1.53%\n",
            "iter 250: loss 2.6372, time 328.67ms, mfu 1.54%\n",
            "iter 260: loss 2.6772, time 339.93ms, mfu 1.54%\n",
            "iter 270: loss 2.6257, time 335.33ms, mfu 1.55%\n",
            "iter 280: loss 2.6003, time 342.16ms, mfu 1.55%\n",
            "iter 290: loss 2.5718, time 329.66ms, mfu 1.56%\n",
            "iter 300: loss 2.5487, time 331.06ms, mfu 1.57%\n",
            "iter 310: loss 2.5847, time 332.48ms, mfu 1.57%\n",
            "iter 320: loss 2.5496, time 340.79ms, mfu 1.57%\n",
            "iter 330: loss 2.5144, time 330.07ms, mfu 1.58%\n",
            "iter 340: loss 2.5615, time 332.47ms, mfu 1.59%\n",
            "iter 350: loss 2.5645, time 336.62ms, mfu 1.59%\n",
            "iter 360: loss 2.5307, time 351.13ms, mfu 1.58%\n",
            "iter 370: loss 2.4932, time 335.30ms, mfu 1.58%\n",
            "iter 380: loss 2.5042, time 361.14ms, mfu 1.58%\n",
            "iter 390: loss 2.4912, time 335.14ms, mfu 1.58%\n",
            "step 400: train loss 2.4352, val loss 2.4452\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 400: loss 2.4417, time 1853.15ms, mfu 1.45%\n",
            "iter 410: loss 2.4829, time 335.60ms, mfu 1.47%\n",
            "iter 420: loss 2.4336, time 335.19ms, mfu 1.48%\n",
            "iter 430: loss 2.4040, time 334.87ms, mfu 1.49%\n",
            "iter 440: loss 2.4504, time 331.86ms, mfu 1.51%\n",
            "iter 450: loss 2.3725, time 328.49ms, mfu 1.52%\n",
            "iter 460: loss 2.4309, time 332.28ms, mfu 1.53%\n",
            "iter 470: loss 2.4124, time 333.76ms, mfu 1.54%\n",
            "iter 480: loss 2.4008, time 403.72ms, mfu 1.52%\n",
            "iter 490: loss 2.3756, time 341.80ms, mfu 1.53%\n",
            "iter 500: loss 2.3675, time 337.95ms, mfu 1.53%\n",
            "iter 510: loss 2.3581, time 334.70ms, mfu 1.54%\n",
            "iter 520: loss 2.3954, time 330.63ms, mfu 1.55%\n",
            "iter 530: loss 2.4107, time 333.14ms, mfu 1.56%\n",
            "iter 540: loss 2.3158, time 337.77ms, mfu 1.56%\n",
            "iter 550: loss 2.3303, time 346.42ms, mfu 1.56%\n",
            "iter 560: loss 2.2852, time 328.76ms, mfu 1.57%\n",
            "iter 570: loss 2.3248, time 329.57ms, mfu 1.58%\n",
            "iter 580: loss 2.2725, time 336.20ms, mfu 1.58%\n",
            "iter 590: loss 2.2671, time 328.73ms, mfu 1.59%\n",
            "step 600: train loss 2.2247, val loss 2.2472\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 600: loss 2.3707, time 1778.16ms, mfu 1.46%\n",
            "iter 610: loss 2.3280, time 327.90ms, mfu 1.48%\n",
            "iter 620: loss 2.3330, time 327.42ms, mfu 1.49%\n",
            "iter 630: loss 2.2437, time 343.14ms, mfu 1.50%\n",
            "iter 640: loss 2.2788, time 332.01ms, mfu 1.51%\n",
            "iter 650: loss 2.2498, time 341.84ms, mfu 1.52%\n",
            "iter 660: loss 2.2351, time 329.80ms, mfu 1.53%\n",
            "iter 670: loss 2.2434, time 329.94ms, mfu 1.54%\n",
            "iter 680: loss 2.2854, time 341.42ms, mfu 1.55%\n",
            "iter 690: loss 2.2390, time 340.18ms, mfu 1.55%\n",
            "iter 700: loss 2.1783, time 338.59ms, mfu 1.55%\n",
            "iter 710: loss 2.1413, time 343.22ms, mfu 1.56%\n",
            "iter 720: loss 2.2145, time 339.16ms, mfu 1.56%\n",
            "iter 730: loss 2.1922, time 334.16ms, mfu 1.57%\n",
            "iter 740: loss 2.1756, time 331.05ms, mfu 1.57%\n",
            "iter 750: loss 2.1850, time 349.08ms, mfu 1.57%\n",
            "iter 760: loss 2.1173, time 342.49ms, mfu 1.57%\n",
            "iter 770: loss 2.1672, time 334.80ms, mfu 1.57%\n",
            "iter 780: loss 2.1383, time 331.50ms, mfu 1.58%\n",
            "iter 790: loss 2.1567, time 351.52ms, mfu 1.58%\n",
            "step 800: train loss 2.0080, val loss 2.0781\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 800: loss 2.1211, time 1744.71ms, mfu 1.45%\n",
            "iter 810: loss 2.0865, time 328.85ms, mfu 1.47%\n",
            "iter 820: loss 2.1315, time 329.85ms, mfu 1.48%\n",
            "iter 830: loss 2.0553, time 357.79ms, mfu 1.49%\n",
            "iter 840: loss 2.0773, time 338.60ms, mfu 1.50%\n",
            "iter 850: loss 2.0729, time 338.38ms, mfu 1.51%\n",
            "iter 860: loss 2.0568, time 351.45ms, mfu 1.51%\n",
            "iter 870: loss 2.0297, time 339.76ms, mfu 1.52%\n",
            "iter 880: loss 2.0644, time 340.07ms, mfu 1.53%\n",
            "iter 890: loss 2.0110, time 336.70ms, mfu 1.53%\n",
            "iter 900: loss 2.0557, time 339.42ms, mfu 1.54%\n",
            "iter 910: loss 2.0323, time 330.31ms, mfu 1.55%\n",
            "iter 920: loss 1.9662, time 334.76ms, mfu 1.55%\n",
            "iter 930: loss 2.0202, time 354.44ms, mfu 1.55%\n",
            "iter 940: loss 1.9854, time 338.27ms, mfu 1.56%\n",
            "iter 950: loss 1.9360, time 340.78ms, mfu 1.56%\n",
            "iter 960: loss 1.9775, time 346.53ms, mfu 1.56%\n",
            "iter 970: loss 1.9981, time 334.22ms, mfu 1.56%\n",
            "iter 980: loss 1.9161, time 328.53ms, mfu 1.57%\n",
            "iter 990: loss 1.9162, time 331.91ms, mfu 1.58%\n",
            "step 1000: train loss 1.8126, val loss 1.9384\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1000: loss 1.8968, time 1796.65ms, mfu 1.45%\n",
            "iter 1010: loss 2.0150, time 333.45ms, mfu 1.47%\n",
            "iter 1020: loss 1.9128, time 331.04ms, mfu 1.48%\n",
            "iter 1030: loss 1.9343, time 340.62ms, mfu 1.49%\n",
            "iter 1040: loss 1.8393, time 353.59ms, mfu 1.50%\n",
            "iter 1050: loss 1.9783, time 340.83ms, mfu 1.51%\n",
            "iter 1060: loss 1.8584, time 329.72ms, mfu 1.52%\n",
            "iter 1070: loss 1.9519, time 333.34ms, mfu 1.53%\n",
            "iter 1080: loss 1.8923, time 340.77ms, mfu 1.53%\n",
            "iter 1090: loss 1.8394, time 338.65ms, mfu 1.54%\n",
            "iter 1100: loss 1.9356, time 339.05ms, mfu 1.55%\n",
            "iter 1110: loss 1.8407, time 341.00ms, mfu 1.55%\n",
            "iter 1120: loss 1.7765, time 330.55ms, mfu 1.56%\n",
            "iter 1130: loss 1.8625, time 334.84ms, mfu 1.56%\n",
            "iter 1140: loss 1.8332, time 333.75ms, mfu 1.57%\n",
            "iter 1150: loss 1.8795, time 340.44ms, mfu 1.57%\n",
            "iter 1160: loss 1.7872, time 330.56ms, mfu 1.58%\n",
            "iter 1170: loss 1.7927, time 332.67ms, mfu 1.58%\n",
            "iter 1180: loss 1.8305, time 332.53ms, mfu 1.59%\n",
            "iter 1190: loss 1.8696, time 328.71ms, mfu 1.59%\n",
            "step 1200: train loss 1.6640, val loss 1.8318\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1200: loss 1.7388, time 1772.33ms, mfu 1.46%\n",
            "iter 1210: loss 1.7850, time 334.57ms, mfu 1.48%\n",
            "iter 1220: loss 1.7908, time 330.68ms, mfu 1.49%\n",
            "iter 1230: loss 1.8284, time 333.95ms, mfu 1.51%\n",
            "iter 1240: loss 1.7253, time 335.36ms, mfu 1.52%\n",
            "iter 1250: loss 1.8374, time 347.68ms, mfu 1.52%\n",
            "iter 1260: loss 1.6876, time 334.99ms, mfu 1.53%\n",
            "iter 1270: loss 1.7057, time 334.38ms, mfu 1.54%\n",
            "iter 1280: loss 1.6992, time 333.69ms, mfu 1.55%\n",
            "iter 1290: loss 1.6625, time 336.23ms, mfu 1.55%\n",
            "iter 1300: loss 1.7659, time 332.72ms, mfu 1.56%\n",
            "iter 1310: loss 1.7007, time 333.83ms, mfu 1.56%\n",
            "iter 1320: loss 1.7669, time 347.70ms, mfu 1.56%\n",
            "iter 1330: loss 1.7263, time 334.60ms, mfu 1.57%\n",
            "iter 1340: loss 1.6896, time 338.05ms, mfu 1.57%\n",
            "iter 1350: loss 1.7333, time 331.73ms, mfu 1.58%\n",
            "iter 1360: loss 1.7210, time 342.74ms, mfu 1.58%\n",
            "iter 1370: loss 1.6556, time 335.02ms, mfu 1.58%\n",
            "iter 1380: loss 1.7106, time 332.44ms, mfu 1.58%\n",
            "iter 1390: loss 1.6568, time 331.42ms, mfu 1.59%\n",
            "step 1400: train loss 1.5676, val loss 1.7456\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1400: loss 1.6570, time 1788.53ms, mfu 1.46%\n",
            "iter 1410: loss 1.6009, time 341.75ms, mfu 1.47%\n",
            "iter 1420: loss 1.7678, time 338.53ms, mfu 1.48%\n",
            "iter 1430: loss 1.7275, time 337.11ms, mfu 1.50%\n",
            "iter 1440: loss 1.6400, time 338.74ms, mfu 1.51%\n",
            "iter 1450: loss 1.6359, time 334.87ms, mfu 1.52%\n",
            "iter 1460: loss 1.6361, time 348.49ms, mfu 1.52%\n",
            "iter 1470: loss 1.6402, time 334.85ms, mfu 1.53%\n",
            "iter 1480: loss 1.6005, time 335.07ms, mfu 1.54%\n",
            "iter 1490: loss 1.6821, time 338.25ms, mfu 1.54%\n",
            "iter 1500: loss 1.6943, time 343.67ms, mfu 1.55%\n",
            "iter 1510: loss 1.6152, time 335.50ms, mfu 1.55%\n",
            "iter 1520: loss 1.6373, time 335.85ms, mfu 1.56%\n",
            "iter 1530: loss 1.5733, time 344.95ms, mfu 1.56%\n",
            "iter 1540: loss 1.6082, time 332.19ms, mfu 1.57%\n",
            "iter 1550: loss 1.6785, time 331.30ms, mfu 1.57%\n",
            "iter 1560: loss 1.5128, time 328.52ms, mfu 1.58%\n",
            "iter 1570: loss 1.6684, time 333.99ms, mfu 1.58%\n",
            "iter 1580: loss 1.6365, time 328.71ms, mfu 1.59%\n",
            "iter 1590: loss 1.5071, time 329.29ms, mfu 1.59%\n",
            "step 1600: train loss 1.4917, val loss 1.6815\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1600: loss 1.7096, time 1808.63ms, mfu 1.46%\n",
            "iter 1610: loss 1.5888, time 332.03ms, mfu 1.48%\n",
            "iter 1620: loss 1.5808, time 338.09ms, mfu 1.49%\n",
            "iter 1630: loss 1.5834, time 334.88ms, mfu 1.50%\n",
            "iter 1640: loss 1.6074, time 338.50ms, mfu 1.51%\n",
            "iter 1650: loss 1.5365, time 337.21ms, mfu 1.52%\n",
            "iter 1660: loss 1.5146, time 334.26ms, mfu 1.53%\n",
            "iter 1670: loss 1.5637, time 349.49ms, mfu 1.53%\n",
            "iter 1680: loss 1.5300, time 338.02ms, mfu 1.54%\n",
            "iter 1690: loss 1.4885, time 330.28ms, mfu 1.55%\n",
            "iter 1700: loss 1.5784, time 330.96ms, mfu 1.56%\n",
            "iter 1710: loss 1.6506, time 328.64ms, mfu 1.57%\n",
            "iter 1720: loss 1.5983, time 335.49ms, mfu 1.57%\n",
            "iter 1730: loss 1.4989, time 333.28ms, mfu 1.57%\n",
            "iter 1740: loss 1.5863, time 329.24ms, mfu 1.58%\n",
            "iter 1750: loss 1.5304, time 342.57ms, mfu 1.58%\n",
            "iter 1760: loss 1.6740, time 335.32ms, mfu 1.58%\n",
            "iter 1770: loss 1.5576, time 338.28ms, mfu 1.58%\n",
            "iter 1780: loss 1.4813, time 344.88ms, mfu 1.58%\n",
            "iter 1790: loss 1.5416, time 334.21ms, mfu 1.59%\n",
            "step 1800: train loss 1.4325, val loss 1.6276\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1800: loss 1.6104, time 1774.86ms, mfu 1.46%\n",
            "iter 1810: loss 1.4690, time 332.56ms, mfu 1.47%\n",
            "iter 1820: loss 1.5553, time 339.46ms, mfu 1.49%\n",
            "iter 1830: loss 1.6017, time 339.38ms, mfu 1.50%\n",
            "iter 1840: loss 1.6118, time 340.32ms, mfu 1.51%\n",
            "iter 1850: loss 1.5682, time 341.63ms, mfu 1.51%\n",
            "iter 1860: loss 1.5104, time 342.60ms, mfu 1.52%\n",
            "iter 1870: loss 1.4466, time 330.84ms, mfu 1.53%\n",
            "iter 1880: loss 1.4027, time 328.53ms, mfu 1.54%\n",
            "iter 1890: loss 1.5361, time 335.40ms, mfu 1.55%\n",
            "iter 1900: loss 1.5316, time 334.94ms, mfu 1.55%\n",
            "iter 1910: loss 1.4669, time 332.36ms, mfu 1.56%\n",
            "iter 1920: loss 1.4505, time 337.03ms, mfu 1.57%\n",
            "iter 1930: loss 1.5683, time 328.62ms, mfu 1.57%\n",
            "iter 1940: loss 1.5195, time 333.38ms, mfu 1.58%\n",
            "iter 1950: loss 1.4611, time 330.50ms, mfu 1.58%\n",
            "iter 1960: loss 1.4938, time 348.22ms, mfu 1.58%\n",
            "iter 1970: loss 1.4701, time 339.45ms, mfu 1.58%\n",
            "iter 1980: loss 1.5692, time 332.74ms, mfu 1.59%\n",
            "iter 1990: loss 1.4353, time 338.46ms, mfu 1.59%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/nanoGPT_results\"\n",
        "samples_dir = os.path.join(base_dir, \"samples\")\n",
        "\n",
        "os.makedirs(samples_dir, exist_ok=True)\n",
        "\n",
        "# iterate over each experiment folder\n",
        "exp_folders = sorted([\n",
        "    d for d in os.listdir(base_dir)\n",
        "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"b\")\n",
        "])\n",
        "\n",
        "print(f\"Found {len(exp_folders)} experiment folders.\")\n",
        "\n",
        "for i, exp in enumerate(exp_folders, 1):\n",
        "    exp_path = os.path.join(base_dir, exp)\n",
        "    ckpt_path = os.path.join(exp_path, \"ckpt.pt\")\n",
        "\n",
        "    if not os.path.isfile(ckpt_path):\n",
        "        print(f\"Skipping {exp} (no ckpt.pt found)\")\n",
        "        continue\n",
        "\n",
        "    out_sample = os.path.join(samples_dir, f\"{exp}_sample.txt\")\n",
        "\n",
        "    print(f\"[{i}/{len(exp_folders)}] Generating sample for {exp}\")\n",
        "\n",
        "    cmd = (\n",
        "        f\"python /content/nanoGPT/sample.py \"\n",
        "        f\"--out_dir={exp_path} \"\n",
        "        f\"--start=' ' \"\n",
        "        f\"--num_samples=3 \"\n",
        "        f\"--max_new_tokens=200 \"\n",
        "        f\"> '{out_sample}'\"\n",
        "    )\n",
        "    subprocess.run(cmd, shell=True)\n",
        "\n",
        "print(\"✅ All samples generated and stored in:\", samples_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgxwC_rZcapN",
        "outputId": "a20d4394-2fdb-43ec-d5cd-f7853d308168"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 32 experiment folders.\n",
            "[1/32] Generating sample for b128_L4_H4_E128_BS16_MI1000_D10_s5\n",
            "[2/32] Generating sample for b128_L4_H4_E128_BS16_MI1000_D20_s6\n",
            "[3/32] Generating sample for b128_L4_H4_E128_BS16_MI2000_D10_s7\n",
            "[4/32] Generating sample for b128_L4_H4_E128_BS16_MI2000_D20_s8\n",
            "[5/32] Generating sample for b128_L4_H4_E128_BS8_MI1000_D10_s1\n",
            "[6/32] Generating sample for b128_L4_H4_E128_BS8_MI1000_D20_s2\n",
            "[7/32] Generating sample for b128_L4_H4_E128_BS8_MI2000_D10_s3\n",
            "[8/32] Generating sample for b128_L4_H4_E128_BS8_MI2000_D20_s4\n",
            "[9/32] Generating sample for b128_L4_H4_E256_BS16_MI1000_D10_s13\n",
            "[10/32] Generating sample for b128_L4_H4_E256_BS16_MI1000_D20_s14\n",
            "[11/32] Generating sample for b128_L4_H4_E256_BS16_MI2000_D10_s15\n",
            "[12/32] Generating sample for b128_L4_H4_E256_BS16_MI2000_D20_s16\n",
            "[13/32] Generating sample for b128_L4_H4_E256_BS8_MI1000_D10_s9\n",
            "[14/32] Generating sample for b128_L4_H4_E256_BS8_MI1000_D20_s10\n",
            "[15/32] Generating sample for b128_L4_H4_E256_BS8_MI2000_D10_s11\n",
            "[16/32] Generating sample for b128_L4_H4_E256_BS8_MI2000_D20_s12\n",
            "[17/32] Generating sample for b128_L4_H8_E128_BS16_MI1000_D10_s21\n",
            "[18/32] Generating sample for b128_L4_H8_E128_BS16_MI1000_D20_s22\n",
            "[19/32] Generating sample for b128_L4_H8_E128_BS16_MI2000_D10_s23\n",
            "[20/32] Generating sample for b128_L4_H8_E128_BS16_MI2000_D20_s24\n",
            "[21/32] Generating sample for b128_L4_H8_E128_BS8_MI1000_D10_s17\n",
            "[22/32] Generating sample for b128_L4_H8_E128_BS8_MI1000_D20_s18\n",
            "[23/32] Generating sample for b128_L4_H8_E128_BS8_MI2000_D10_s19\n",
            "[24/32] Generating sample for b128_L4_H8_E128_BS8_MI2000_D20_s20\n",
            "[25/32] Generating sample for b128_L4_H8_E256_BS16_MI1000_D10_s29\n",
            "[26/32] Generating sample for b128_L4_H8_E256_BS16_MI1000_D20_s30\n",
            "[27/32] Generating sample for b128_L4_H8_E256_BS16_MI2000_D10_s31\n",
            "[28/32] Generating sample for b128_L4_H8_E256_BS16_MI2000_D20_s32\n",
            "[29/32] Generating sample for b128_L4_H8_E256_BS8_MI1000_D10_s25\n",
            "[30/32] Generating sample for b128_L4_H8_E256_BS8_MI1000_D20_s26\n",
            "[31/32] Generating sample for b128_L4_H8_E256_BS8_MI2000_D10_s27\n",
            "[32/32] Generating sample for b128_L4_H8_E256_BS8_MI2000_D20_s28\n",
            "✅ All samples generated and stored in: /content/drive/MyDrive/nanoGPT_results/samples\n"
          ]
        }
      ]
    }
  ]
}
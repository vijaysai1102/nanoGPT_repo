{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgWCPgScL-y4",
        "outputId": "ae7823b3-0f1b-4669-dbf6-6f24b3a9acee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Results will be saved to: /content/drive/MyDrive/nanoGPT_results\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/nanoGPT_results\"\n",
        "!mkdir -p \"$SAVE_DIR\"\n",
        "\n",
        "print(\"Results will be saved to:\", SAVE_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install tqdm numpy requests matplotlib ninja\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq9nTCaKMAXH",
        "outputId": "72363c3c-25aa-43e5-b913-d34675727bd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\n",
            "/content/nanoGPT\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data/shakespeare_char\n",
        "!python prepare.py\n",
        "%cd ../..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkHXYlRBMAO3",
        "outputId": "678f46ee-591e-4d48-d8cd-e85c59ebbf9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT/data/shakespeare_char\n",
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_experiments.py\n",
        "import os, sys, itertools, subprocess, re, csv, time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "SAVE_DIR = os.environ.get(\"SAVE_DIR\", \"/content/drive/MyDrive/nanoGPT_results\")\n",
        "\n",
        "# Full Hyperparameter Grid\n",
        "BLOCK_SIZES = [64, 128]\n",
        "N_LAYERS = [4, 6]\n",
        "N_HEADS = [4, 8]\n",
        "N_EMBDS = [128, 256]\n",
        "BATCH_SIZES = [8, 16]\n",
        "MAX_ITERS = [1000, 2000]\n",
        "DROPOUTS = [0.1, 0.2]\n",
        "\n",
        "# Member → fixed hyperparams\n",
        "MEMBER_MAP = {\n",
        "    1: (64, 4),\n",
        "    2: (64, 6),\n",
        "    3: (128, 4),\n",
        "    4: (128, 6),\n",
        "}\n",
        "\n",
        "CONFIG_TEMPLATE = r\"\"\"\n",
        "out_dir = \"{save_dir}/{out_name}\"\n",
        "dataset = \"shakespeare_char\"\n",
        "eval_interval = 200\n",
        "log_interval = 10\n",
        "always_save_checkpoint = True\n",
        "\n",
        "batch_size = {batch_size}\n",
        "block_size = {block_size}\n",
        "n_layer = {n_layer}\n",
        "n_head = {n_head}\n",
        "n_embd = {n_embd}\n",
        "dropout = {dropout}\n",
        "\n",
        "learning_rate = 3e-4\n",
        "max_iters = {max_iters}\n",
        "lr_decay_iters = {max_iters}\n",
        "\n",
        "seed = {seed}\n",
        "device = \"{device}\"\n",
        "\n",
        "num_workers = 0\n",
        "compile = False\n",
        "\"\"\"\n",
        "\n",
        "def list_experiments(member_id):\n",
        "    block_size, n_layer = MEMBER_MAP[member_id]\n",
        "    grid = list(itertools.product(N_HEADS, N_EMBDS, BATCH_SIZES, MAX_ITERS, DROPOUTS))\n",
        "    exps = []\n",
        "    for seed, (nh, ne, bs, mi, do) in enumerate(grid, 1):\n",
        "        out_name = f\"b{block_size}_L{n_layer}_H{nh}_E{ne}_BS{bs}_MI{mi}_D{int(do*100)}_s{seed}\"\n",
        "        exps.append({\n",
        "            \"block_size\": block_size, \"n_layer\": n_layer,\n",
        "            \"n_head\": nh, \"n_embd\": ne,\n",
        "            \"batch_size\": bs, \"max_iters\": mi,\n",
        "            \"dropout\": do, \"seed\": seed,\n",
        "            \"out_name\": out_name\n",
        "        })\n",
        "    return exps\n",
        "\n",
        "def parse_losses(stdout_line):\n",
        "    m = re.search(r\"train loss ([0-9.]+).*val loss ([0-9.]+)\", stdout_line)\n",
        "    if m:\n",
        "        return float(m.group(1)), float(m.group(2))\n",
        "    return None, None\n",
        "\n",
        "def extract_model_params(logtext):\n",
        "    m = re.search(r\"number of parameters:\\s*([0-9.]+)M\", logtext)\n",
        "    if m:\n",
        "        return float(m.group(1)) * 1e6\n",
        "    return None\n",
        "\n",
        "def run_training(cfg, device):\n",
        "    cfg_file = Path(f\"{cfg['out_name']}.py\")\n",
        "    cfg_file.write_text(CONFIG_TEMPLATE.format(**cfg, save_dir=SAVE_DIR, device=device))\n",
        "\n",
        "    p = subprocess.Popen(\n",
        "        [\"python\", \"train.py\", str(cfg_file)],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "    )\n",
        "\n",
        "    train_loss = None\n",
        "    val_loss = None\n",
        "    param_count = None\n",
        "    log_buf = \"\"\n",
        "\n",
        "    for line in p.stdout:\n",
        "        print(line, end=\"\")\n",
        "        log_buf += line\n",
        "        tl, vl = parse_losses(line)\n",
        "        if tl is not None:\n",
        "            train_loss, val_loss = tl, vl\n",
        "\n",
        "        if param_count is None:\n",
        "            param_count = extract_model_params(log_buf)\n",
        "\n",
        "    p.wait()\n",
        "    loss_gap = val_loss - train_loss if train_loss and val_loss else None\n",
        "    return train_loss, val_loss, loss_gap, param_count, cfg_file\n",
        "\n",
        "def main():\n",
        "    member_id = int(sys.argv[1])\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    result_csv = Path(SAVE_DIR) / \"results.csv\"\n",
        "    if not result_csv.exists():\n",
        "        with open(result_csv, \"w\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                \"Experiment\",\n",
        "                \"Train Loss\",\n",
        "                \"Val Loss\",\n",
        "                \"Loss Gap\",\n",
        "                \"Total Params\",\n",
        "                \"Config Path\"\n",
        "            ])\n",
        "\n",
        "    exps = list_experiments(member_id)\n",
        "    print(f\"Running {len(exps)} experiments for Member {member_id}\")\n",
        "\n",
        "    for i, exp in enumerate(exps, 1):\n",
        "        print(f\"\\n=== Experiment {i}/{len(exps)}: {exp['out_name']} ===\")\n",
        "        tr, vl, gap, params, cfg_path = run_training(exp, device)\n",
        "\n",
        "        with open(result_csv, \"a\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                exp[\"out_name\"], tr, vl, gap, params, str(cfg_path.resolve())\n",
        "            ])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJRlL-PYMAE8",
        "outputId": "8c0a7b8e-49e1-4382-ac81-d2f6df181717"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_experiments.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SAVE_DIR\"] = SAVE_DIR\n"
      ],
      "metadata": {
        "id": "Gtxeo5cGL_5O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_experiments.py 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnILrA6ZL_x0",
        "outputId": "6368d139-1f68-4439-a767-d0b89ffda8d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iter 1440: loss 1.9429, time 441.87ms, mfu 0.21%\n",
            "iter 1450: loss 2.0295, time 451.32ms, mfu 0.21%\n",
            "iter 1460: loss 1.8885, time 448.29ms, mfu 0.22%\n",
            "iter 1470: loss 1.8857, time 447.25ms, mfu 0.22%\n",
            "iter 1480: loss 1.8402, time 445.90ms, mfu 0.22%\n",
            "iter 1490: loss 1.9817, time 445.47ms, mfu 0.22%\n",
            "iter 1500: loss 1.9517, time 445.90ms, mfu 0.22%\n",
            "iter 1510: loss 1.9383, time 459.37ms, mfu 0.22%\n",
            "iter 1520: loss 1.8172, time 449.07ms, mfu 0.22%\n",
            "iter 1530: loss 1.9190, time 449.17ms, mfu 0.22%\n",
            "iter 1540: loss 1.8097, time 446.29ms, mfu 0.22%\n",
            "iter 1550: loss 1.8602, time 446.01ms, mfu 0.22%\n",
            "iter 1560: loss 1.8831, time 448.57ms, mfu 0.22%\n",
            "iter 1570: loss 1.8380, time 441.78ms, mfu 0.22%\n",
            "iter 1580: loss 1.9441, time 448.88ms, mfu 0.22%\n",
            "iter 1590: loss 1.9122, time 446.00ms, mfu 0.22%\n",
            "step 1600: train loss 1.7423, val loss 1.8972\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E128_BS16_MI2000_D20_s8\n",
            "iter 1600: loss 1.8964, time 2184.57ms, mfu 0.21%\n",
            "iter 1610: loss 1.8695, time 444.91ms, mfu 0.21%\n",
            "iter 1620: loss 1.8855, time 443.30ms, mfu 0.21%\n",
            "iter 1630: loss 1.8276, time 442.76ms, mfu 0.21%\n",
            "iter 1640: loss 1.7372, time 455.65ms, mfu 0.21%\n",
            "iter 1650: loss 1.7901, time 443.03ms, mfu 0.21%\n",
            "iter 1660: loss 1.9025, time 440.19ms, mfu 0.22%\n",
            "iter 1670: loss 1.8918, time 464.20ms, mfu 0.22%\n",
            "iter 1680: loss 1.8688, time 447.27ms, mfu 0.22%\n",
            "iter 1690: loss 1.7989, time 443.88ms, mfu 0.22%\n",
            "iter 1700: loss 1.7651, time 444.60ms, mfu 0.22%\n",
            "iter 1710: loss 1.7268, time 447.09ms, mfu 0.22%\n",
            "iter 1720: loss 1.7532, time 456.15ms, mfu 0.22%\n",
            "iter 1730: loss 1.8245, time 446.48ms, mfu 0.22%\n",
            "iter 1740: loss 1.6901, time 446.01ms, mfu 0.22%\n",
            "iter 1750: loss 1.8461, time 451.46ms, mfu 0.22%\n",
            "iter 1760: loss 1.9396, time 438.09ms, mfu 0.22%\n",
            "iter 1770: loss 1.7620, time 441.51ms, mfu 0.22%\n",
            "iter 1780: loss 1.8732, time 443.05ms, mfu 0.22%\n",
            "iter 1790: loss 1.8029, time 447.62ms, mfu 0.22%\n",
            "step 1800: train loss 1.6670, val loss 1.8337\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E128_BS16_MI2000_D20_s8\n",
            "iter 1800: loss 1.7592, time 2211.27ms, mfu 0.21%\n",
            "iter 1810: loss 1.7934, time 440.80ms, mfu 0.21%\n",
            "iter 1820: loss 1.6891, time 438.53ms, mfu 0.21%\n",
            "iter 1830: loss 1.7752, time 453.75ms, mfu 0.21%\n",
            "iter 1840: loss 1.8566, time 453.63ms, mfu 0.21%\n",
            "iter 1850: loss 1.8358, time 453.87ms, mfu 0.21%\n",
            "iter 1860: loss 1.7757, time 451.78ms, mfu 0.22%\n",
            "iter 1870: loss 1.8581, time 452.70ms, mfu 0.22%\n",
            "iter 1880: loss 1.7875, time 454.49ms, mfu 0.22%\n",
            "iter 1890: loss 1.7834, time 457.95ms, mfu 0.22%\n",
            "iter 1900: loss 1.6890, time 451.63ms, mfu 0.22%\n",
            "iter 1910: loss 1.7249, time 443.32ms, mfu 0.22%\n",
            "iter 1920: loss 1.6933, time 447.41ms, mfu 0.22%\n",
            "iter 1930: loss 1.7313, time 447.28ms, mfu 0.22%\n",
            "iter 1940: loss 1.7410, time 442.25ms, mfu 0.22%\n",
            "iter 1950: loss 1.7235, time 443.14ms, mfu 0.22%\n",
            "iter 1960: loss 1.7415, time 448.00ms, mfu 0.22%\n",
            "iter 1970: loss 1.7053, time 440.48ms, mfu 0.22%\n",
            "iter 1980: loss 1.7635, time 445.31ms, mfu 0.22%\n",
            "iter 1990: loss 1.6656, time 476.30ms, mfu 0.22%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 9/32: b64_L6_H4_E256_BS8_MI1000_D10_s9 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS8_MI1000_D10_s9.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 9\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2409, val loss 4.2377\n",
            "iter 0: loss 4.2595, time 2416.19ms, mfu -100.00%\n",
            "iter 10: loss 4.1630, time 430.01ms, mfu 0.45%\n",
            "iter 20: loss 3.9602, time 432.70ms, mfu 0.45%\n",
            "iter 30: loss 3.7573, time 440.91ms, mfu 0.45%\n",
            "iter 40: loss 3.6041, time 426.39ms, mfu 0.45%\n",
            "iter 50: loss 3.4337, time 429.88ms, mfu 0.45%\n",
            "iter 60: loss 3.4373, time 432.97ms, mfu 0.45%\n",
            "iter 70: loss 3.3312, time 432.21ms, mfu 0.45%\n",
            "iter 80: loss 3.2026, time 431.67ms, mfu 0.45%\n",
            "iter 90: loss 3.1079, time 432.26ms, mfu 0.45%\n",
            "iter 100: loss 2.9660, time 430.29ms, mfu 0.45%\n",
            "iter 110: loss 3.0429, time 432.42ms, mfu 0.45%\n",
            "iter 120: loss 2.9103, time 425.49ms, mfu 0.45%\n",
            "iter 130: loss 2.8064, time 429.77ms, mfu 0.45%\n",
            "iter 140: loss 2.8048, time 430.33ms, mfu 0.45%\n",
            "iter 150: loss 2.8009, time 434.34ms, mfu 0.45%\n",
            "iter 160: loss 2.7884, time 430.42ms, mfu 0.45%\n",
            "iter 170: loss 2.8185, time 433.03ms, mfu 0.45%\n",
            "iter 180: loss 2.7472, time 428.34ms, mfu 0.45%\n",
            "iter 190: loss 2.7209, time 430.41ms, mfu 0.45%\n",
            "step 200: train loss 2.6772, val loss 2.6887\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 200: loss 2.7164, time 2185.49ms, mfu 0.41%\n",
            "iter 210: loss 2.6671, time 434.34ms, mfu 0.42%\n",
            "iter 220: loss 2.6315, time 429.38ms, mfu 0.42%\n",
            "iter 230: loss 2.6769, time 427.35ms, mfu 0.42%\n",
            "iter 240: loss 2.6589, time 432.30ms, mfu 0.43%\n",
            "iter 250: loss 2.6797, time 440.44ms, mfu 0.43%\n",
            "iter 260: loss 2.5618, time 429.16ms, mfu 0.43%\n",
            "iter 270: loss 2.5633, time 430.40ms, mfu 0.43%\n",
            "iter 280: loss 2.5719, time 435.73ms, mfu 0.43%\n",
            "iter 290: loss 2.5764, time 430.75ms, mfu 0.44%\n",
            "iter 300: loss 2.5648, time 428.60ms, mfu 0.44%\n",
            "iter 310: loss 2.5107, time 431.65ms, mfu 0.44%\n",
            "iter 320: loss 2.4797, time 433.41ms, mfu 0.44%\n",
            "iter 330: loss 2.4736, time 429.58ms, mfu 0.44%\n",
            "iter 340: loss 2.4977, time 430.70ms, mfu 0.44%\n",
            "iter 350: loss 2.4248, time 428.51ms, mfu 0.44%\n",
            "iter 360: loss 2.3528, time 434.67ms, mfu 0.44%\n",
            "iter 370: loss 2.4522, time 439.28ms, mfu 0.44%\n",
            "iter 380: loss 2.3904, time 432.77ms, mfu 0.44%\n",
            "iter 390: loss 2.3467, time 442.26ms, mfu 0.44%\n",
            "step 400: train loss 2.3367, val loss 2.3511\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 400: loss 2.3821, time 2164.97ms, mfu 0.41%\n",
            "iter 410: loss 2.2685, time 432.66ms, mfu 0.41%\n",
            "iter 420: loss 2.3832, time 430.02ms, mfu 0.42%\n",
            "iter 430: loss 2.4519, time 440.41ms, mfu 0.42%\n",
            "iter 440: loss 2.3102, time 444.41ms, mfu 0.42%\n",
            "iter 450: loss 2.2991, time 432.61ms, mfu 0.42%\n",
            "iter 460: loss 2.4112, time 441.41ms, mfu 0.43%\n",
            "iter 470: loss 2.2236, time 438.52ms, mfu 0.43%\n",
            "iter 480: loss 2.3088, time 436.46ms, mfu 0.43%\n",
            "iter 490: loss 2.2470, time 434.68ms, mfu 0.43%\n",
            "iter 500: loss 2.2213, time 438.61ms, mfu 0.43%\n",
            "iter 510: loss 2.2841, time 431.87ms, mfu 0.43%\n",
            "iter 520: loss 2.1976, time 427.98ms, mfu 0.44%\n",
            "iter 530: loss 2.1625, time 429.54ms, mfu 0.44%\n",
            "iter 540: loss 2.2451, time 429.36ms, mfu 0.44%\n",
            "iter 550: loss 2.1920, time 430.91ms, mfu 0.44%\n",
            "iter 560: loss 2.0665, time 428.12ms, mfu 0.44%\n",
            "iter 570: loss 2.1899, time 424.51ms, mfu 0.44%\n",
            "iter 580: loss 2.1952, time 434.41ms, mfu 0.44%\n",
            "iter 590: loss 2.0326, time 436.16ms, mfu 0.44%\n",
            "step 600: train loss 2.0730, val loss 2.1194\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 600: loss 2.1331, time 2179.84ms, mfu 0.41%\n",
            "iter 610: loss 2.1617, time 432.82ms, mfu 0.41%\n",
            "iter 620: loss 2.1219, time 437.90ms, mfu 0.42%\n",
            "iter 630: loss 2.0724, time 433.57ms, mfu 0.42%\n",
            "iter 640: loss 2.1058, time 431.10ms, mfu 0.42%\n",
            "iter 650: loss 2.1034, time 425.81ms, mfu 0.43%\n",
            "iter 660: loss 1.9716, time 433.18ms, mfu 0.43%\n",
            "iter 670: loss 2.0467, time 430.01ms, mfu 0.43%\n",
            "iter 680: loss 1.9927, time 436.10ms, mfu 0.43%\n",
            "iter 690: loss 2.0876, time 434.23ms, mfu 0.43%\n",
            "iter 700: loss 2.0952, time 427.07ms, mfu 0.44%\n",
            "iter 710: loss 2.0316, time 429.62ms, mfu 0.44%\n",
            "iter 720: loss 2.1112, time 434.49ms, mfu 0.44%\n",
            "iter 730: loss 2.0110, time 436.21ms, mfu 0.44%\n",
            "iter 740: loss 1.9398, time 437.07ms, mfu 0.44%\n",
            "iter 750: loss 1.9554, time 423.44ms, mfu 0.44%\n",
            "iter 760: loss 1.9497, time 428.24ms, mfu 0.44%\n",
            "iter 770: loss 1.8691, time 432.58ms, mfu 0.44%\n",
            "iter 780: loss 2.0215, time 432.68ms, mfu 0.44%\n",
            "iter 790: loss 1.9310, time 432.04ms, mfu 0.44%\n",
            "step 800: train loss 1.8393, val loss 1.9467\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 800: loss 1.8239, time 2219.63ms, mfu 0.41%\n",
            "iter 810: loss 1.7910, time 430.76ms, mfu 0.41%\n",
            "iter 820: loss 2.1085, time 429.15ms, mfu 0.42%\n",
            "iter 830: loss 1.8710, time 447.88ms, mfu 0.42%\n",
            "iter 840: loss 1.8763, time 434.34ms, mfu 0.42%\n",
            "iter 850: loss 1.8380, time 431.93ms, mfu 0.42%\n",
            "iter 860: loss 1.8803, time 431.07ms, mfu 0.43%\n",
            "iter 870: loss 1.8894, time 435.29ms, mfu 0.43%\n",
            "iter 880: loss 1.7750, time 439.64ms, mfu 0.43%\n",
            "iter 890: loss 1.8111, time 433.49ms, mfu 0.43%\n",
            "iter 900: loss 1.8193, time 435.60ms, mfu 0.43%\n",
            "iter 910: loss 1.8781, time 446.97ms, mfu 0.43%\n",
            "iter 920: loss 1.8071, time 431.09ms, mfu 0.44%\n",
            "iter 930: loss 1.7922, time 425.39ms, mfu 0.44%\n",
            "iter 940: loss 1.6821, time 432.38ms, mfu 0.44%\n",
            "iter 950: loss 1.5708, time 432.55ms, mfu 0.44%\n",
            "iter 960: loss 1.6820, time 431.91ms, mfu 0.44%\n",
            "iter 970: loss 1.7308, time 431.92ms, mfu 0.44%\n",
            "iter 980: loss 1.7914, time 429.41ms, mfu 0.44%\n",
            "iter 990: loss 1.8862, time 444.42ms, mfu 0.44%\n",
            "step 1000: train loss 1.6640, val loss 1.8361\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "iter 1000: loss 1.7858, time 2183.69ms, mfu 0.41%\n",
            "\n",
            "=== Experiment 10/32: b64_L6_H4_E256_BS8_MI1000_D20_s10 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS8_MI1000_D20_s10.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 10\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2409, val loss 4.2377\n",
            "iter 0: loss 4.2493, time 2402.33ms, mfu -100.00%\n",
            "iter 10: loss 4.1666, time 436.47ms, mfu 0.45%\n",
            "iter 20: loss 4.0082, time 435.48ms, mfu 0.45%\n",
            "iter 30: loss 3.8069, time 440.24ms, mfu 0.45%\n",
            "iter 40: loss 3.6416, time 434.31ms, mfu 0.45%\n",
            "iter 50: loss 3.4722, time 436.73ms, mfu 0.45%\n",
            "iter 60: loss 3.4787, time 456.46ms, mfu 0.44%\n",
            "iter 70: loss 3.4058, time 445.60ms, mfu 0.44%\n",
            "iter 80: loss 3.2824, time 431.93ms, mfu 0.44%\n",
            "iter 90: loss 3.1852, time 436.29ms, mfu 0.44%\n",
            "iter 100: loss 3.0340, time 436.61ms, mfu 0.44%\n",
            "iter 110: loss 3.1268, time 434.07ms, mfu 0.44%\n",
            "iter 120: loss 2.9696, time 436.18ms, mfu 0.44%\n",
            "iter 130: loss 2.8613, time 438.48ms, mfu 0.44%\n",
            "iter 140: loss 2.8445, time 433.66ms, mfu 0.44%\n",
            "iter 150: loss 2.8448, time 438.10ms, mfu 0.44%\n",
            "iter 160: loss 2.8221, time 436.36ms, mfu 0.44%\n",
            "iter 170: loss 2.8672, time 446.85ms, mfu 0.44%\n",
            "iter 180: loss 2.7860, time 434.29ms, mfu 0.44%\n",
            "iter 190: loss 2.7681, time 428.87ms, mfu 0.44%\n",
            "step 200: train loss 2.6976, val loss 2.7058\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 200: loss 2.7452, time 2196.44ms, mfu 0.41%\n",
            "iter 210: loss 2.7078, time 438.87ms, mfu 0.41%\n",
            "iter 220: loss 2.6789, time 440.29ms, mfu 0.42%\n",
            "iter 230: loss 2.7250, time 427.59ms, mfu 0.42%\n",
            "iter 240: loss 2.7057, time 432.17ms, mfu 0.42%\n",
            "iter 250: loss 2.7251, time 443.68ms, mfu 0.42%\n",
            "iter 260: loss 2.6046, time 431.89ms, mfu 0.43%\n",
            "iter 270: loss 2.5934, time 429.92ms, mfu 0.43%\n",
            "iter 280: loss 2.6069, time 437.07ms, mfu 0.43%\n",
            "iter 290: loss 2.6148, time 434.00ms, mfu 0.43%\n",
            "iter 300: loss 2.5983, time 434.50ms, mfu 0.43%\n",
            "iter 310: loss 2.5490, time 433.14ms, mfu 0.44%\n",
            "iter 320: loss 2.5038, time 433.58ms, mfu 0.44%\n",
            "iter 330: loss 2.5029, time 435.89ms, mfu 0.44%\n",
            "iter 340: loss 2.5333, time 430.42ms, mfu 0.44%\n",
            "iter 350: loss 2.4567, time 439.62ms, mfu 0.44%\n",
            "iter 360: loss 2.4319, time 434.86ms, mfu 0.44%\n",
            "iter 370: loss 2.4945, time 431.11ms, mfu 0.44%\n",
            "iter 380: loss 2.4208, time 428.95ms, mfu 0.44%\n",
            "iter 390: loss 2.4022, time 430.30ms, mfu 0.44%\n",
            "step 400: train loss 2.3832, val loss 2.3951\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 400: loss 2.4471, time 2178.79ms, mfu 0.41%\n",
            "iter 410: loss 2.3620, time 441.93ms, mfu 0.41%\n",
            "iter 420: loss 2.4347, time 426.69ms, mfu 0.42%\n",
            "iter 430: loss 2.5352, time 444.17ms, mfu 0.42%\n",
            "iter 440: loss 2.3403, time 450.84ms, mfu 0.42%\n",
            "iter 450: loss 2.3992, time 439.68ms, mfu 0.42%\n",
            "iter 460: loss 2.4738, time 434.57ms, mfu 0.42%\n",
            "iter 470: loss 2.3039, time 435.25ms, mfu 0.43%\n",
            "iter 480: loss 2.3687, time 433.87ms, mfu 0.43%\n",
            "iter 490: loss 2.3394, time 429.58ms, mfu 0.43%\n",
            "iter 500: loss 2.3196, time 434.33ms, mfu 0.43%\n",
            "iter 510: loss 2.3863, time 433.09ms, mfu 0.43%\n",
            "iter 520: loss 2.2840, time 444.64ms, mfu 0.43%\n",
            "iter 530: loss 2.2470, time 438.20ms, mfu 0.44%\n",
            "iter 540: loss 2.3272, time 432.68ms, mfu 0.44%\n",
            "iter 550: loss 2.3321, time 445.30ms, mfu 0.44%\n",
            "iter 560: loss 2.1883, time 434.57ms, mfu 0.44%\n",
            "iter 570: loss 2.3106, time 434.37ms, mfu 0.44%\n",
            "iter 580: loss 2.3017, time 436.24ms, mfu 0.44%\n",
            "iter 590: loss 2.1640, time 437.76ms, mfu 0.44%\n",
            "step 600: train loss 2.1575, val loss 2.1835\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 600: loss 2.2270, time 2207.30ms, mfu 0.40%\n",
            "iter 610: loss 2.2683, time 429.80ms, mfu 0.41%\n",
            "iter 620: loss 2.2055, time 441.86ms, mfu 0.41%\n",
            "iter 630: loss 2.1807, time 471.39ms, mfu 0.41%\n",
            "iter 640: loss 2.2019, time 434.72ms, mfu 0.42%\n",
            "iter 650: loss 2.1847, time 433.17ms, mfu 0.42%\n",
            "iter 660: loss 2.0713, time 434.54ms, mfu 0.42%\n",
            "iter 670: loss 2.1911, time 438.61ms, mfu 0.42%\n",
            "iter 680: loss 2.0952, time 435.12ms, mfu 0.43%\n",
            "iter 690: loss 2.2061, time 432.94ms, mfu 0.43%\n",
            "iter 700: loss 2.1831, time 434.57ms, mfu 0.43%\n",
            "iter 710: loss 2.1405, time 442.88ms, mfu 0.43%\n",
            "iter 720: loss 2.1966, time 435.23ms, mfu 0.43%\n",
            "iter 730: loss 2.1076, time 434.76ms, mfu 0.43%\n",
            "iter 740: loss 2.1083, time 449.47ms, mfu 0.43%\n",
            "iter 750: loss 2.0437, time 440.94ms, mfu 0.43%\n",
            "iter 760: loss 2.0705, time 438.68ms, mfu 0.44%\n",
            "iter 770: loss 1.9924, time 433.46ms, mfu 0.44%\n",
            "iter 780: loss 2.1663, time 435.54ms, mfu 0.44%\n",
            "iter 790: loss 2.0364, time 443.12ms, mfu 0.44%\n",
            "step 800: train loss 1.9565, val loss 2.0263\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 800: loss 1.9401, time 2180.57ms, mfu 0.40%\n",
            "iter 810: loss 1.9551, time 445.15ms, mfu 0.41%\n",
            "iter 820: loss 2.2006, time 448.94ms, mfu 0.41%\n",
            "iter 830: loss 2.0265, time 443.14ms, mfu 0.41%\n",
            "iter 840: loss 2.0188, time 433.17ms, mfu 0.42%\n",
            "iter 850: loss 1.9605, time 431.92ms, mfu 0.42%\n",
            "iter 860: loss 2.0203, time 432.00ms, mfu 0.42%\n",
            "iter 870: loss 2.0258, time 437.49ms, mfu 0.42%\n",
            "iter 880: loss 1.9126, time 435.47ms, mfu 0.43%\n",
            "iter 890: loss 1.9347, time 438.00ms, mfu 0.43%\n",
            "iter 900: loss 1.9334, time 442.32ms, mfu 0.43%\n",
            "iter 910: loss 1.9797, time 436.55ms, mfu 0.43%\n",
            "iter 920: loss 1.9433, time 432.97ms, mfu 0.43%\n",
            "iter 930: loss 1.9222, time 450.79ms, mfu 0.43%\n",
            "iter 940: loss 1.8148, time 438.16ms, mfu 0.43%\n",
            "iter 950: loss 1.7329, time 433.47ms, mfu 0.44%\n",
            "iter 960: loss 1.7648, time 434.24ms, mfu 0.44%\n",
            "iter 970: loss 1.8707, time 437.71ms, mfu 0.44%\n",
            "iter 980: loss 1.9337, time 429.05ms, mfu 0.44%\n",
            "iter 990: loss 2.0236, time 434.55ms, mfu 0.44%\n",
            "step 1000: train loss 1.7787, val loss 1.9224\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "iter 1000: loss 1.9091, time 2196.47ms, mfu 0.40%\n",
            "\n",
            "=== Experiment 11/32: b64_L6_H4_E256_BS8_MI2000_D10_s11 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS8_MI2000_D10_s11.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 11\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2409, val loss 4.2377\n",
            "iter 0: loss 4.2595, time 2401.65ms, mfu -100.00%\n",
            "iter 10: loss 4.1630, time 434.58ms, mfu 0.45%\n",
            "iter 20: loss 3.9602, time 447.10ms, mfu 0.45%\n",
            "iter 30: loss 3.7573, time 432.45ms, mfu 0.45%\n",
            "iter 40: loss 3.6041, time 427.64ms, mfu 0.45%\n",
            "iter 50: loss 3.4337, time 432.78ms, mfu 0.45%\n",
            "iter 60: loss 3.4373, time 427.29ms, mfu 0.45%\n",
            "iter 70: loss 3.3312, time 430.73ms, mfu 0.45%\n",
            "iter 80: loss 3.2026, time 441.56ms, mfu 0.45%\n",
            "iter 90: loss 3.1079, time 429.14ms, mfu 0.45%\n",
            "iter 100: loss 2.9660, time 428.71ms, mfu 0.45%\n",
            "iter 110: loss 3.0429, time 429.49ms, mfu 0.45%\n",
            "iter 120: loss 2.9103, time 432.92ms, mfu 0.45%\n",
            "iter 130: loss 2.8064, time 428.35ms, mfu 0.45%\n",
            "iter 140: loss 2.8048, time 430.73ms, mfu 0.45%\n",
            "iter 150: loss 2.8009, time 429.73ms, mfu 0.45%\n",
            "iter 160: loss 2.7884, time 451.05ms, mfu 0.45%\n",
            "iter 170: loss 2.8185, time 436.40ms, mfu 0.45%\n",
            "iter 180: loss 2.7472, time 436.67ms, mfu 0.45%\n",
            "iter 190: loss 2.7209, time 441.72ms, mfu 0.45%\n",
            "step 200: train loss 2.6772, val loss 2.6887\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 200: loss 2.7164, time 2155.69ms, mfu 0.41%\n",
            "iter 210: loss 2.6671, time 430.07ms, mfu 0.42%\n",
            "iter 220: loss 2.6315, time 437.40ms, mfu 0.42%\n",
            "iter 230: loss 2.6769, time 427.99ms, mfu 0.42%\n",
            "iter 240: loss 2.6589, time 434.94ms, mfu 0.42%\n",
            "iter 250: loss 2.6797, time 433.65ms, mfu 0.43%\n",
            "iter 260: loss 2.5618, time 434.01ms, mfu 0.43%\n",
            "iter 270: loss 2.5633, time 428.74ms, mfu 0.43%\n",
            "iter 280: loss 2.5719, time 425.11ms, mfu 0.43%\n",
            "iter 290: loss 2.5764, time 438.64ms, mfu 0.43%\n",
            "iter 300: loss 2.5648, time 433.68ms, mfu 0.44%\n",
            "iter 310: loss 2.5107, time 434.69ms, mfu 0.44%\n",
            "iter 320: loss 2.4797, time 433.97ms, mfu 0.44%\n",
            "iter 330: loss 2.4736, time 430.33ms, mfu 0.44%\n",
            "iter 340: loss 2.4977, time 426.60ms, mfu 0.44%\n",
            "iter 350: loss 2.4248, time 427.12ms, mfu 0.44%\n",
            "iter 360: loss 2.3528, time 437.91ms, mfu 0.44%\n",
            "iter 370: loss 2.4522, time 428.75ms, mfu 0.44%\n",
            "iter 380: loss 2.3904, time 437.61ms, mfu 0.44%\n",
            "iter 390: loss 2.3467, time 428.65ms, mfu 0.44%\n",
            "step 400: train loss 2.3367, val loss 2.3511\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 400: loss 2.3821, time 2142.98ms, mfu 0.41%\n",
            "iter 410: loss 2.2685, time 428.76ms, mfu 0.41%\n",
            "iter 420: loss 2.3832, time 434.04ms, mfu 0.42%\n",
            "iter 430: loss 2.4519, time 432.85ms, mfu 0.42%\n",
            "iter 440: loss 2.3102, time 421.89ms, mfu 0.42%\n",
            "iter 450: loss 2.2991, time 426.05ms, mfu 0.43%\n",
            "iter 460: loss 2.4112, time 433.68ms, mfu 0.43%\n",
            "iter 470: loss 2.2236, time 431.27ms, mfu 0.43%\n",
            "iter 480: loss 2.3088, time 430.90ms, mfu 0.43%\n",
            "iter 490: loss 2.2470, time 432.59ms, mfu 0.44%\n",
            "iter 500: loss 2.2213, time 441.48ms, mfu 0.44%\n",
            "iter 510: loss 2.2841, time 437.98ms, mfu 0.44%\n",
            "iter 520: loss 2.1976, time 436.70ms, mfu 0.44%\n",
            "iter 530: loss 2.1625, time 426.80ms, mfu 0.44%\n",
            "iter 540: loss 2.2451, time 427.75ms, mfu 0.44%\n",
            "iter 550: loss 2.1920, time 426.30ms, mfu 0.44%\n",
            "iter 560: loss 2.0665, time 429.21ms, mfu 0.44%\n",
            "iter 570: loss 2.1899, time 439.04ms, mfu 0.44%\n",
            "iter 580: loss 2.1952, time 436.06ms, mfu 0.44%\n",
            "iter 590: loss 2.0326, time 433.45ms, mfu 0.44%\n",
            "step 600: train loss 2.0730, val loss 2.1194\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 600: loss 2.1331, time 2183.92ms, mfu 0.41%\n",
            "iter 610: loss 2.1617, time 439.39ms, mfu 0.41%\n",
            "iter 620: loss 2.1219, time 430.89ms, mfu 0.42%\n",
            "iter 630: loss 2.0724, time 426.16ms, mfu 0.42%\n",
            "iter 640: loss 2.1058, time 430.16ms, mfu 0.42%\n",
            "iter 650: loss 2.1034, time 442.80ms, mfu 0.42%\n",
            "iter 660: loss 1.9716, time 430.55ms, mfu 0.43%\n",
            "iter 670: loss 2.0467, time 431.16ms, mfu 0.43%\n",
            "iter 680: loss 1.9927, time 433.26ms, mfu 0.43%\n",
            "iter 690: loss 2.0876, time 425.19ms, mfu 0.43%\n",
            "iter 700: loss 2.0952, time 430.15ms, mfu 0.44%\n",
            "iter 710: loss 2.0316, time 435.70ms, mfu 0.44%\n",
            "iter 720: loss 2.1112, time 425.41ms, mfu 0.44%\n",
            "iter 730: loss 2.0110, time 426.62ms, mfu 0.44%\n",
            "iter 740: loss 1.9398, time 436.60ms, mfu 0.44%\n",
            "iter 750: loss 1.9554, time 426.74ms, mfu 0.44%\n",
            "iter 760: loss 1.9497, time 427.67ms, mfu 0.44%\n",
            "iter 770: loss 1.8691, time 432.46ms, mfu 0.44%\n",
            "iter 780: loss 2.0215, time 429.60ms, mfu 0.45%\n",
            "iter 790: loss 1.9310, time 437.77ms, mfu 0.44%\n",
            "step 800: train loss 1.8393, val loss 1.9467\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 800: loss 1.8239, time 2162.36ms, mfu 0.41%\n",
            "iter 810: loss 1.7910, time 431.32ms, mfu 0.41%\n",
            "iter 820: loss 2.1085, time 434.45ms, mfu 0.42%\n",
            "iter 830: loss 1.8710, time 443.65ms, mfu 0.42%\n",
            "iter 840: loss 1.8763, time 433.39ms, mfu 0.42%\n",
            "iter 850: loss 1.8380, time 431.05ms, mfu 0.42%\n",
            "iter 860: loss 1.8803, time 434.15ms, mfu 0.43%\n",
            "iter 870: loss 1.8894, time 434.92ms, mfu 0.43%\n",
            "iter 880: loss 1.7750, time 424.70ms, mfu 0.43%\n",
            "iter 890: loss 1.8111, time 422.77ms, mfu 0.43%\n",
            "iter 900: loss 1.8193, time 435.46ms, mfu 0.44%\n",
            "iter 910: loss 1.8781, time 436.87ms, mfu 0.44%\n",
            "iter 920: loss 1.8071, time 430.03ms, mfu 0.44%\n",
            "iter 930: loss 1.7922, time 434.91ms, mfu 0.44%\n",
            "iter 940: loss 1.6821, time 425.89ms, mfu 0.44%\n",
            "iter 950: loss 1.5708, time 422.28ms, mfu 0.44%\n",
            "iter 960: loss 1.6820, time 429.46ms, mfu 0.44%\n",
            "iter 970: loss 1.7308, time 426.41ms, mfu 0.45%\n",
            "iter 980: loss 1.7914, time 420.98ms, mfu 0.45%\n",
            "iter 990: loss 1.8862, time 430.79ms, mfu 0.45%\n",
            "step 1000: train loss 1.6640, val loss 1.8361\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1000: loss 1.7858, time 2160.30ms, mfu 0.41%\n",
            "iter 1010: loss 1.7552, time 438.68ms, mfu 0.41%\n",
            "iter 1020: loss 1.7884, time 429.91ms, mfu 0.42%\n",
            "iter 1030: loss 1.8660, time 435.64ms, mfu 0.42%\n",
            "iter 1040: loss 1.6985, time 430.35ms, mfu 0.42%\n",
            "iter 1050: loss 1.7498, time 422.40ms, mfu 0.43%\n",
            "iter 1060: loss 1.7263, time 429.70ms, mfu 0.43%\n",
            "iter 1070: loss 1.9390, time 428.99ms, mfu 0.43%\n",
            "iter 1080: loss 1.6544, time 430.14ms, mfu 0.43%\n",
            "iter 1090: loss 1.5998, time 429.65ms, mfu 0.44%\n",
            "iter 1100: loss 1.7147, time 429.16ms, mfu 0.44%\n",
            "iter 1110: loss 1.6487, time 432.33ms, mfu 0.44%\n",
            "iter 1120: loss 1.6565, time 437.17ms, mfu 0.44%\n",
            "iter 1130: loss 1.8313, time 436.42ms, mfu 0.44%\n",
            "iter 1140: loss 1.5443, time 428.53ms, mfu 0.44%\n",
            "iter 1150: loss 1.7520, time 433.83ms, mfu 0.44%\n",
            "iter 1160: loss 1.5696, time 430.30ms, mfu 0.44%\n",
            "iter 1170: loss 1.6152, time 425.77ms, mfu 0.44%\n",
            "iter 1180: loss 1.6143, time 430.44ms, mfu 0.45%\n",
            "iter 1190: loss 1.6190, time 431.87ms, mfu 0.45%\n",
            "step 1200: train loss 1.5738, val loss 1.7357\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1200: loss 1.8419, time 2192.01ms, mfu 0.41%\n",
            "iter 1210: loss 1.6543, time 428.81ms, mfu 0.41%\n",
            "iter 1220: loss 1.6484, time 424.83ms, mfu 0.42%\n",
            "iter 1230: loss 1.5294, time 439.02ms, mfu 0.42%\n",
            "iter 1240: loss 1.5826, time 427.27ms, mfu 0.42%\n",
            "iter 1250: loss 1.5704, time 423.72ms, mfu 0.43%\n",
            "iter 1260: loss 1.6434, time 454.46ms, mfu 0.43%\n",
            "iter 1270: loss 1.5392, time 428.84ms, mfu 0.43%\n",
            "iter 1280: loss 1.5104, time 428.43ms, mfu 0.43%\n",
            "iter 1290: loss 1.6913, time 429.77ms, mfu 0.43%\n",
            "iter 1300: loss 1.5604, time 427.60ms, mfu 0.44%\n",
            "iter 1310: loss 1.6304, time 432.96ms, mfu 0.44%\n",
            "iter 1320: loss 1.5685, time 425.05ms, mfu 0.44%\n",
            "iter 1330: loss 1.5703, time 430.68ms, mfu 0.44%\n",
            "iter 1340: loss 1.6062, time 435.41ms, mfu 0.44%\n",
            "iter 1350: loss 1.4647, time 440.10ms, mfu 0.44%\n",
            "iter 1360: loss 1.6479, time 432.06ms, mfu 0.44%\n",
            "iter 1370: loss 1.7432, time 430.20ms, mfu 0.44%\n",
            "iter 1380: loss 1.4689, time 427.36ms, mfu 0.44%\n",
            "iter 1390: loss 1.4526, time 428.59ms, mfu 0.45%\n",
            "step 1400: train loss 1.4874, val loss 1.6781\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1400: loss 1.5218, time 2146.92ms, mfu 0.41%\n",
            "iter 1410: loss 1.5222, time 429.37ms, mfu 0.41%\n",
            "iter 1420: loss 1.3946, time 437.71ms, mfu 0.42%\n",
            "iter 1430: loss 1.6251, time 433.01ms, mfu 0.42%\n",
            "iter 1440: loss 1.4285, time 429.35ms, mfu 0.42%\n",
            "iter 1450: loss 1.6174, time 428.90ms, mfu 0.43%\n",
            "iter 1460: loss 1.6413, time 430.85ms, mfu 0.43%\n",
            "iter 1470: loss 1.6312, time 433.62ms, mfu 0.43%\n",
            "iter 1480: loss 1.5093, time 459.30ms, mfu 0.43%\n",
            "iter 1490: loss 1.7291, time 429.11ms, mfu 0.43%\n",
            "iter 1500: loss 1.5678, time 432.20ms, mfu 0.43%\n",
            "iter 1510: loss 1.5894, time 431.05ms, mfu 0.44%\n",
            "iter 1520: loss 1.5006, time 427.86ms, mfu 0.44%\n",
            "iter 1530: loss 1.6985, time 434.92ms, mfu 0.44%\n",
            "iter 1540: loss 1.5201, time 430.19ms, mfu 0.44%\n",
            "iter 1550: loss 1.4768, time 427.52ms, mfu 0.44%\n",
            "iter 1560: loss 1.5627, time 443.26ms, mfu 0.44%\n",
            "iter 1570: loss 1.4412, time 430.76ms, mfu 0.44%\n",
            "iter 1580: loss 1.4898, time 431.48ms, mfu 0.44%\n",
            "iter 1590: loss 1.6266, time 439.14ms, mfu 0.44%\n",
            "step 1600: train loss 1.4318, val loss 1.6506\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1600: loss 1.6353, time 2149.81ms, mfu 0.41%\n",
            "iter 1610: loss 1.4754, time 433.05ms, mfu 0.41%\n",
            "iter 1620: loss 1.4292, time 437.03ms, mfu 0.42%\n",
            "iter 1630: loss 1.5892, time 455.24ms, mfu 0.42%\n",
            "iter 1640: loss 1.5314, time 433.30ms, mfu 0.42%\n",
            "iter 1650: loss 1.5431, time 432.93ms, mfu 0.42%\n",
            "iter 1660: loss 1.5857, time 435.28ms, mfu 0.42%\n",
            "iter 1670: loss 1.5300, time 432.40ms, mfu 0.43%\n",
            "iter 1680: loss 1.4628, time 431.99ms, mfu 0.43%\n",
            "iter 1690: loss 1.3894, time 432.48ms, mfu 0.43%\n",
            "iter 1700: loss 1.5791, time 436.34ms, mfu 0.43%\n",
            "iter 1710: loss 1.5165, time 432.26ms, mfu 0.43%\n",
            "iter 1720: loss 1.5082, time 431.20ms, mfu 0.44%\n",
            "iter 1730: loss 1.5610, time 430.35ms, mfu 0.44%\n",
            "iter 1740: loss 1.4875, time 435.71ms, mfu 0.44%\n",
            "iter 1750: loss 1.5531, time 431.95ms, mfu 0.44%\n",
            "iter 1760: loss 1.4744, time 437.48ms, mfu 0.44%\n",
            "iter 1770: loss 1.5217, time 434.33ms, mfu 0.44%\n",
            "iter 1780: loss 1.5357, time 433.64ms, mfu 0.44%\n",
            "iter 1790: loss 1.4350, time 433.55ms, mfu 0.44%\n",
            "step 1800: train loss 1.3933, val loss 1.5989\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "iter 1800: loss 1.6240, time 2168.69ms, mfu 0.41%\n",
            "iter 1810: loss 1.4931, time 436.62ms, mfu 0.41%\n",
            "iter 1820: loss 1.5250, time 432.06ms, mfu 0.41%\n",
            "iter 1830: loss 1.4192, time 451.21ms, mfu 0.42%\n",
            "iter 1840: loss 1.4990, time 431.36ms, mfu 0.42%\n",
            "iter 1850: loss 1.5884, time 431.95ms, mfu 0.42%\n",
            "iter 1860: loss 1.4234, time 445.12ms, mfu 0.42%\n",
            "iter 1870: loss 1.4886, time 427.06ms, mfu 0.43%\n",
            "iter 1880: loss 1.6489, time 432.74ms, mfu 0.43%\n",
            "iter 1890: loss 1.4268, time 440.07ms, mfu 0.43%\n",
            "iter 1900: loss 1.3301, time 435.33ms, mfu 0.43%\n",
            "iter 1910: loss 1.4211, time 431.02ms, mfu 0.43%\n",
            "iter 1920: loss 1.3384, time 426.81ms, mfu 0.44%\n",
            "iter 1930: loss 1.3940, time 422.64ms, mfu 0.44%\n",
            "iter 1940: loss 1.5188, time 432.01ms, mfu 0.44%\n",
            "iter 1950: loss 1.5373, time 429.56ms, mfu 0.44%\n",
            "iter 1960: loss 1.4620, time 427.32ms, mfu 0.44%\n",
            "iter 1970: loss 1.3854, time 438.56ms, mfu 0.44%\n",
            "iter 1980: loss 1.4028, time 428.15ms, mfu 0.44%\n",
            "iter 1990: loss 1.4614, time 427.89ms, mfu 0.44%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 12/32: b64_L6_H4_E256_BS8_MI2000_D20_s12 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS8_MI2000_D20_s12.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 12\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2409, val loss 4.2377\n",
            "iter 0: loss 4.2493, time 2423.51ms, mfu -100.00%\n",
            "iter 10: loss 4.1666, time 432.67ms, mfu 0.45%\n",
            "iter 20: loss 4.0082, time 432.35ms, mfu 0.45%\n",
            "iter 30: loss 3.8069, time 430.98ms, mfu 0.45%\n",
            "iter 40: loss 3.6416, time 431.07ms, mfu 0.45%\n",
            "iter 50: loss 3.4722, time 428.97ms, mfu 0.45%\n",
            "iter 60: loss 3.4787, time 431.92ms, mfu 0.45%\n",
            "iter 70: loss 3.4058, time 444.67ms, mfu 0.45%\n",
            "iter 80: loss 3.2824, time 430.68ms, mfu 0.45%\n",
            "iter 90: loss 3.1852, time 430.35ms, mfu 0.45%\n",
            "iter 100: loss 3.0340, time 429.34ms, mfu 0.45%\n",
            "iter 110: loss 3.1268, time 433.53ms, mfu 0.45%\n",
            "iter 120: loss 2.9696, time 428.91ms, mfu 0.45%\n",
            "iter 130: loss 2.8613, time 441.49ms, mfu 0.45%\n",
            "iter 140: loss 2.8445, time 431.02ms, mfu 0.45%\n",
            "iter 150: loss 2.8448, time 427.88ms, mfu 0.45%\n",
            "iter 160: loss 2.8221, time 433.30ms, mfu 0.45%\n",
            "iter 170: loss 2.8672, time 432.69ms, mfu 0.45%\n",
            "iter 180: loss 2.7860, time 433.12ms, mfu 0.45%\n",
            "iter 190: loss 2.7681, time 430.15ms, mfu 0.45%\n",
            "step 200: train loss 2.6976, val loss 2.7058\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 200: loss 2.7452, time 2174.49ms, mfu 0.41%\n",
            "iter 210: loss 2.7078, time 433.79ms, mfu 0.42%\n",
            "iter 220: loss 2.6789, time 435.18ms, mfu 0.42%\n",
            "iter 230: loss 2.7250, time 445.24ms, mfu 0.42%\n",
            "iter 240: loss 2.7057, time 429.50ms, mfu 0.42%\n",
            "iter 250: loss 2.7251, time 431.96ms, mfu 0.43%\n",
            "iter 260: loss 2.6046, time 424.53ms, mfu 0.43%\n",
            "iter 270: loss 2.5934, time 438.79ms, mfu 0.43%\n",
            "iter 280: loss 2.6069, time 430.57ms, mfu 0.43%\n",
            "iter 290: loss 2.6148, time 435.92ms, mfu 0.43%\n",
            "iter 300: loss 2.5983, time 436.80ms, mfu 0.44%\n",
            "iter 310: loss 2.5490, time 429.26ms, mfu 0.44%\n",
            "iter 320: loss 2.5038, time 428.19ms, mfu 0.44%\n",
            "iter 330: loss 2.5029, time 429.86ms, mfu 0.44%\n",
            "iter 340: loss 2.5333, time 431.12ms, mfu 0.44%\n",
            "iter 350: loss 2.4567, time 443.23ms, mfu 0.44%\n",
            "iter 360: loss 2.4319, time 434.45ms, mfu 0.44%\n",
            "iter 370: loss 2.4945, time 438.90ms, mfu 0.44%\n",
            "iter 380: loss 2.4208, time 433.70ms, mfu 0.44%\n",
            "iter 390: loss 2.4022, time 440.32ms, mfu 0.44%\n",
            "step 400: train loss 2.3832, val loss 2.3951\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 400: loss 2.4471, time 2282.76ms, mfu 0.41%\n",
            "iter 410: loss 2.3620, time 438.77ms, mfu 0.41%\n",
            "iter 420: loss 2.4347, time 437.63ms, mfu 0.41%\n",
            "iter 430: loss 2.5352, time 439.78ms, mfu 0.42%\n",
            "iter 440: loss 2.3403, time 441.18ms, mfu 0.42%\n",
            "iter 450: loss 2.3992, time 441.59ms, mfu 0.42%\n",
            "iter 460: loss 2.4738, time 428.63ms, mfu 0.42%\n",
            "iter 470: loss 2.3039, time 434.09ms, mfu 0.43%\n",
            "iter 480: loss 2.3687, time 441.38ms, mfu 0.43%\n",
            "iter 490: loss 2.3394, time 429.33ms, mfu 0.43%\n",
            "iter 500: loss 2.3196, time 429.12ms, mfu 0.43%\n",
            "iter 510: loss 2.3863, time 437.17ms, mfu 0.43%\n",
            "iter 520: loss 2.2840, time 432.97ms, mfu 0.44%\n",
            "iter 530: loss 2.2470, time 428.74ms, mfu 0.44%\n",
            "iter 540: loss 2.3272, time 436.72ms, mfu 0.44%\n",
            "iter 550: loss 2.3321, time 435.69ms, mfu 0.44%\n",
            "iter 560: loss 2.1883, time 436.24ms, mfu 0.44%\n",
            "iter 570: loss 2.3106, time 433.90ms, mfu 0.44%\n",
            "iter 580: loss 2.3017, time 434.73ms, mfu 0.44%\n",
            "iter 590: loss 2.1640, time 441.59ms, mfu 0.44%\n",
            "step 600: train loss 2.1575, val loss 2.1835\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 600: loss 2.2270, time 2176.72ms, mfu 0.41%\n",
            "iter 610: loss 2.2683, time 430.70ms, mfu 0.41%\n",
            "iter 620: loss 2.2055, time 430.32ms, mfu 0.41%\n",
            "iter 630: loss 2.1807, time 444.16ms, mfu 0.42%\n",
            "iter 640: loss 2.2019, time 425.63ms, mfu 0.42%\n",
            "iter 650: loss 2.1847, time 434.08ms, mfu 0.42%\n",
            "iter 660: loss 2.0713, time 431.52ms, mfu 0.43%\n",
            "iter 670: loss 2.1911, time 438.99ms, mfu 0.43%\n",
            "iter 680: loss 2.0952, time 427.51ms, mfu 0.43%\n",
            "iter 690: loss 2.2061, time 426.93ms, mfu 0.43%\n",
            "iter 700: loss 2.1831, time 438.63ms, mfu 0.43%\n",
            "iter 710: loss 2.1405, time 428.91ms, mfu 0.44%\n",
            "iter 720: loss 2.1966, time 426.68ms, mfu 0.44%\n",
            "iter 730: loss 2.1076, time 444.18ms, mfu 0.44%\n",
            "iter 740: loss 2.1083, time 436.45ms, mfu 0.44%\n",
            "iter 750: loss 2.0437, time 424.02ms, mfu 0.44%\n",
            "iter 760: loss 2.0705, time 432.43ms, mfu 0.44%\n",
            "iter 770: loss 1.9924, time 431.49ms, mfu 0.44%\n",
            "iter 780: loss 2.1663, time 431.53ms, mfu 0.44%\n",
            "iter 790: loss 2.0364, time 425.96ms, mfu 0.44%\n",
            "step 800: train loss 1.9565, val loss 2.0263\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 800: loss 1.9401, time 2177.15ms, mfu 0.41%\n",
            "iter 810: loss 1.9551, time 453.90ms, mfu 0.41%\n",
            "iter 820: loss 2.2006, time 436.03ms, mfu 0.41%\n",
            "iter 830: loss 2.0265, time 430.61ms, mfu 0.42%\n",
            "iter 840: loss 2.0188, time 429.94ms, mfu 0.42%\n",
            "iter 850: loss 1.9605, time 429.45ms, mfu 0.42%\n",
            "iter 860: loss 2.0203, time 429.24ms, mfu 0.43%\n",
            "iter 870: loss 2.0258, time 431.84ms, mfu 0.43%\n",
            "iter 880: loss 1.9126, time 430.58ms, mfu 0.43%\n",
            "iter 890: loss 1.9347, time 441.58ms, mfu 0.43%\n",
            "iter 900: loss 1.9334, time 432.49ms, mfu 0.43%\n",
            "iter 910: loss 1.9797, time 430.01ms, mfu 0.44%\n",
            "iter 920: loss 1.9433, time 434.63ms, mfu 0.44%\n",
            "iter 930: loss 1.9222, time 428.77ms, mfu 0.44%\n",
            "iter 940: loss 1.8148, time 436.82ms, mfu 0.44%\n",
            "iter 950: loss 1.7329, time 452.67ms, mfu 0.44%\n",
            "iter 960: loss 1.7648, time 433.17ms, mfu 0.44%\n",
            "iter 970: loss 1.8707, time 431.75ms, mfu 0.44%\n",
            "iter 980: loss 1.9337, time 431.83ms, mfu 0.44%\n",
            "iter 990: loss 2.0236, time 433.19ms, mfu 0.44%\n",
            "step 1000: train loss 1.7787, val loss 1.9224\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1000: loss 1.9091, time 2209.29ms, mfu 0.41%\n",
            "iter 1010: loss 1.8969, time 439.53ms, mfu 0.41%\n",
            "iter 1020: loss 1.8844, time 437.45ms, mfu 0.41%\n",
            "iter 1030: loss 1.9965, time 434.02ms, mfu 0.42%\n",
            "iter 1040: loss 1.8439, time 431.64ms, mfu 0.42%\n",
            "iter 1050: loss 1.9099, time 426.07ms, mfu 0.42%\n",
            "iter 1060: loss 1.8824, time 429.06ms, mfu 0.43%\n",
            "iter 1070: loss 2.0558, time 429.51ms, mfu 0.43%\n",
            "iter 1080: loss 1.7925, time 432.99ms, mfu 0.43%\n",
            "iter 1090: loss 1.7199, time 432.23ms, mfu 0.43%\n",
            "iter 1100: loss 1.8719, time 433.38ms, mfu 0.43%\n",
            "iter 1110: loss 1.7915, time 434.16ms, mfu 0.44%\n",
            "iter 1120: loss 1.8193, time 432.34ms, mfu 0.44%\n",
            "iter 1130: loss 1.9649, time 430.57ms, mfu 0.44%\n",
            "iter 1140: loss 1.6849, time 439.38ms, mfu 0.44%\n",
            "iter 1150: loss 1.8590, time 424.00ms, mfu 0.44%\n",
            "iter 1160: loss 1.6660, time 431.73ms, mfu 0.44%\n",
            "iter 1170: loss 1.7275, time 428.99ms, mfu 0.44%\n",
            "iter 1180: loss 1.7129, time 431.53ms, mfu 0.44%\n",
            "iter 1190: loss 1.7712, time 434.26ms, mfu 0.44%\n",
            "step 1200: train loss 1.6680, val loss 1.8167\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1200: loss 1.9468, time 2181.92ms, mfu 0.41%\n",
            "iter 1210: loss 1.7285, time 434.16ms, mfu 0.41%\n",
            "iter 1220: loss 1.7543, time 460.71ms, mfu 0.41%\n",
            "iter 1230: loss 1.7241, time 434.39ms, mfu 0.42%\n",
            "iter 1240: loss 1.7173, time 433.96ms, mfu 0.42%\n",
            "iter 1250: loss 1.6608, time 439.21ms, mfu 0.42%\n",
            "iter 1260: loss 1.7294, time 436.19ms, mfu 0.42%\n",
            "iter 1270: loss 1.6556, time 434.82ms, mfu 0.43%\n",
            "iter 1280: loss 1.6565, time 432.47ms, mfu 0.43%\n",
            "iter 1290: loss 1.7987, time 432.68ms, mfu 0.43%\n",
            "iter 1300: loss 1.6458, time 441.15ms, mfu 0.43%\n",
            "iter 1310: loss 1.7468, time 437.75ms, mfu 0.43%\n",
            "iter 1320: loss 1.6639, time 434.86ms, mfu 0.43%\n",
            "iter 1330: loss 1.6392, time 441.27ms, mfu 0.44%\n",
            "iter 1340: loss 1.7820, time 433.02ms, mfu 0.44%\n",
            "iter 1350: loss 1.5872, time 431.15ms, mfu 0.44%\n",
            "iter 1360: loss 1.7386, time 442.06ms, mfu 0.44%\n",
            "iter 1370: loss 1.8658, time 438.62ms, mfu 0.44%\n",
            "iter 1380: loss 1.6185, time 439.21ms, mfu 0.44%\n",
            "iter 1390: loss 1.5742, time 433.81ms, mfu 0.44%\n",
            "step 1400: train loss 1.5747, val loss 1.7575\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1400: loss 1.6377, time 2205.62ms, mfu 0.40%\n",
            "iter 1410: loss 1.5964, time 442.94ms, mfu 0.41%\n",
            "iter 1420: loss 1.4844, time 445.98ms, mfu 0.41%\n",
            "iter 1430: loss 1.7451, time 437.43ms, mfu 0.41%\n",
            "iter 1440: loss 1.5262, time 437.12ms, mfu 0.42%\n",
            "iter 1450: loss 1.7546, time 433.37ms, mfu 0.42%\n",
            "iter 1460: loss 1.7608, time 431.65ms, mfu 0.42%\n",
            "iter 1470: loss 1.7461, time 422.77ms, mfu 0.43%\n",
            "iter 1480: loss 1.5779, time 428.78ms, mfu 0.43%\n",
            "iter 1490: loss 1.8590, time 433.89ms, mfu 0.43%\n",
            "iter 1500: loss 1.7007, time 426.17ms, mfu 0.43%\n",
            "iter 1510: loss 1.7177, time 428.66ms, mfu 0.44%\n",
            "iter 1520: loss 1.5833, time 432.15ms, mfu 0.44%\n",
            "iter 1530: loss 1.8266, time 436.27ms, mfu 0.44%\n",
            "iter 1540: loss 1.6373, time 431.15ms, mfu 0.44%\n",
            "iter 1550: loss 1.6116, time 431.57ms, mfu 0.44%\n",
            "iter 1560: loss 1.6557, time 426.19ms, mfu 0.44%\n",
            "iter 1570: loss 1.5427, time 435.35ms, mfu 0.44%\n",
            "iter 1580: loss 1.6072, time 429.28ms, mfu 0.44%\n",
            "iter 1590: loss 1.7636, time 432.04ms, mfu 0.44%\n",
            "step 1600: train loss 1.5089, val loss 1.7233\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1600: loss 1.7064, time 2220.79ms, mfu 0.41%\n",
            "iter 1610: loss 1.6055, time 426.38ms, mfu 0.41%\n",
            "iter 1620: loss 1.5368, time 425.01ms, mfu 0.42%\n",
            "iter 1630: loss 1.7079, time 449.27ms, mfu 0.42%\n",
            "iter 1640: loss 1.6076, time 434.50ms, mfu 0.42%\n",
            "iter 1650: loss 1.6310, time 436.32ms, mfu 0.42%\n",
            "iter 1660: loss 1.6866, time 430.97ms, mfu 0.43%\n",
            "iter 1670: loss 1.5965, time 428.39ms, mfu 0.43%\n",
            "iter 1680: loss 1.5281, time 430.83ms, mfu 0.43%\n",
            "iter 1690: loss 1.4932, time 429.45ms, mfu 0.43%\n",
            "iter 1700: loss 1.6655, time 431.95ms, mfu 0.44%\n",
            "iter 1710: loss 1.5963, time 434.21ms, mfu 0.44%\n",
            "iter 1720: loss 1.6158, time 428.83ms, mfu 0.44%\n",
            "iter 1730: loss 1.6666, time 427.08ms, mfu 0.44%\n",
            "iter 1740: loss 1.5839, time 430.42ms, mfu 0.44%\n",
            "iter 1750: loss 1.7042, time 428.93ms, mfu 0.44%\n",
            "iter 1760: loss 1.5589, time 429.16ms, mfu 0.44%\n",
            "iter 1770: loss 1.6159, time 441.17ms, mfu 0.44%\n",
            "iter 1780: loss 1.6537, time 435.79ms, mfu 0.44%\n",
            "iter 1790: loss 1.5605, time 427.25ms, mfu 0.44%\n",
            "step 1800: train loss 1.4631, val loss 1.6607\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "iter 1800: loss 1.7330, time 2181.68ms, mfu 0.41%\n",
            "iter 1810: loss 1.5825, time 428.43ms, mfu 0.41%\n",
            "iter 1820: loss 1.6552, time 427.12ms, mfu 0.42%\n",
            "iter 1830: loss 1.4612, time 450.00ms, mfu 0.42%\n",
            "iter 1840: loss 1.6184, time 430.30ms, mfu 0.42%\n",
            "iter 1850: loss 1.6960, time 429.51ms, mfu 0.43%\n",
            "iter 1860: loss 1.5612, time 433.64ms, mfu 0.43%\n",
            "iter 1870: loss 1.5475, time 432.40ms, mfu 0.43%\n",
            "iter 1880: loss 1.7428, time 424.06ms, mfu 0.43%\n",
            "iter 1890: loss 1.5479, time 429.97ms, mfu 0.43%\n",
            "iter 1900: loss 1.4110, time 429.20ms, mfu 0.44%\n",
            "iter 1910: loss 1.4790, time 433.06ms, mfu 0.44%\n",
            "iter 1920: loss 1.4142, time 429.31ms, mfu 0.44%\n",
            "iter 1930: loss 1.4611, time 431.10ms, mfu 0.44%\n",
            "iter 1940: loss 1.6551, time 427.07ms, mfu 0.44%\n",
            "iter 1950: loss 1.6235, time 425.55ms, mfu 0.44%\n",
            "iter 1960: loss 1.5247, time 439.72ms, mfu 0.44%\n",
            "iter 1970: loss 1.4738, time 429.14ms, mfu 0.44%\n",
            "iter 1980: loss 1.4903, time 427.91ms, mfu 0.45%\n",
            "iter 1990: loss 1.5596, time 436.51ms, mfu 0.45%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 13/32: b64_L6_H4_E256_BS16_MI1000_D10_s13 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS16_MI1000_D10_s13.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 13\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2405, val loss 4.2356\n",
            "iter 0: loss 4.2344, time 2556.20ms, mfu -100.00%\n",
            "iter 10: loss 4.1896, time 450.97ms, mfu 0.86%\n",
            "iter 20: loss 3.9887, time 453.72ms, mfu 0.86%\n",
            "iter 30: loss 3.7851, time 459.96ms, mfu 0.86%\n",
            "iter 40: loss 3.6314, time 448.82ms, mfu 0.86%\n",
            "iter 50: loss 3.5079, time 452.75ms, mfu 0.86%\n",
            "iter 60: loss 3.4069, time 455.67ms, mfu 0.86%\n",
            "iter 70: loss 3.2966, time 451.12ms, mfu 0.86%\n",
            "iter 80: loss 3.1370, time 453.28ms, mfu 0.86%\n",
            "iter 90: loss 3.1097, time 457.62ms, mfu 0.86%\n",
            "iter 100: loss 3.0409, time 452.81ms, mfu 0.86%\n",
            "iter 110: loss 2.9823, time 454.21ms, mfu 0.86%\n",
            "iter 120: loss 2.8784, time 453.03ms, mfu 0.86%\n",
            "iter 130: loss 2.8985, time 448.67ms, mfu 0.86%\n",
            "iter 140: loss 2.8320, time 451.71ms, mfu 0.86%\n",
            "iter 150: loss 2.8463, time 453.79ms, mfu 0.86%\n",
            "iter 160: loss 2.8096, time 447.11ms, mfu 0.86%\n",
            "iter 170: loss 2.7505, time 446.65ms, mfu 0.86%\n",
            "iter 180: loss 2.7500, time 444.18ms, mfu 0.86%\n",
            "iter 190: loss 2.6250, time 453.53ms, mfu 0.86%\n",
            "step 200: train loss 2.6585, val loss 2.6734\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 200: loss 2.6494, time 2305.41ms, mfu 0.79%\n",
            "iter 210: loss 2.6714, time 454.82ms, mfu 0.80%\n",
            "iter 220: loss 2.5859, time 446.21ms, mfu 0.81%\n",
            "iter 230: loss 2.6295, time 445.35ms, mfu 0.81%\n",
            "iter 240: loss 2.6199, time 461.13ms, mfu 0.82%\n",
            "iter 250: loss 2.6295, time 447.79ms, mfu 0.82%\n",
            "iter 260: loss 2.5175, time 450.42ms, mfu 0.83%\n",
            "iter 270: loss 2.5698, time 442.95ms, mfu 0.83%\n",
            "iter 280: loss 2.5654, time 447.57ms, mfu 0.83%\n",
            "iter 290: loss 2.4971, time 445.39ms, mfu 0.84%\n",
            "iter 300: loss 2.4645, time 450.13ms, mfu 0.84%\n",
            "iter 310: loss 2.4341, time 450.85ms, mfu 0.84%\n",
            "iter 320: loss 2.4453, time 453.30ms, mfu 0.84%\n",
            "iter 330: loss 2.4231, time 448.69ms, mfu 0.85%\n",
            "iter 340: loss 2.3910, time 451.22ms, mfu 0.85%\n",
            "iter 350: loss 2.4302, time 455.67ms, mfu 0.85%\n",
            "iter 360: loss 2.3770, time 454.01ms, mfu 0.85%\n",
            "iter 370: loss 2.3648, time 452.41ms, mfu 0.85%\n",
            "iter 380: loss 2.3476, time 454.29ms, mfu 0.85%\n",
            "iter 390: loss 2.3236, time 454.14ms, mfu 0.85%\n",
            "step 400: train loss 2.2870, val loss 2.3075\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 400: loss 2.3145, time 2368.14ms, mfu 0.78%\n",
            "iter 410: loss 2.3348, time 460.91ms, mfu 0.79%\n",
            "iter 420: loss 2.3385, time 463.54ms, mfu 0.79%\n",
            "iter 430: loss 2.2969, time 450.52ms, mfu 0.80%\n",
            "iter 440: loss 2.3428, time 455.02ms, mfu 0.81%\n",
            "iter 450: loss 2.3246, time 457.81ms, mfu 0.81%\n",
            "iter 460: loss 2.3020, time 460.19ms, mfu 0.81%\n",
            "iter 470: loss 2.2695, time 443.43ms, mfu 0.82%\n",
            "iter 480: loss 2.3003, time 455.34ms, mfu 0.82%\n",
            "iter 490: loss 2.1978, time 453.32ms, mfu 0.83%\n",
            "iter 500: loss 2.1519, time 451.95ms, mfu 0.83%\n",
            "iter 510: loss 2.1231, time 448.24ms, mfu 0.83%\n",
            "iter 520: loss 2.0761, time 448.41ms, mfu 0.84%\n",
            "iter 530: loss 2.1156, time 469.40ms, mfu 0.84%\n",
            "iter 540: loss 2.1623, time 449.86ms, mfu 0.84%\n",
            "iter 550: loss 2.1086, time 450.53ms, mfu 0.84%\n",
            "iter 560: loss 2.0621, time 451.36ms, mfu 0.84%\n",
            "iter 570: loss 2.1577, time 457.66ms, mfu 0.84%\n",
            "iter 580: loss 2.0758, time 455.58ms, mfu 0.84%\n",
            "iter 590: loss 2.1101, time 446.86ms, mfu 0.85%\n",
            "step 600: train loss 1.9872, val loss 2.0526\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 600: loss 2.0058, time 2326.73ms, mfu 0.78%\n",
            "iter 610: loss 2.0435, time 457.84ms, mfu 0.79%\n",
            "iter 620: loss 2.0262, time 449.75ms, mfu 0.79%\n",
            "iter 630: loss 2.0139, time 476.52ms, mfu 0.80%\n",
            "iter 640: loss 1.9721, time 446.46ms, mfu 0.80%\n",
            "iter 650: loss 1.9330, time 445.84ms, mfu 0.81%\n",
            "iter 660: loss 2.0682, time 468.68ms, mfu 0.81%\n",
            "iter 670: loss 1.9269, time 453.79ms, mfu 0.82%\n",
            "iter 680: loss 1.9818, time 458.45ms, mfu 0.82%\n",
            "iter 690: loss 1.9591, time 454.17ms, mfu 0.82%\n",
            "iter 700: loss 1.8255, time 453.23ms, mfu 0.83%\n",
            "iter 710: loss 1.9742, time 468.41ms, mfu 0.83%\n",
            "iter 720: loss 1.8787, time 452.98ms, mfu 0.83%\n",
            "iter 730: loss 1.9307, time 450.27ms, mfu 0.83%\n",
            "iter 740: loss 1.8993, time 453.92ms, mfu 0.84%\n",
            "iter 750: loss 1.8058, time 452.02ms, mfu 0.84%\n",
            "iter 760: loss 1.8646, time 450.28ms, mfu 0.84%\n",
            "iter 770: loss 1.9279, time 450.72ms, mfu 0.84%\n",
            "iter 780: loss 1.8142, time 450.16ms, mfu 0.85%\n",
            "iter 790: loss 1.9127, time 452.76ms, mfu 0.85%\n",
            "step 800: train loss 1.7400, val loss 1.8802\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 800: loss 1.7595, time 2307.60ms, mfu 0.78%\n",
            "iter 810: loss 1.8504, time 452.57ms, mfu 0.79%\n",
            "iter 820: loss 1.7469, time 444.82ms, mfu 0.80%\n",
            "iter 830: loss 1.7819, time 455.83ms, mfu 0.80%\n",
            "iter 840: loss 1.8677, time 456.04ms, mfu 0.81%\n",
            "iter 850: loss 1.7761, time 456.16ms, mfu 0.81%\n",
            "iter 860: loss 1.7102, time 443.54ms, mfu 0.82%\n",
            "iter 870: loss 1.7744, time 452.79ms, mfu 0.82%\n",
            "iter 880: loss 1.6904, time 441.37ms, mfu 0.83%\n",
            "iter 890: loss 1.7619, time 445.76ms, mfu 0.83%\n",
            "iter 900: loss 1.7204, time 446.10ms, mfu 0.84%\n",
            "iter 910: loss 1.7540, time 446.55ms, mfu 0.84%\n",
            "iter 920: loss 1.8199, time 453.11ms, mfu 0.84%\n",
            "iter 930: loss 1.6695, time 448.88ms, mfu 0.84%\n",
            "iter 940: loss 1.7926, time 448.54ms, mfu 0.85%\n",
            "iter 950: loss 1.6287, time 460.36ms, mfu 0.85%\n",
            "iter 960: loss 1.6748, time 446.11ms, mfu 0.85%\n",
            "iter 970: loss 1.7285, time 447.74ms, mfu 0.85%\n",
            "iter 980: loss 1.6695, time 451.36ms, mfu 0.85%\n",
            "iter 990: loss 1.7066, time 447.95ms, mfu 0.85%\n",
            "step 1000: train loss 1.5928, val loss 1.7779\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "iter 1000: loss 1.6831, time 2366.29ms, mfu 0.78%\n",
            "\n",
            "=== Experiment 14/32: b64_L6_H4_E256_BS16_MI1000_D20_s14 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS16_MI1000_D20_s14.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 14\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2405, val loss 4.2356\n",
            "iter 0: loss 4.2322, time 2551.42ms, mfu -100.00%\n",
            "iter 10: loss 4.2043, time 454.19ms, mfu 0.86%\n",
            "iter 20: loss 4.0240, time 460.35ms, mfu 0.85%\n",
            "iter 30: loss 3.8298, time 445.54ms, mfu 0.86%\n",
            "iter 40: loss 3.6725, time 460.34ms, mfu 0.86%\n",
            "iter 50: loss 3.5446, time 443.30ms, mfu 0.86%\n",
            "iter 60: loss 3.4502, time 446.17ms, mfu 0.86%\n",
            "iter 70: loss 3.3634, time 453.27ms, mfu 0.86%\n",
            "iter 80: loss 3.2168, time 442.71ms, mfu 0.86%\n",
            "iter 90: loss 3.1820, time 458.30ms, mfu 0.86%\n",
            "iter 100: loss 3.1133, time 455.27ms, mfu 0.86%\n",
            "iter 110: loss 3.0587, time 444.90ms, mfu 0.86%\n",
            "iter 120: loss 2.9342, time 460.84ms, mfu 0.86%\n",
            "iter 130: loss 2.9539, time 446.14ms, mfu 0.86%\n",
            "iter 140: loss 2.8773, time 445.99ms, mfu 0.86%\n",
            "iter 150: loss 2.8981, time 452.71ms, mfu 0.86%\n",
            "iter 160: loss 2.8657, time 451.22ms, mfu 0.86%\n",
            "iter 170: loss 2.7892, time 454.84ms, mfu 0.86%\n",
            "iter 180: loss 2.7909, time 451.39ms, mfu 0.86%\n",
            "iter 190: loss 2.6731, time 450.37ms, mfu 0.86%\n",
            "step 200: train loss 2.6836, val loss 2.6941\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 200: loss 2.7033, time 2351.82ms, mfu 0.79%\n",
            "iter 210: loss 2.7138, time 445.64ms, mfu 0.80%\n",
            "iter 220: loss 2.6241, time 458.28ms, mfu 0.80%\n",
            "iter 230: loss 2.6723, time 455.69ms, mfu 0.81%\n",
            "iter 240: loss 2.6645, time 449.87ms, mfu 0.81%\n",
            "iter 250: loss 2.6657, time 451.52ms, mfu 0.82%\n",
            "iter 260: loss 2.5749, time 450.37ms, mfu 0.82%\n",
            "iter 270: loss 2.6122, time 450.81ms, mfu 0.83%\n",
            "iter 280: loss 2.6056, time 452.24ms, mfu 0.83%\n",
            "iter 290: loss 2.5462, time 448.62ms, mfu 0.83%\n",
            "iter 300: loss 2.5158, time 455.18ms, mfu 0.84%\n",
            "iter 310: loss 2.4907, time 455.05ms, mfu 0.84%\n",
            "iter 320: loss 2.5078, time 453.08ms, mfu 0.84%\n",
            "iter 330: loss 2.4889, time 461.64ms, mfu 0.84%\n",
            "iter 340: loss 2.4455, time 450.68ms, mfu 0.84%\n",
            "iter 350: loss 2.4966, time 461.09ms, mfu 0.84%\n",
            "iter 360: loss 2.4227, time 452.27ms, mfu 0.84%\n",
            "iter 370: loss 2.4013, time 449.15ms, mfu 0.85%\n",
            "iter 380: loss 2.4196, time 448.15ms, mfu 0.85%\n",
            "iter 390: loss 2.3964, time 448.48ms, mfu 0.85%\n",
            "step 400: train loss 2.3468, val loss 2.3586\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 400: loss 2.3773, time 2358.18ms, mfu 0.78%\n",
            "iter 410: loss 2.4007, time 461.97ms, mfu 0.79%\n",
            "iter 420: loss 2.4134, time 443.35ms, mfu 0.80%\n",
            "iter 430: loss 2.3678, time 473.64ms, mfu 0.80%\n",
            "iter 440: loss 2.4020, time 451.90ms, mfu 0.81%\n",
            "iter 450: loss 2.4052, time 449.27ms, mfu 0.81%\n",
            "iter 460: loss 2.3418, time 457.41ms, mfu 0.82%\n",
            "iter 470: loss 2.3560, time 451.23ms, mfu 0.82%\n",
            "iter 480: loss 2.3740, time 452.98ms, mfu 0.82%\n",
            "iter 490: loss 2.2766, time 450.02ms, mfu 0.83%\n",
            "iter 500: loss 2.2562, time 446.02ms, mfu 0.83%\n",
            "iter 510: loss 2.2100, time 448.53ms, mfu 0.84%\n",
            "iter 520: loss 2.1640, time 441.99ms, mfu 0.84%\n",
            "iter 530: loss 2.2061, time 443.74ms, mfu 0.84%\n",
            "iter 540: loss 2.2613, time 446.37ms, mfu 0.85%\n",
            "iter 550: loss 2.1988, time 446.86ms, mfu 0.85%\n",
            "iter 560: loss 2.1982, time 446.46ms, mfu 0.85%\n",
            "iter 570: loss 2.2791, time 447.34ms, mfu 0.85%\n",
            "iter 580: loss 2.1930, time 446.83ms, mfu 0.85%\n",
            "iter 590: loss 2.2188, time 449.83ms, mfu 0.86%\n",
            "step 600: train loss 2.0801, val loss 2.1246\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 600: loss 2.1194, time 2328.23ms, mfu 0.79%\n",
            "iter 610: loss 2.1684, time 452.13ms, mfu 0.79%\n",
            "iter 620: loss 2.1211, time 449.41ms, mfu 0.80%\n",
            "iter 630: loss 2.1007, time 453.07ms, mfu 0.81%\n",
            "iter 640: loss 2.1203, time 450.96ms, mfu 0.81%\n",
            "iter 650: loss 2.0322, time 450.21ms, mfu 0.82%\n",
            "iter 660: loss 2.2006, time 448.58ms, mfu 0.82%\n",
            "iter 670: loss 2.0357, time 451.19ms, mfu 0.83%\n",
            "iter 680: loss 2.1092, time 442.66ms, mfu 0.83%\n",
            "iter 690: loss 2.0661, time 447.47ms, mfu 0.84%\n",
            "iter 700: loss 1.9742, time 441.65ms, mfu 0.84%\n",
            "iter 710: loss 2.1251, time 447.94ms, mfu 0.84%\n",
            "iter 720: loss 2.0190, time 450.04ms, mfu 0.84%\n",
            "iter 730: loss 2.0905, time 447.10ms, mfu 0.85%\n",
            "iter 740: loss 2.0072, time 446.48ms, mfu 0.85%\n",
            "iter 750: loss 1.9201, time 457.24ms, mfu 0.85%\n",
            "iter 760: loss 2.0124, time 439.87ms, mfu 0.85%\n",
            "iter 770: loss 2.0458, time 444.72ms, mfu 0.86%\n",
            "iter 780: loss 1.9605, time 453.76ms, mfu 0.86%\n",
            "iter 790: loss 2.0513, time 452.94ms, mfu 0.86%\n",
            "step 800: train loss 1.8540, val loss 1.9607\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 800: loss 1.8873, time 2398.01ms, mfu 0.79%\n",
            "iter 810: loss 2.0005, time 451.85ms, mfu 0.79%\n",
            "iter 820: loss 1.9146, time 452.37ms, mfu 0.80%\n",
            "iter 830: loss 1.9030, time 448.31ms, mfu 0.81%\n",
            "iter 840: loss 2.0144, time 448.27ms, mfu 0.81%\n",
            "iter 850: loss 1.9254, time 437.67ms, mfu 0.82%\n",
            "iter 860: loss 1.8557, time 446.56ms, mfu 0.83%\n",
            "iter 870: loss 1.8938, time 443.88ms, mfu 0.83%\n",
            "iter 880: loss 1.8078, time 456.32ms, mfu 0.83%\n",
            "iter 890: loss 1.8838, time 441.49ms, mfu 0.84%\n",
            "iter 900: loss 1.8458, time 443.44ms, mfu 0.84%\n",
            "iter 910: loss 1.8919, time 472.32ms, mfu 0.84%\n",
            "iter 920: loss 1.9329, time 448.05ms, mfu 0.84%\n",
            "iter 930: loss 1.7948, time 451.01ms, mfu 0.84%\n",
            "iter 940: loss 1.9014, time 446.48ms, mfu 0.85%\n",
            "iter 950: loss 1.7477, time 443.69ms, mfu 0.85%\n",
            "iter 960: loss 1.8028, time 450.38ms, mfu 0.85%\n",
            "iter 970: loss 1.8490, time 444.72ms, mfu 0.85%\n",
            "iter 980: loss 1.7721, time 445.56ms, mfu 0.86%\n",
            "iter 990: loss 1.8697, time 448.93ms, mfu 0.86%\n",
            "step 1000: train loss 1.6935, val loss 1.8589\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "iter 1000: loss 1.8198, time 2336.30ms, mfu 0.79%\n",
            "\n",
            "=== Experiment 15/32: b64_L6_H4_E256_BS16_MI2000_D10_s15 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS16_MI2000_D10_s15.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 15\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2405, val loss 4.2356\n",
            "iter 0: loss 4.2344, time 2606.33ms, mfu -100.00%\n",
            "iter 10: loss 4.1896, time 452.59ms, mfu 0.86%\n",
            "iter 20: loss 3.9887, time 453.86ms, mfu 0.86%\n",
            "iter 30: loss 3.7851, time 450.88ms, mfu 0.86%\n",
            "iter 40: loss 3.6314, time 455.34ms, mfu 0.86%\n",
            "iter 50: loss 3.5079, time 461.00ms, mfu 0.86%\n",
            "iter 60: loss 3.4069, time 455.37ms, mfu 0.86%\n",
            "iter 70: loss 3.2966, time 459.69ms, mfu 0.86%\n",
            "iter 80: loss 3.1370, time 475.88ms, mfu 0.85%\n",
            "iter 90: loss 3.1097, time 452.17ms, mfu 0.85%\n",
            "iter 100: loss 3.0409, time 448.90ms, mfu 0.85%\n",
            "iter 110: loss 2.9823, time 454.40ms, mfu 0.85%\n",
            "iter 120: loss 2.8784, time 452.41ms, mfu 0.85%\n",
            "iter 130: loss 2.8985, time 457.07ms, mfu 0.85%\n",
            "iter 140: loss 2.8320, time 451.94ms, mfu 0.85%\n",
            "iter 150: loss 2.8463, time 453.18ms, mfu 0.86%\n",
            "iter 160: loss 2.8096, time 459.79ms, mfu 0.85%\n",
            "iter 170: loss 2.7505, time 450.61ms, mfu 0.85%\n",
            "iter 180: loss 2.7500, time 448.43ms, mfu 0.86%\n",
            "iter 190: loss 2.6250, time 448.00ms, mfu 0.86%\n",
            "step 200: train loss 2.6585, val loss 2.6734\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 200: loss 2.6494, time 2335.07ms, mfu 0.79%\n",
            "iter 210: loss 2.6714, time 456.90ms, mfu 0.79%\n",
            "iter 220: loss 2.5859, time 446.79ms, mfu 0.80%\n",
            "iter 230: loss 2.6295, time 453.07ms, mfu 0.81%\n",
            "iter 240: loss 2.6199, time 452.62ms, mfu 0.81%\n",
            "iter 250: loss 2.6295, time 453.40ms, mfu 0.82%\n",
            "iter 260: loss 2.5175, time 456.15ms, mfu 0.82%\n",
            "iter 270: loss 2.5698, time 455.12ms, mfu 0.82%\n",
            "iter 280: loss 2.5654, time 453.80ms, mfu 0.83%\n",
            "iter 290: loss 2.4971, time 449.92ms, mfu 0.83%\n",
            "iter 300: loss 2.4645, time 446.56ms, mfu 0.83%\n",
            "iter 310: loss 2.4341, time 448.53ms, mfu 0.84%\n",
            "iter 320: loss 2.4453, time 448.14ms, mfu 0.84%\n",
            "iter 330: loss 2.4231, time 452.09ms, mfu 0.84%\n",
            "iter 340: loss 2.3910, time 449.22ms, mfu 0.85%\n",
            "iter 350: loss 2.4302, time 448.95ms, mfu 0.85%\n",
            "iter 360: loss 2.3770, time 448.98ms, mfu 0.85%\n",
            "iter 370: loss 2.3648, time 454.70ms, mfu 0.85%\n",
            "iter 380: loss 2.3476, time 451.57ms, mfu 0.85%\n",
            "iter 390: loss 2.3236, time 452.27ms, mfu 0.85%\n",
            "step 400: train loss 2.2870, val loss 2.3075\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 400: loss 2.3145, time 2342.05ms, mfu 0.78%\n",
            "iter 410: loss 2.3348, time 459.85ms, mfu 0.79%\n",
            "iter 420: loss 2.3385, time 454.99ms, mfu 0.80%\n",
            "iter 430: loss 2.2969, time 469.25ms, mfu 0.80%\n",
            "iter 440: loss 2.3428, time 468.55ms, mfu 0.80%\n",
            "iter 450: loss 2.3246, time 453.02ms, mfu 0.81%\n",
            "iter 460: loss 2.3020, time 449.92ms, mfu 0.81%\n",
            "iter 470: loss 2.2695, time 473.83ms, mfu 0.81%\n",
            "iter 480: loss 2.3003, time 452.33ms, mfu 0.82%\n",
            "iter 490: loss 2.1978, time 450.79ms, mfu 0.82%\n",
            "iter 500: loss 2.1519, time 460.11ms, mfu 0.83%\n",
            "iter 510: loss 2.1231, time 458.06ms, mfu 0.83%\n",
            "iter 520: loss 2.0761, time 461.48ms, mfu 0.83%\n",
            "iter 530: loss 2.1156, time 451.36ms, mfu 0.83%\n",
            "iter 540: loss 2.1623, time 451.25ms, mfu 0.84%\n",
            "iter 550: loss 2.1086, time 457.59ms, mfu 0.84%\n",
            "iter 560: loss 2.0621, time 453.05ms, mfu 0.84%\n",
            "iter 570: loss 2.1577, time 449.51ms, mfu 0.84%\n",
            "iter 580: loss 2.0758, time 450.75ms, mfu 0.84%\n",
            "iter 590: loss 2.1101, time 445.18ms, mfu 0.85%\n",
            "step 600: train loss 1.9872, val loss 2.0526\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 600: loss 2.0058, time 2390.97ms, mfu 0.78%\n",
            "iter 610: loss 2.0435, time 452.41ms, mfu 0.79%\n",
            "iter 620: loss 2.0262, time 448.92ms, mfu 0.79%\n",
            "iter 630: loss 2.0139, time 460.70ms, mfu 0.80%\n",
            "iter 640: loss 1.9721, time 453.22ms, mfu 0.80%\n",
            "iter 650: loss 1.9330, time 460.80ms, mfu 0.81%\n",
            "iter 660: loss 2.0682, time 456.09ms, mfu 0.81%\n",
            "iter 670: loss 1.9269, time 454.83ms, mfu 0.82%\n",
            "iter 680: loss 1.9818, time 459.56ms, mfu 0.82%\n",
            "iter 690: loss 1.9591, time 452.66ms, mfu 0.82%\n",
            "iter 700: loss 1.8255, time 457.09ms, mfu 0.83%\n",
            "iter 710: loss 1.9742, time 448.01ms, mfu 0.83%\n",
            "iter 720: loss 1.8787, time 458.73ms, mfu 0.83%\n",
            "iter 730: loss 1.9307, time 469.71ms, mfu 0.83%\n",
            "iter 740: loss 1.8993, time 452.65ms, mfu 0.83%\n",
            "iter 750: loss 1.8058, time 448.01ms, mfu 0.84%\n",
            "iter 760: loss 1.8646, time 450.69ms, mfu 0.84%\n",
            "iter 770: loss 1.9279, time 451.16ms, mfu 0.84%\n",
            "iter 780: loss 1.8142, time 451.17ms, mfu 0.84%\n",
            "iter 790: loss 1.9127, time 449.92ms, mfu 0.85%\n",
            "step 800: train loss 1.7400, val loss 1.8802\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 800: loss 1.7595, time 2338.26ms, mfu 0.78%\n",
            "iter 810: loss 1.8504, time 449.45ms, mfu 0.79%\n",
            "iter 820: loss 1.7469, time 450.76ms, mfu 0.79%\n",
            "iter 830: loss 1.7819, time 452.62ms, mfu 0.80%\n",
            "iter 840: loss 1.8677, time 453.07ms, mfu 0.81%\n",
            "iter 850: loss 1.7761, time 457.03ms, mfu 0.81%\n",
            "iter 860: loss 1.7102, time 461.13ms, mfu 0.81%\n",
            "iter 870: loss 1.7744, time 449.34ms, mfu 0.82%\n",
            "iter 880: loss 1.6904, time 446.09ms, mfu 0.82%\n",
            "iter 890: loss 1.7619, time 452.00ms, mfu 0.83%\n",
            "iter 900: loss 1.7204, time 446.59ms, mfu 0.83%\n",
            "iter 910: loss 1.7540, time 453.49ms, mfu 0.83%\n",
            "iter 920: loss 1.8199, time 451.03ms, mfu 0.84%\n",
            "iter 930: loss 1.6695, time 453.27ms, mfu 0.84%\n",
            "iter 940: loss 1.7926, time 455.11ms, mfu 0.84%\n",
            "iter 950: loss 1.6287, time 447.63ms, mfu 0.84%\n",
            "iter 960: loss 1.6748, time 450.90ms, mfu 0.85%\n",
            "iter 970: loss 1.7285, time 451.32ms, mfu 0.85%\n",
            "iter 980: loss 1.6695, time 452.62ms, mfu 0.85%\n",
            "iter 990: loss 1.7066, time 454.46ms, mfu 0.85%\n",
            "step 1000: train loss 1.5928, val loss 1.7779\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1000: loss 1.6831, time 2332.00ms, mfu 0.78%\n",
            "iter 1010: loss 1.6686, time 449.24ms, mfu 0.79%\n",
            "iter 1020: loss 1.6260, time 446.99ms, mfu 0.80%\n",
            "iter 1030: loss 1.6107, time 458.60ms, mfu 0.80%\n",
            "iter 1040: loss 1.7907, time 461.74ms, mfu 0.81%\n",
            "iter 1050: loss 1.6768, time 455.16ms, mfu 0.81%\n",
            "iter 1060: loss 1.7193, time 449.40ms, mfu 0.82%\n",
            "iter 1070: loss 1.6448, time 471.66ms, mfu 0.82%\n",
            "iter 1080: loss 1.6457, time 456.15ms, mfu 0.82%\n",
            "iter 1090: loss 1.6252, time 458.35ms, mfu 0.82%\n",
            "iter 1100: loss 1.5147, time 453.20ms, mfu 0.83%\n",
            "iter 1110: loss 1.6126, time 458.40ms, mfu 0.83%\n",
            "iter 1120: loss 1.5478, time 464.03ms, mfu 0.83%\n",
            "iter 1130: loss 1.6921, time 459.40ms, mfu 0.83%\n",
            "iter 1140: loss 1.6156, time 460.86ms, mfu 0.83%\n",
            "iter 1150: loss 1.6807, time 454.77ms, mfu 0.83%\n",
            "iter 1160: loss 1.5966, time 444.96ms, mfu 0.84%\n",
            "iter 1170: loss 1.6007, time 446.90ms, mfu 0.84%\n",
            "iter 1180: loss 1.6141, time 455.32ms, mfu 0.84%\n",
            "iter 1190: loss 1.5597, time 452.55ms, mfu 0.84%\n",
            "step 1200: train loss 1.4810, val loss 1.6695\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1200: loss 1.6135, time 2327.59ms, mfu 0.78%\n",
            "iter 1210: loss 1.6477, time 456.73ms, mfu 0.78%\n",
            "iter 1220: loss 1.4289, time 452.60ms, mfu 0.79%\n",
            "iter 1230: loss 1.5571, time 463.24ms, mfu 0.80%\n",
            "iter 1240: loss 1.4571, time 455.15ms, mfu 0.80%\n",
            "iter 1250: loss 1.5133, time 458.07ms, mfu 0.81%\n",
            "iter 1260: loss 1.6190, time 448.26ms, mfu 0.81%\n",
            "iter 1270: loss 1.4409, time 454.31ms, mfu 0.82%\n",
            "iter 1280: loss 1.5929, time 451.26ms, mfu 0.82%\n",
            "iter 1290: loss 1.5339, time 453.74ms, mfu 0.83%\n",
            "iter 1300: loss 1.5043, time 453.19ms, mfu 0.83%\n",
            "iter 1310: loss 1.5762, time 451.21ms, mfu 0.83%\n",
            "iter 1320: loss 1.4987, time 454.33ms, mfu 0.83%\n",
            "iter 1330: loss 1.6174, time 455.77ms, mfu 0.84%\n",
            "iter 1340: loss 1.4376, time 458.30ms, mfu 0.84%\n",
            "iter 1350: loss 1.5227, time 452.62ms, mfu 0.84%\n",
            "iter 1360: loss 1.4847, time 449.18ms, mfu 0.84%\n",
            "iter 1370: loss 1.4454, time 449.42ms, mfu 0.84%\n",
            "iter 1380: loss 1.4413, time 449.24ms, mfu 0.85%\n",
            "iter 1390: loss 1.3879, time 450.39ms, mfu 0.85%\n",
            "step 1400: train loss 1.4110, val loss 1.6193\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1400: loss 1.4704, time 2364.40ms, mfu 0.78%\n",
            "iter 1410: loss 1.4558, time 448.84ms, mfu 0.79%\n",
            "iter 1420: loss 1.4809, time 449.30ms, mfu 0.80%\n",
            "iter 1430: loss 1.4584, time 464.37ms, mfu 0.80%\n",
            "iter 1440: loss 1.5126, time 453.06ms, mfu 0.81%\n",
            "iter 1450: loss 1.4817, time 459.65ms, mfu 0.81%\n",
            "iter 1460: loss 1.4564, time 465.54ms, mfu 0.81%\n",
            "iter 1470: loss 1.4629, time 449.54ms, mfu 0.82%\n",
            "iter 1480: loss 1.4825, time 447.52ms, mfu 0.82%\n",
            "iter 1490: loss 1.4812, time 458.02ms, mfu 0.83%\n",
            "iter 1500: loss 1.4208, time 453.38ms, mfu 0.83%\n",
            "iter 1510: loss 1.4062, time 474.03ms, mfu 0.83%\n",
            "iter 1520: loss 1.4086, time 461.05ms, mfu 0.83%\n",
            "iter 1530: loss 1.5576, time 450.58ms, mfu 0.83%\n",
            "iter 1540: loss 1.4592, time 462.96ms, mfu 0.83%\n",
            "iter 1550: loss 1.4397, time 463.66ms, mfu 0.83%\n",
            "iter 1560: loss 1.3371, time 454.73ms, mfu 0.84%\n",
            "iter 1570: loss 1.4081, time 449.19ms, mfu 0.84%\n",
            "iter 1580: loss 1.3988, time 451.05ms, mfu 0.84%\n",
            "iter 1590: loss 1.3612, time 453.34ms, mfu 0.84%\n",
            "step 1600: train loss 1.3484, val loss 1.5751\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1600: loss 1.4113, time 2320.06ms, mfu 0.78%\n",
            "iter 1610: loss 1.5026, time 449.18ms, mfu 0.78%\n",
            "iter 1620: loss 1.4304, time 466.15ms, mfu 0.79%\n",
            "iter 1630: loss 1.4296, time 454.95ms, mfu 0.80%\n",
            "iter 1640: loss 1.4065, time 461.34ms, mfu 0.80%\n",
            "iter 1650: loss 1.4410, time 454.83ms, mfu 0.81%\n",
            "iter 1660: loss 1.3960, time 455.56ms, mfu 0.81%\n",
            "iter 1670: loss 1.3708, time 452.41ms, mfu 0.82%\n",
            "iter 1680: loss 1.2554, time 450.57ms, mfu 0.82%\n",
            "iter 1690: loss 1.5015, time 454.09ms, mfu 0.82%\n",
            "iter 1700: loss 1.4895, time 451.35ms, mfu 0.83%\n",
            "iter 1710: loss 1.3949, time 446.62ms, mfu 0.83%\n",
            "iter 1720: loss 1.4017, time 463.42ms, mfu 0.83%\n",
            "iter 1730: loss 1.4134, time 454.78ms, mfu 0.83%\n",
            "iter 1740: loss 1.3978, time 453.38ms, mfu 0.84%\n",
            "iter 1750: loss 1.5280, time 454.07ms, mfu 0.84%\n",
            "iter 1760: loss 1.4193, time 450.73ms, mfu 0.84%\n",
            "iter 1770: loss 1.3364, time 458.58ms, mfu 0.84%\n",
            "iter 1780: loss 1.3840, time 457.60ms, mfu 0.84%\n",
            "iter 1790: loss 1.4705, time 454.35ms, mfu 0.84%\n",
            "step 1800: train loss 1.3185, val loss 1.5614\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "iter 1800: loss 1.4495, time 2347.35ms, mfu 0.78%\n",
            "iter 1810: loss 1.3783, time 452.95ms, mfu 0.78%\n",
            "iter 1820: loss 1.4059, time 458.46ms, mfu 0.79%\n",
            "iter 1830: loss 1.4091, time 464.76ms, mfu 0.80%\n",
            "iter 1840: loss 1.3672, time 450.28ms, mfu 0.80%\n",
            "iter 1850: loss 1.4814, time 461.55ms, mfu 0.81%\n",
            "iter 1860: loss 1.3511, time 453.20ms, mfu 0.81%\n",
            "iter 1870: loss 1.3522, time 455.14ms, mfu 0.82%\n",
            "iter 1880: loss 1.3574, time 447.85ms, mfu 0.82%\n",
            "iter 1890: loss 1.3250, time 448.96ms, mfu 0.83%\n",
            "iter 1900: loss 1.4736, time 451.48ms, mfu 0.83%\n",
            "iter 1910: loss 1.3865, time 449.69ms, mfu 0.83%\n",
            "iter 1920: loss 1.3462, time 449.59ms, mfu 0.84%\n",
            "iter 1930: loss 1.3146, time 453.33ms, mfu 0.84%\n",
            "iter 1940: loss 1.3119, time 455.90ms, mfu 0.84%\n",
            "iter 1950: loss 1.3207, time 447.65ms, mfu 0.84%\n",
            "iter 1960: loss 1.2822, time 456.82ms, mfu 0.84%\n",
            "iter 1970: loss 1.2914, time 456.01ms, mfu 0.84%\n",
            "iter 1980: loss 1.3893, time 459.45ms, mfu 0.84%\n",
            "iter 1990: loss 1.3951, time 451.84ms, mfu 0.85%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 16/32: b64_L6_H4_E256_BS16_MI2000_D20_s16 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H4_E256_BS16_MI2000_D20_s16.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 4\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 16\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2405, val loss 4.2356\n",
            "iter 0: loss 4.2322, time 2535.48ms, mfu -100.00%\n",
            "iter 10: loss 4.2043, time 439.98ms, mfu 0.88%\n",
            "iter 20: loss 4.0240, time 444.61ms, mfu 0.88%\n",
            "iter 30: loss 3.8298, time 444.53ms, mfu 0.88%\n",
            "iter 40: loss 3.6725, time 448.57ms, mfu 0.88%\n",
            "iter 50: loss 3.5446, time 444.79ms, mfu 0.88%\n",
            "iter 60: loss 3.4502, time 443.00ms, mfu 0.88%\n",
            "iter 70: loss 3.3634, time 448.13ms, mfu 0.88%\n",
            "iter 80: loss 3.2168, time 485.85ms, mfu 0.87%\n",
            "iter 90: loss 3.1820, time 437.28ms, mfu 0.87%\n",
            "iter 100: loss 3.1133, time 447.01ms, mfu 0.87%\n",
            "iter 110: loss 3.0587, time 439.91ms, mfu 0.87%\n",
            "iter 120: loss 2.9342, time 440.60ms, mfu 0.87%\n",
            "iter 130: loss 2.9539, time 445.55ms, mfu 0.87%\n",
            "iter 140: loss 2.8773, time 446.80ms, mfu 0.87%\n",
            "iter 150: loss 2.8981, time 441.14ms, mfu 0.87%\n",
            "iter 160: loss 2.8657, time 445.83ms, mfu 0.87%\n",
            "iter 170: loss 2.7892, time 443.30ms, mfu 0.87%\n",
            "iter 180: loss 2.7909, time 445.41ms, mfu 0.87%\n",
            "iter 190: loss 2.6731, time 445.88ms, mfu 0.87%\n",
            "step 200: train loss 2.6836, val loss 2.6941\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 200: loss 2.7033, time 2266.94ms, mfu 0.80%\n",
            "iter 210: loss 2.7138, time 455.53ms, mfu 0.81%\n",
            "iter 220: loss 2.6241, time 446.31ms, mfu 0.81%\n",
            "iter 230: loss 2.6723, time 442.82ms, mfu 0.82%\n",
            "iter 240: loss 2.6645, time 462.24ms, mfu 0.82%\n",
            "iter 250: loss 2.6657, time 445.64ms, mfu 0.83%\n",
            "iter 260: loss 2.5749, time 450.80ms, mfu 0.83%\n",
            "iter 270: loss 2.6122, time 453.10ms, mfu 0.83%\n",
            "iter 280: loss 2.6056, time 451.71ms, mfu 0.84%\n",
            "iter 290: loss 2.5462, time 455.41ms, mfu 0.84%\n",
            "iter 300: loss 2.5158, time 449.10ms, mfu 0.84%\n",
            "iter 310: loss 2.4907, time 446.76ms, mfu 0.84%\n",
            "iter 320: loss 2.5078, time 454.47ms, mfu 0.85%\n",
            "iter 330: loss 2.4889, time 446.62ms, mfu 0.85%\n",
            "iter 340: loss 2.4455, time 447.80ms, mfu 0.85%\n",
            "iter 350: loss 2.4966, time 447.81ms, mfu 0.85%\n",
            "iter 360: loss 2.4227, time 451.43ms, mfu 0.85%\n",
            "iter 370: loss 2.4013, time 462.33ms, mfu 0.85%\n",
            "iter 380: loss 2.4196, time 442.62ms, mfu 0.85%\n",
            "iter 390: loss 2.3964, time 440.78ms, mfu 0.86%\n",
            "step 400: train loss 2.3468, val loss 2.3586\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 400: loss 2.3773, time 2336.43ms, mfu 0.79%\n",
            "iter 410: loss 2.4007, time 447.57ms, mfu 0.80%\n",
            "iter 420: loss 2.4134, time 458.15ms, mfu 0.80%\n",
            "iter 430: loss 2.3678, time 457.91ms, mfu 0.81%\n",
            "iter 440: loss 2.4020, time 442.55ms, mfu 0.81%\n",
            "iter 450: loss 2.4052, time 446.28ms, mfu 0.82%\n",
            "iter 460: loss 2.3418, time 451.43ms, mfu 0.82%\n",
            "iter 470: loss 2.3560, time 442.97ms, mfu 0.83%\n",
            "iter 480: loss 2.3740, time 441.30ms, mfu 0.83%\n",
            "iter 490: loss 2.2766, time 450.04ms, mfu 0.84%\n",
            "iter 500: loss 2.2562, time 445.16ms, mfu 0.84%\n",
            "iter 510: loss 2.2100, time 453.42ms, mfu 0.84%\n",
            "iter 520: loss 2.1640, time 447.04ms, mfu 0.84%\n",
            "iter 530: loss 2.2061, time 452.13ms, mfu 0.85%\n",
            "iter 540: loss 2.2613, time 442.66ms, mfu 0.85%\n",
            "iter 550: loss 2.1988, time 448.09ms, mfu 0.85%\n",
            "iter 560: loss 2.1982, time 461.20ms, mfu 0.85%\n",
            "iter 570: loss 2.2791, time 443.87ms, mfu 0.85%\n",
            "iter 580: loss 2.1930, time 443.23ms, mfu 0.86%\n",
            "iter 590: loss 2.2188, time 442.79ms, mfu 0.86%\n",
            "step 600: train loss 2.0801, val loss 2.1246\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 600: loss 2.1194, time 2284.26ms, mfu 0.79%\n",
            "iter 610: loss 2.1684, time 452.21ms, mfu 0.80%\n",
            "iter 620: loss 2.1211, time 449.55ms, mfu 0.80%\n",
            "iter 630: loss 2.1007, time 450.23ms, mfu 0.81%\n",
            "iter 640: loss 2.1203, time 452.00ms, mfu 0.81%\n",
            "iter 650: loss 2.0322, time 442.14ms, mfu 0.82%\n",
            "iter 660: loss 2.2006, time 456.09ms, mfu 0.82%\n",
            "iter 670: loss 2.0357, time 451.87ms, mfu 0.83%\n",
            "iter 680: loss 2.1092, time 441.69ms, mfu 0.83%\n",
            "iter 690: loss 2.0661, time 447.20ms, mfu 0.84%\n",
            "iter 700: loss 1.9742, time 446.38ms, mfu 0.84%\n",
            "iter 710: loss 2.1251, time 440.07ms, mfu 0.84%\n",
            "iter 720: loss 2.0190, time 441.53ms, mfu 0.85%\n",
            "iter 730: loss 2.0905, time 442.28ms, mfu 0.85%\n",
            "iter 740: loss 2.0072, time 436.91ms, mfu 0.85%\n",
            "iter 750: loss 1.9201, time 446.83ms, mfu 0.86%\n",
            "iter 760: loss 2.0124, time 440.76ms, mfu 0.86%\n",
            "iter 770: loss 2.0458, time 449.26ms, mfu 0.86%\n",
            "iter 780: loss 1.9605, time 446.36ms, mfu 0.86%\n",
            "iter 790: loss 2.0513, time 437.83ms, mfu 0.86%\n",
            "step 800: train loss 1.8540, val loss 1.9607\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 800: loss 1.8873, time 2301.21ms, mfu 0.79%\n",
            "iter 810: loss 2.0005, time 446.02ms, mfu 0.80%\n",
            "iter 820: loss 1.9146, time 459.57ms, mfu 0.81%\n",
            "iter 830: loss 1.9030, time 460.79ms, mfu 0.81%\n",
            "iter 840: loss 2.0144, time 446.08ms, mfu 0.82%\n",
            "iter 850: loss 1.9254, time 458.10ms, mfu 0.82%\n",
            "iter 860: loss 1.8557, time 445.77ms, mfu 0.82%\n",
            "iter 870: loss 1.8938, time 445.72ms, mfu 0.83%\n",
            "iter 880: loss 1.8078, time 441.34ms, mfu 0.83%\n",
            "iter 890: loss 1.8838, time 445.75ms, mfu 0.84%\n",
            "iter 900: loss 1.8458, time 444.52ms, mfu 0.84%\n",
            "iter 910: loss 1.8919, time 446.12ms, mfu 0.84%\n",
            "iter 920: loss 1.9329, time 441.49ms, mfu 0.85%\n",
            "iter 930: loss 1.7948, time 457.50ms, mfu 0.85%\n",
            "iter 940: loss 1.9014, time 438.31ms, mfu 0.85%\n",
            "iter 950: loss 1.7477, time 439.75ms, mfu 0.86%\n",
            "iter 960: loss 1.8028, time 454.14ms, mfu 0.86%\n",
            "iter 970: loss 1.8490, time 435.10ms, mfu 0.86%\n",
            "iter 980: loss 1.7721, time 439.19ms, mfu 0.86%\n",
            "iter 990: loss 1.8697, time 440.19ms, mfu 0.86%\n",
            "step 1000: train loss 1.6935, val loss 1.8589\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1000: loss 1.8198, time 2283.16ms, mfu 0.79%\n",
            "iter 1010: loss 1.7705, time 450.85ms, mfu 0.80%\n",
            "iter 1020: loss 1.7585, time 439.07ms, mfu 0.81%\n",
            "iter 1030: loss 1.7019, time 452.88ms, mfu 0.81%\n",
            "iter 1040: loss 1.9361, time 441.89ms, mfu 0.82%\n",
            "iter 1050: loss 1.8159, time 444.30ms, mfu 0.83%\n",
            "iter 1060: loss 1.8463, time 436.90ms, mfu 0.83%\n",
            "iter 1070: loss 1.7757, time 444.12ms, mfu 0.84%\n",
            "iter 1080: loss 1.7743, time 445.40ms, mfu 0.84%\n",
            "iter 1090: loss 1.7592, time 461.47ms, mfu 0.84%\n",
            "iter 1100: loss 1.6400, time 442.62ms, mfu 0.84%\n",
            "iter 1110: loss 1.7540, time 461.47ms, mfu 0.84%\n",
            "iter 1120: loss 1.6458, time 443.73ms, mfu 0.85%\n",
            "iter 1130: loss 1.8301, time 441.55ms, mfu 0.85%\n",
            "iter 1140: loss 1.7259, time 447.55ms, mfu 0.85%\n",
            "iter 1150: loss 1.7784, time 444.65ms, mfu 0.85%\n",
            "iter 1160: loss 1.6894, time 438.70ms, mfu 0.86%\n",
            "iter 1170: loss 1.7035, time 439.26ms, mfu 0.86%\n",
            "iter 1180: loss 1.7397, time 443.21ms, mfu 0.86%\n",
            "iter 1190: loss 1.6892, time 445.11ms, mfu 0.86%\n",
            "step 1200: train loss 1.5721, val loss 1.7476\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1200: loss 1.7314, time 2279.70ms, mfu 0.79%\n",
            "iter 1210: loss 1.7522, time 444.57ms, mfu 0.80%\n",
            "iter 1220: loss 1.5355, time 450.32ms, mfu 0.81%\n",
            "iter 1230: loss 1.6644, time 446.03ms, mfu 0.81%\n",
            "iter 1240: loss 1.5776, time 439.71ms, mfu 0.82%\n",
            "iter 1250: loss 1.6178, time 446.22ms, mfu 0.83%\n",
            "iter 1260: loss 1.6962, time 439.55ms, mfu 0.83%\n",
            "iter 1270: loss 1.5473, time 442.24ms, mfu 0.84%\n",
            "iter 1280: loss 1.6871, time 441.49ms, mfu 0.84%\n",
            "iter 1290: loss 1.6334, time 434.43ms, mfu 0.85%\n",
            "iter 1300: loss 1.6287, time 442.26ms, mfu 0.85%\n",
            "iter 1310: loss 1.6984, time 440.28ms, mfu 0.85%\n",
            "iter 1320: loss 1.5942, time 446.34ms, mfu 0.85%\n",
            "iter 1330: loss 1.7290, time 446.92ms, mfu 0.86%\n",
            "iter 1340: loss 1.5711, time 449.48ms, mfu 0.86%\n",
            "iter 1350: loss 1.6531, time 446.03ms, mfu 0.86%\n",
            "iter 1360: loss 1.5968, time 465.37ms, mfu 0.86%\n",
            "iter 1370: loss 1.5378, time 450.71ms, mfu 0.86%\n",
            "iter 1380: loss 1.5263, time 445.75ms, mfu 0.86%\n",
            "iter 1390: loss 1.5234, time 442.30ms, mfu 0.86%\n",
            "step 1400: train loss 1.4907, val loss 1.6869\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1400: loss 1.5612, time 2286.53ms, mfu 0.79%\n",
            "iter 1410: loss 1.5648, time 464.24ms, mfu 0.80%\n",
            "iter 1420: loss 1.5671, time 439.15ms, mfu 0.80%\n",
            "iter 1430: loss 1.5820, time 448.19ms, mfu 0.81%\n",
            "iter 1440: loss 1.6192, time 442.25ms, mfu 0.82%\n",
            "iter 1450: loss 1.6246, time 450.86ms, mfu 0.82%\n",
            "iter 1460: loss 1.5960, time 448.73ms, mfu 0.83%\n",
            "iter 1470: loss 1.5737, time 445.08ms, mfu 0.83%\n",
            "iter 1480: loss 1.5952, time 443.56ms, mfu 0.84%\n",
            "iter 1490: loss 1.5932, time 444.99ms, mfu 0.84%\n",
            "iter 1500: loss 1.5460, time 442.76ms, mfu 0.84%\n",
            "iter 1510: loss 1.4938, time 443.46ms, mfu 0.85%\n",
            "iter 1520: loss 1.4879, time 445.62ms, mfu 0.85%\n",
            "iter 1530: loss 1.6139, time 440.09ms, mfu 0.85%\n",
            "iter 1540: loss 1.5565, time 444.14ms, mfu 0.86%\n",
            "iter 1550: loss 1.5443, time 443.12ms, mfu 0.86%\n",
            "iter 1560: loss 1.4356, time 442.22ms, mfu 0.86%\n",
            "iter 1570: loss 1.5073, time 449.21ms, mfu 0.86%\n",
            "iter 1580: loss 1.5205, time 441.94ms, mfu 0.86%\n",
            "iter 1590: loss 1.4586, time 448.76ms, mfu 0.86%\n",
            "step 1600: train loss 1.4216, val loss 1.6322\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1600: loss 1.5195, time 2351.52ms, mfu 0.79%\n",
            "iter 1610: loss 1.6002, time 449.90ms, mfu 0.80%\n",
            "iter 1620: loss 1.5184, time 447.93ms, mfu 0.81%\n",
            "iter 1630: loss 1.5181, time 451.10ms, mfu 0.81%\n",
            "iter 1640: loss 1.5162, time 453.81ms, mfu 0.82%\n",
            "iter 1650: loss 1.5680, time 458.33ms, mfu 0.82%\n",
            "iter 1660: loss 1.4934, time 443.18ms, mfu 0.83%\n",
            "iter 1670: loss 1.4800, time 445.72ms, mfu 0.83%\n",
            "iter 1680: loss 1.3599, time 445.31ms, mfu 0.83%\n",
            "iter 1690: loss 1.6392, time 452.73ms, mfu 0.84%\n",
            "iter 1700: loss 1.5691, time 458.49ms, mfu 0.84%\n",
            "iter 1710: loss 1.4721, time 441.72ms, mfu 0.84%\n",
            "iter 1720: loss 1.4967, time 437.87ms, mfu 0.85%\n",
            "iter 1730: loss 1.5219, time 442.96ms, mfu 0.85%\n",
            "iter 1740: loss 1.4961, time 449.10ms, mfu 0.85%\n",
            "iter 1750: loss 1.6161, time 453.32ms, mfu 0.85%\n",
            "iter 1760: loss 1.5254, time 438.32ms, mfu 0.86%\n",
            "iter 1770: loss 1.4060, time 440.65ms, mfu 0.86%\n",
            "iter 1780: loss 1.4871, time 442.95ms, mfu 0.86%\n",
            "iter 1790: loss 1.5500, time 441.21ms, mfu 0.86%\n",
            "step 1800: train loss 1.3888, val loss 1.6100\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "iter 1800: loss 1.5447, time 2286.92ms, mfu 0.79%\n",
            "iter 1810: loss 1.4898, time 450.79ms, mfu 0.80%\n",
            "iter 1820: loss 1.5237, time 445.23ms, mfu 0.81%\n",
            "iter 1830: loss 1.5089, time 443.63ms, mfu 0.81%\n",
            "iter 1840: loss 1.4550, time 446.78ms, mfu 0.82%\n",
            "iter 1850: loss 1.5618, time 444.24ms, mfu 0.83%\n",
            "iter 1860: loss 1.4357, time 442.98ms, mfu 0.83%\n",
            "iter 1870: loss 1.4313, time 443.38ms, mfu 0.84%\n",
            "iter 1880: loss 1.4543, time 446.44ms, mfu 0.84%\n",
            "iter 1890: loss 1.4443, time 464.97ms, mfu 0.84%\n",
            "iter 1900: loss 1.5867, time 439.66ms, mfu 0.84%\n",
            "iter 1910: loss 1.4742, time 442.98ms, mfu 0.85%\n",
            "iter 1920: loss 1.4414, time 446.51ms, mfu 0.85%\n",
            "iter 1930: loss 1.4112, time 444.71ms, mfu 0.85%\n",
            "iter 1940: loss 1.4287, time 445.52ms, mfu 0.85%\n",
            "iter 1950: loss 1.4057, time 439.06ms, mfu 0.86%\n",
            "iter 1960: loss 1.3546, time 442.52ms, mfu 0.86%\n",
            "iter 1970: loss 1.4032, time 451.03ms, mfu 0.86%\n",
            "iter 1980: loss 1.5105, time 448.56ms, mfu 0.86%\n",
            "iter 1990: loss 1.4680, time 443.61ms, mfu 0.86%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 17/32: b64_L6_H8_E128_BS8_MI1000_D10_s17 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS8_MI1000_D10_s17.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 17\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1872, val loss 4.1823\n",
            "iter 0: loss 4.2154, time 2399.59ms, mfu -100.00%\n",
            "iter 10: loss 4.1796, time 448.63ms, mfu 0.11%\n",
            "iter 20: loss 4.1313, time 432.73ms, mfu 0.11%\n",
            "iter 30: loss 4.0448, time 431.02ms, mfu 0.11%\n",
            "iter 40: loss 3.9618, time 447.92ms, mfu 0.11%\n",
            "iter 50: loss 3.8523, time 438.14ms, mfu 0.11%\n",
            "iter 60: loss 3.8532, time 437.05ms, mfu 0.11%\n",
            "iter 70: loss 3.7076, time 448.16ms, mfu 0.11%\n",
            "iter 80: loss 3.6633, time 432.99ms, mfu 0.11%\n",
            "iter 90: loss 3.6127, time 441.34ms, mfu 0.11%\n",
            "iter 100: loss 3.5839, time 438.02ms, mfu 0.11%\n",
            "iter 110: loss 3.5235, time 438.97ms, mfu 0.11%\n",
            "iter 120: loss 3.5105, time 440.90ms, mfu 0.11%\n",
            "iter 130: loss 3.4605, time 432.76ms, mfu 0.11%\n",
            "iter 140: loss 3.4384, time 435.92ms, mfu 0.12%\n",
            "iter 150: loss 3.3742, time 442.42ms, mfu 0.12%\n",
            "iter 160: loss 3.3823, time 440.50ms, mfu 0.12%\n",
            "iter 170: loss 3.3621, time 439.54ms, mfu 0.12%\n",
            "iter 180: loss 3.2835, time 444.84ms, mfu 0.12%\n",
            "iter 190: loss 3.2485, time 443.20ms, mfu 0.11%\n",
            "step 200: train loss 3.2060, val loss 3.2118\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 200: loss 3.1856, time 2124.96ms, mfu 0.11%\n",
            "iter 210: loss 3.1829, time 429.06ms, mfu 0.11%\n",
            "iter 220: loss 3.2306, time 428.81ms, mfu 0.11%\n",
            "iter 230: loss 3.1376, time 442.57ms, mfu 0.11%\n",
            "iter 240: loss 3.1418, time 430.12ms, mfu 0.11%\n",
            "iter 250: loss 3.1160, time 431.67ms, mfu 0.11%\n",
            "iter 260: loss 3.0771, time 443.94ms, mfu 0.11%\n",
            "iter 270: loss 3.0618, time 433.93ms, mfu 0.11%\n",
            "iter 280: loss 3.0382, time 438.75ms, mfu 0.11%\n",
            "iter 290: loss 2.9735, time 435.25ms, mfu 0.11%\n",
            "iter 300: loss 2.9599, time 433.51ms, mfu 0.11%\n",
            "iter 310: loss 2.9860, time 434.99ms, mfu 0.11%\n",
            "iter 320: loss 2.9723, time 430.49ms, mfu 0.11%\n",
            "iter 330: loss 2.9304, time 432.33ms, mfu 0.11%\n",
            "iter 340: loss 2.9152, time 441.66ms, mfu 0.11%\n",
            "iter 350: loss 2.8451, time 438.09ms, mfu 0.11%\n",
            "iter 360: loss 2.8694, time 438.72ms, mfu 0.11%\n",
            "iter 370: loss 2.8949, time 452.95ms, mfu 0.11%\n",
            "iter 380: loss 2.7938, time 427.85ms, mfu 0.11%\n",
            "iter 390: loss 2.7696, time 428.11ms, mfu 0.12%\n",
            "step 400: train loss 2.7238, val loss 2.7401\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 400: loss 2.7467, time 2129.16ms, mfu 0.11%\n",
            "iter 410: loss 2.6936, time 438.22ms, mfu 0.11%\n",
            "iter 420: loss 2.7585, time 447.98ms, mfu 0.11%\n",
            "iter 430: loss 2.7110, time 437.24ms, mfu 0.11%\n",
            "iter 440: loss 2.7210, time 434.17ms, mfu 0.11%\n",
            "iter 450: loss 2.6478, time 446.62ms, mfu 0.11%\n",
            "iter 460: loss 2.5961, time 442.88ms, mfu 0.11%\n",
            "iter 470: loss 2.6531, time 442.68ms, mfu 0.11%\n",
            "iter 480: loss 2.6077, time 437.93ms, mfu 0.11%\n",
            "iter 490: loss 2.5327, time 439.80ms, mfu 0.11%\n",
            "iter 500: loss 2.5918, time 443.00ms, mfu 0.11%\n",
            "iter 510: loss 2.4774, time 435.31ms, mfu 0.11%\n",
            "iter 520: loss 2.6194, time 432.92ms, mfu 0.11%\n",
            "iter 530: loss 2.5190, time 441.79ms, mfu 0.11%\n",
            "iter 540: loss 2.5915, time 438.41ms, mfu 0.11%\n",
            "iter 550: loss 2.5453, time 434.58ms, mfu 0.11%\n",
            "iter 560: loss 2.6130, time 456.71ms, mfu 0.11%\n",
            "iter 570: loss 2.4885, time 437.35ms, mfu 0.11%\n",
            "iter 580: loss 2.4793, time 435.23ms, mfu 0.11%\n",
            "iter 590: loss 2.5191, time 434.36ms, mfu 0.11%\n",
            "step 600: train loss 2.4184, val loss 2.4239\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 600: loss 2.5259, time 2094.26ms, mfu 0.11%\n",
            "iter 610: loss 2.3905, time 445.49ms, mfu 0.11%\n",
            "iter 620: loss 2.4077, time 435.05ms, mfu 0.11%\n",
            "iter 630: loss 2.4401, time 439.19ms, mfu 0.11%\n",
            "iter 640: loss 2.4822, time 433.00ms, mfu 0.11%\n",
            "iter 650: loss 2.4877, time 435.55ms, mfu 0.11%\n",
            "iter 660: loss 2.4967, time 435.25ms, mfu 0.11%\n",
            "iter 670: loss 2.3678, time 434.20ms, mfu 0.11%\n",
            "iter 680: loss 2.4298, time 437.68ms, mfu 0.11%\n",
            "iter 690: loss 2.3825, time 441.17ms, mfu 0.11%\n",
            "iter 700: loss 2.3731, time 442.31ms, mfu 0.11%\n",
            "iter 710: loss 2.4399, time 441.58ms, mfu 0.11%\n",
            "iter 720: loss 2.3356, time 431.71ms, mfu 0.11%\n",
            "iter 730: loss 2.3918, time 440.66ms, mfu 0.11%\n",
            "iter 740: loss 2.3714, time 439.08ms, mfu 0.11%\n",
            "iter 750: loss 2.2802, time 439.25ms, mfu 0.11%\n",
            "iter 760: loss 2.4177, time 427.26ms, mfu 0.11%\n",
            "iter 770: loss 2.3995, time 431.34ms, mfu 0.11%\n",
            "iter 780: loss 2.3015, time 433.53ms, mfu 0.11%\n",
            "iter 790: loss 2.3345, time 436.88ms, mfu 0.11%\n",
            "step 800: train loss 2.2380, val loss 2.2574\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 800: loss 2.3200, time 2136.69ms, mfu 0.11%\n",
            "iter 810: loss 2.2797, time 441.20ms, mfu 0.11%\n",
            "iter 820: loss 2.3547, time 440.91ms, mfu 0.11%\n",
            "iter 830: loss 2.2375, time 441.71ms, mfu 0.11%\n",
            "iter 840: loss 2.3065, time 433.66ms, mfu 0.11%\n",
            "iter 850: loss 2.2784, time 430.99ms, mfu 0.11%\n",
            "iter 860: loss 2.1706, time 438.91ms, mfu 0.11%\n",
            "iter 870: loss 2.3055, time 439.11ms, mfu 0.11%\n",
            "iter 880: loss 2.1824, time 436.44ms, mfu 0.11%\n",
            "iter 890: loss 2.3120, time 434.60ms, mfu 0.11%\n",
            "iter 900: loss 2.1978, time 430.36ms, mfu 0.11%\n",
            "iter 910: loss 2.1866, time 439.36ms, mfu 0.11%\n",
            "iter 920: loss 2.2071, time 438.64ms, mfu 0.11%\n",
            "iter 930: loss 2.3091, time 440.27ms, mfu 0.11%\n",
            "iter 940: loss 2.2236, time 436.77ms, mfu 0.11%\n",
            "iter 950: loss 2.2636, time 432.82ms, mfu 0.11%\n",
            "iter 960: loss 2.2511, time 437.61ms, mfu 0.11%\n",
            "iter 970: loss 2.1551, time 447.36ms, mfu 0.11%\n",
            "iter 980: loss 2.1262, time 439.81ms, mfu 0.11%\n",
            "iter 990: loss 2.1320, time 439.51ms, mfu 0.11%\n",
            "step 1000: train loss 2.0774, val loss 2.1062\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "iter 1000: loss 2.1723, time 2124.83ms, mfu 0.11%\n",
            "\n",
            "=== Experiment 18/32: b64_L6_H8_E128_BS8_MI1000_D20_s18 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS8_MI1000_D20_s18.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 18\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1872, val loss 4.1823\n",
            "iter 0: loss 4.2109, time 2406.95ms, mfu -100.00%\n",
            "iter 10: loss 4.1878, time 435.20ms, mfu 0.12%\n",
            "iter 20: loss 4.1413, time 440.33ms, mfu 0.12%\n",
            "iter 30: loss 4.0663, time 440.87ms, mfu 0.12%\n",
            "iter 40: loss 3.9897, time 430.97ms, mfu 0.12%\n",
            "iter 50: loss 3.8888, time 439.21ms, mfu 0.12%\n",
            "iter 60: loss 3.8766, time 451.53ms, mfu 0.12%\n",
            "iter 70: loss 3.7336, time 432.35ms, mfu 0.12%\n",
            "iter 80: loss 3.6844, time 434.52ms, mfu 0.12%\n",
            "iter 90: loss 3.6405, time 443.12ms, mfu 0.12%\n",
            "iter 100: loss 3.6261, time 431.64ms, mfu 0.12%\n",
            "iter 110: loss 3.5604, time 433.58ms, mfu 0.12%\n",
            "iter 120: loss 3.5425, time 434.24ms, mfu 0.12%\n",
            "iter 130: loss 3.4968, time 432.31ms, mfu 0.12%\n",
            "iter 140: loss 3.4829, time 435.66ms, mfu 0.12%\n",
            "iter 150: loss 3.4208, time 435.86ms, mfu 0.12%\n",
            "iter 160: loss 3.4242, time 437.07ms, mfu 0.12%\n",
            "iter 170: loss 3.4016, time 436.13ms, mfu 0.12%\n",
            "iter 180: loss 3.3275, time 433.30ms, mfu 0.12%\n",
            "iter 190: loss 3.2778, time 437.19ms, mfu 0.12%\n",
            "step 200: train loss 3.2216, val loss 3.2272\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 200: loss 3.2269, time 2105.16ms, mfu 0.11%\n",
            "iter 210: loss 3.2166, time 435.41ms, mfu 0.11%\n",
            "iter 220: loss 3.2630, time 446.38ms, mfu 0.11%\n",
            "iter 230: loss 3.1655, time 434.18ms, mfu 0.11%\n",
            "iter 240: loss 3.1708, time 439.08ms, mfu 0.11%\n",
            "iter 250: loss 3.1443, time 457.83ms, mfu 0.11%\n",
            "iter 260: loss 3.1080, time 436.35ms, mfu 0.11%\n",
            "iter 270: loss 3.0955, time 435.93ms, mfu 0.11%\n",
            "iter 280: loss 3.0815, time 442.00ms, mfu 0.11%\n",
            "iter 290: loss 3.0139, time 442.03ms, mfu 0.11%\n",
            "iter 300: loss 2.9879, time 441.25ms, mfu 0.11%\n",
            "iter 310: loss 3.0114, time 430.57ms, mfu 0.11%\n",
            "iter 320: loss 3.0012, time 437.78ms, mfu 0.11%\n",
            "iter 330: loss 2.9617, time 435.41ms, mfu 0.11%\n",
            "iter 340: loss 2.9522, time 428.83ms, mfu 0.11%\n",
            "iter 350: loss 2.8785, time 433.94ms, mfu 0.11%\n",
            "iter 360: loss 2.9029, time 442.91ms, mfu 0.11%\n",
            "iter 370: loss 2.9186, time 431.53ms, mfu 0.11%\n",
            "iter 380: loss 2.8345, time 434.31ms, mfu 0.11%\n",
            "iter 390: loss 2.8020, time 434.48ms, mfu 0.12%\n",
            "step 400: train loss 2.7476, val loss 2.7613\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 400: loss 2.7929, time 2123.62ms, mfu 0.11%\n",
            "iter 410: loss 2.7362, time 446.45ms, mfu 0.11%\n",
            "iter 420: loss 2.7805, time 437.41ms, mfu 0.11%\n",
            "iter 430: loss 2.7293, time 432.77ms, mfu 0.11%\n",
            "iter 440: loss 2.7388, time 448.24ms, mfu 0.11%\n",
            "iter 450: loss 2.6832, time 436.20ms, mfu 0.11%\n",
            "iter 460: loss 2.6284, time 435.82ms, mfu 0.11%\n",
            "iter 470: loss 2.6833, time 434.53ms, mfu 0.11%\n",
            "iter 480: loss 2.6394, time 432.86ms, mfu 0.11%\n",
            "iter 490: loss 2.5629, time 435.69ms, mfu 0.11%\n",
            "iter 500: loss 2.6381, time 439.79ms, mfu 0.11%\n",
            "iter 510: loss 2.5179, time 433.88ms, mfu 0.11%\n",
            "iter 520: loss 2.6424, time 441.27ms, mfu 0.11%\n",
            "iter 530: loss 2.5482, time 439.41ms, mfu 0.11%\n",
            "iter 540: loss 2.6333, time 437.40ms, mfu 0.11%\n",
            "iter 550: loss 2.5667, time 446.85ms, mfu 0.11%\n",
            "iter 560: loss 2.6433, time 431.63ms, mfu 0.11%\n",
            "iter 570: loss 2.5314, time 433.90ms, mfu 0.11%\n",
            "iter 580: loss 2.5007, time 434.89ms, mfu 0.11%\n",
            "iter 590: loss 2.5646, time 429.74ms, mfu 0.11%\n",
            "step 600: train loss 2.4497, val loss 2.4512\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 600: loss 2.5488, time 2135.86ms, mfu 0.11%\n",
            "iter 610: loss 2.4178, time 436.64ms, mfu 0.11%\n",
            "iter 620: loss 2.4673, time 434.35ms, mfu 0.11%\n",
            "iter 630: loss 2.4939, time 452.45ms, mfu 0.11%\n",
            "iter 640: loss 2.5065, time 437.05ms, mfu 0.11%\n",
            "iter 650: loss 2.5242, time 437.37ms, mfu 0.11%\n",
            "iter 660: loss 2.5186, time 430.64ms, mfu 0.11%\n",
            "iter 670: loss 2.4102, time 435.60ms, mfu 0.11%\n",
            "iter 680: loss 2.4669, time 434.84ms, mfu 0.11%\n",
            "iter 690: loss 2.4366, time 433.75ms, mfu 0.11%\n",
            "iter 700: loss 2.4190, time 435.77ms, mfu 0.11%\n",
            "iter 710: loss 2.4689, time 449.97ms, mfu 0.11%\n",
            "iter 720: loss 2.3916, time 435.99ms, mfu 0.11%\n",
            "iter 730: loss 2.4086, time 441.26ms, mfu 0.11%\n",
            "iter 740: loss 2.4142, time 457.34ms, mfu 0.11%\n",
            "iter 750: loss 2.3349, time 439.73ms, mfu 0.11%\n",
            "iter 760: loss 2.4779, time 430.46ms, mfu 0.11%\n",
            "iter 770: loss 2.4549, time 431.08ms, mfu 0.11%\n",
            "iter 780: loss 2.3054, time 435.90ms, mfu 0.11%\n",
            "iter 790: loss 2.3748, time 430.83ms, mfu 0.11%\n",
            "step 800: train loss 2.2909, val loss 2.3035\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 800: loss 2.3709, time 2120.33ms, mfu 0.11%\n",
            "iter 810: loss 2.3446, time 431.36ms, mfu 0.11%\n",
            "iter 820: loss 2.4146, time 443.94ms, mfu 0.11%\n",
            "iter 830: loss 2.2695, time 432.78ms, mfu 0.11%\n",
            "iter 840: loss 2.3897, time 442.29ms, mfu 0.11%\n",
            "iter 850: loss 2.3505, time 436.85ms, mfu 0.11%\n",
            "iter 860: loss 2.2436, time 427.55ms, mfu 0.11%\n",
            "iter 870: loss 2.3992, time 432.03ms, mfu 0.11%\n",
            "iter 880: loss 2.2604, time 440.99ms, mfu 0.11%\n",
            "iter 890: loss 2.3818, time 435.52ms, mfu 0.11%\n",
            "iter 900: loss 2.2766, time 435.39ms, mfu 0.11%\n",
            "iter 910: loss 2.2598, time 432.72ms, mfu 0.11%\n",
            "iter 920: loss 2.3075, time 439.08ms, mfu 0.11%\n",
            "iter 930: loss 2.3892, time 444.28ms, mfu 0.11%\n",
            "iter 940: loss 2.3138, time 434.73ms, mfu 0.11%\n",
            "iter 950: loss 2.3408, time 439.24ms, mfu 0.11%\n",
            "iter 960: loss 2.3213, time 436.47ms, mfu 0.11%\n",
            "iter 970: loss 2.2661, time 436.00ms, mfu 0.11%\n",
            "iter 980: loss 2.2468, time 436.25ms, mfu 0.11%\n",
            "iter 990: loss 2.2040, time 433.22ms, mfu 0.11%\n",
            "step 1000: train loss 2.1584, val loss 2.1735\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "iter 1000: loss 2.2347, time 2093.42ms, mfu 0.11%\n",
            "\n",
            "=== Experiment 19/32: b64_L6_H8_E128_BS8_MI2000_D10_s19 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS8_MI2000_D10_s19.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 19\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1872, val loss 4.1823\n",
            "iter 0: loss 4.2154, time 2446.57ms, mfu -100.00%\n",
            "iter 10: loss 4.1796, time 440.54ms, mfu 0.12%\n",
            "iter 20: loss 4.1313, time 451.96ms, mfu 0.11%\n",
            "iter 30: loss 4.0448, time 438.60ms, mfu 0.11%\n",
            "iter 40: loss 3.9618, time 438.50ms, mfu 0.12%\n",
            "iter 50: loss 3.8523, time 438.73ms, mfu 0.12%\n",
            "iter 60: loss 3.8532, time 440.71ms, mfu 0.12%\n",
            "iter 70: loss 3.7076, time 434.89ms, mfu 0.12%\n",
            "iter 80: loss 3.6633, time 443.77ms, mfu 0.12%\n",
            "iter 90: loss 3.6127, time 439.56ms, mfu 0.12%\n",
            "iter 100: loss 3.5839, time 440.70ms, mfu 0.12%\n",
            "iter 110: loss 3.5235, time 443.02ms, mfu 0.12%\n",
            "iter 120: loss 3.5105, time 436.27ms, mfu 0.12%\n",
            "iter 130: loss 3.4605, time 452.14ms, mfu 0.11%\n",
            "iter 140: loss 3.4384, time 438.21ms, mfu 0.11%\n",
            "iter 150: loss 3.3742, time 433.92ms, mfu 0.12%\n",
            "iter 160: loss 3.3823, time 453.40ms, mfu 0.11%\n",
            "iter 170: loss 3.3621, time 443.52ms, mfu 0.11%\n",
            "iter 180: loss 3.2835, time 439.77ms, mfu 0.11%\n",
            "iter 190: loss 3.2485, time 450.83ms, mfu 0.11%\n",
            "step 200: train loss 3.2060, val loss 3.2118\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 200: loss 3.1856, time 2148.76ms, mfu 0.11%\n",
            "iter 210: loss 3.1829, time 445.63ms, mfu 0.11%\n",
            "iter 220: loss 3.2306, time 440.92ms, mfu 0.11%\n",
            "iter 230: loss 3.1376, time 437.95ms, mfu 0.11%\n",
            "iter 240: loss 3.1418, time 442.17ms, mfu 0.11%\n",
            "iter 250: loss 3.1160, time 437.64ms, mfu 0.11%\n",
            "iter 260: loss 3.0771, time 443.48ms, mfu 0.11%\n",
            "iter 270: loss 3.0618, time 437.11ms, mfu 0.11%\n",
            "iter 280: loss 3.0382, time 436.15ms, mfu 0.11%\n",
            "iter 290: loss 2.9735, time 444.38ms, mfu 0.11%\n",
            "iter 300: loss 2.9599, time 436.40ms, mfu 0.11%\n",
            "iter 310: loss 2.9860, time 443.51ms, mfu 0.11%\n",
            "iter 320: loss 2.9723, time 445.12ms, mfu 0.11%\n",
            "iter 330: loss 2.9304, time 438.48ms, mfu 0.11%\n",
            "iter 340: loss 2.9152, time 440.35ms, mfu 0.11%\n",
            "iter 350: loss 2.8451, time 450.84ms, mfu 0.11%\n",
            "iter 360: loss 2.8694, time 439.32ms, mfu 0.11%\n",
            "iter 370: loss 2.8949, time 439.98ms, mfu 0.11%\n",
            "iter 380: loss 2.7938, time 440.25ms, mfu 0.11%\n",
            "iter 390: loss 2.7696, time 440.37ms, mfu 0.11%\n",
            "step 400: train loss 2.7238, val loss 2.7401\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 400: loss 2.7467, time 2179.10ms, mfu 0.10%\n",
            "iter 410: loss 2.6936, time 437.02ms, mfu 0.11%\n",
            "iter 420: loss 2.7585, time 441.08ms, mfu 0.11%\n",
            "iter 430: loss 2.7110, time 446.47ms, mfu 0.11%\n",
            "iter 440: loss 2.7210, time 444.52ms, mfu 0.11%\n",
            "iter 450: loss 2.6478, time 444.46ms, mfu 0.11%\n",
            "iter 460: loss 2.5961, time 436.30ms, mfu 0.11%\n",
            "iter 470: loss 2.6531, time 436.30ms, mfu 0.11%\n",
            "iter 480: loss 2.6077, time 447.06ms, mfu 0.11%\n",
            "iter 490: loss 2.5327, time 434.50ms, mfu 0.11%\n",
            "iter 500: loss 2.5918, time 433.92ms, mfu 0.11%\n",
            "iter 510: loss 2.4774, time 447.14ms, mfu 0.11%\n",
            "iter 520: loss 2.6194, time 433.89ms, mfu 0.11%\n",
            "iter 530: loss 2.5190, time 434.89ms, mfu 0.11%\n",
            "iter 540: loss 2.5915, time 441.07ms, mfu 0.11%\n",
            "iter 550: loss 2.5453, time 429.57ms, mfu 0.11%\n",
            "iter 560: loss 2.6130, time 441.23ms, mfu 0.11%\n",
            "iter 570: loss 2.4885, time 436.59ms, mfu 0.11%\n",
            "iter 580: loss 2.4793, time 435.50ms, mfu 0.11%\n",
            "iter 590: loss 2.5191, time 448.90ms, mfu 0.11%\n",
            "step 600: train loss 2.4184, val loss 2.4239\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 600: loss 2.5259, time 2117.13ms, mfu 0.11%\n",
            "iter 610: loss 2.3905, time 433.76ms, mfu 0.11%\n",
            "iter 620: loss 2.4077, time 433.95ms, mfu 0.11%\n",
            "iter 630: loss 2.4401, time 436.18ms, mfu 0.11%\n",
            "iter 640: loss 2.4822, time 446.46ms, mfu 0.11%\n",
            "iter 650: loss 2.4877, time 435.48ms, mfu 0.11%\n",
            "iter 660: loss 2.4967, time 435.56ms, mfu 0.11%\n",
            "iter 670: loss 2.3678, time 442.11ms, mfu 0.11%\n",
            "iter 680: loss 2.4298, time 436.72ms, mfu 0.11%\n",
            "iter 690: loss 2.3825, time 440.14ms, mfu 0.11%\n",
            "iter 700: loss 2.3731, time 438.52ms, mfu 0.11%\n",
            "iter 710: loss 2.4399, time 442.39ms, mfu 0.11%\n",
            "iter 720: loss 2.3356, time 437.46ms, mfu 0.11%\n",
            "iter 730: loss 2.3918, time 447.96ms, mfu 0.11%\n",
            "iter 740: loss 2.3714, time 449.33ms, mfu 0.11%\n",
            "iter 750: loss 2.2802, time 444.39ms, mfu 0.11%\n",
            "iter 760: loss 2.4177, time 443.68ms, mfu 0.11%\n",
            "iter 770: loss 2.3995, time 437.06ms, mfu 0.11%\n",
            "iter 780: loss 2.3015, time 443.36ms, mfu 0.11%\n",
            "iter 790: loss 2.3345, time 439.14ms, mfu 0.11%\n",
            "step 800: train loss 2.2380, val loss 2.2574\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 800: loss 2.3200, time 2147.85ms, mfu 0.10%\n",
            "iter 810: loss 2.2797, time 439.40ms, mfu 0.11%\n",
            "iter 820: loss 2.3547, time 443.75ms, mfu 0.11%\n",
            "iter 830: loss 2.2375, time 447.84ms, mfu 0.11%\n",
            "iter 840: loss 2.3065, time 443.71ms, mfu 0.11%\n",
            "iter 850: loss 2.2784, time 439.96ms, mfu 0.11%\n",
            "iter 860: loss 2.1706, time 443.08ms, mfu 0.11%\n",
            "iter 870: loss 2.3055, time 447.79ms, mfu 0.11%\n",
            "iter 880: loss 2.1824, time 440.86ms, mfu 0.11%\n",
            "iter 890: loss 2.3120, time 445.20ms, mfu 0.11%\n",
            "iter 900: loss 2.1978, time 438.93ms, mfu 0.11%\n",
            "iter 910: loss 2.1866, time 443.75ms, mfu 0.11%\n",
            "iter 920: loss 2.2071, time 435.42ms, mfu 0.11%\n",
            "iter 930: loss 2.3091, time 433.13ms, mfu 0.11%\n",
            "iter 940: loss 2.2236, time 452.32ms, mfu 0.11%\n",
            "iter 950: loss 2.2636, time 446.64ms, mfu 0.11%\n",
            "iter 960: loss 2.2511, time 444.65ms, mfu 0.11%\n",
            "iter 970: loss 2.1551, time 442.96ms, mfu 0.11%\n",
            "iter 980: loss 2.1262, time 441.38ms, mfu 0.11%\n",
            "iter 990: loss 2.1320, time 440.26ms, mfu 0.11%\n",
            "step 1000: train loss 2.0774, val loss 2.1062\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1000: loss 2.1723, time 2137.23ms, mfu 0.10%\n",
            "iter 1010: loss 2.1119, time 434.24ms, mfu 0.11%\n",
            "iter 1020: loss 2.1179, time 444.33ms, mfu 0.11%\n",
            "iter 1030: loss 2.1385, time 435.26ms, mfu 0.11%\n",
            "iter 1040: loss 2.0836, time 442.11ms, mfu 0.11%\n",
            "iter 1050: loss 2.2371, time 444.64ms, mfu 0.11%\n",
            "iter 1060: loss 1.9603, time 441.10ms, mfu 0.11%\n",
            "iter 1070: loss 2.2320, time 445.93ms, mfu 0.11%\n",
            "iter 1080: loss 2.1834, time 446.42ms, mfu 0.11%\n",
            "iter 1090: loss 2.0829, time 436.85ms, mfu 0.11%\n",
            "iter 1100: loss 2.0550, time 435.15ms, mfu 0.11%\n",
            "iter 1110: loss 2.0560, time 444.06ms, mfu 0.11%\n",
            "iter 1120: loss 2.0429, time 446.95ms, mfu 0.11%\n",
            "iter 1130: loss 2.0045, time 441.19ms, mfu 0.11%\n",
            "iter 1140: loss 2.0881, time 437.54ms, mfu 0.11%\n",
            "iter 1150: loss 2.0401, time 439.36ms, mfu 0.11%\n",
            "iter 1160: loss 2.1309, time 433.03ms, mfu 0.11%\n",
            "iter 1170: loss 2.0869, time 435.18ms, mfu 0.11%\n",
            "iter 1180: loss 2.0276, time 450.03ms, mfu 0.11%\n",
            "iter 1190: loss 2.0320, time 437.99ms, mfu 0.11%\n",
            "step 1200: train loss 1.9226, val loss 2.0010\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1200: loss 2.0041, time 2125.19ms, mfu 0.10%\n",
            "iter 1210: loss 1.9672, time 435.49ms, mfu 0.11%\n",
            "iter 1220: loss 2.0024, time 436.82ms, mfu 0.11%\n",
            "iter 1230: loss 2.0603, time 440.70ms, mfu 0.11%\n",
            "iter 1240: loss 1.8908, time 448.16ms, mfu 0.11%\n",
            "iter 1250: loss 1.9525, time 446.86ms, mfu 0.11%\n",
            "iter 1260: loss 1.9546, time 447.48ms, mfu 0.11%\n",
            "iter 1270: loss 1.9279, time 445.13ms, mfu 0.11%\n",
            "iter 1280: loss 2.0356, time 445.30ms, mfu 0.11%\n",
            "iter 1290: loss 2.0469, time 446.84ms, mfu 0.11%\n",
            "iter 1300: loss 1.9686, time 439.80ms, mfu 0.11%\n",
            "iter 1310: loss 2.0217, time 436.39ms, mfu 0.11%\n",
            "iter 1320: loss 1.9922, time 445.13ms, mfu 0.11%\n",
            "iter 1330: loss 1.8776, time 435.38ms, mfu 0.11%\n",
            "iter 1340: loss 1.9340, time 444.76ms, mfu 0.11%\n",
            "iter 1350: loss 1.9786, time 439.66ms, mfu 0.11%\n",
            "iter 1360: loss 1.8131, time 445.39ms, mfu 0.11%\n",
            "iter 1370: loss 1.9776, time 439.63ms, mfu 0.11%\n",
            "iter 1380: loss 1.9939, time 445.91ms, mfu 0.11%\n",
            "iter 1390: loss 1.8737, time 439.43ms, mfu 0.11%\n",
            "step 1400: train loss 1.8023, val loss 1.9224\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1400: loss 1.9174, time 2149.60ms, mfu 0.10%\n",
            "iter 1410: loss 2.0350, time 441.82ms, mfu 0.11%\n",
            "iter 1420: loss 1.7880, time 449.50ms, mfu 0.11%\n",
            "iter 1430: loss 1.9474, time 442.82ms, mfu 0.11%\n",
            "iter 1440: loss 2.1217, time 444.25ms, mfu 0.11%\n",
            "iter 1450: loss 1.8259, time 450.53ms, mfu 0.11%\n",
            "iter 1460: loss 1.9138, time 448.90ms, mfu 0.11%\n",
            "iter 1470: loss 1.8575, time 448.39ms, mfu 0.11%\n",
            "iter 1480: loss 1.6823, time 455.43ms, mfu 0.11%\n",
            "iter 1490: loss 1.7853, time 448.09ms, mfu 0.11%\n",
            "iter 1500: loss 1.7608, time 444.41ms, mfu 0.11%\n",
            "iter 1510: loss 1.8828, time 445.34ms, mfu 0.11%\n",
            "iter 1520: loss 1.7877, time 439.31ms, mfu 0.11%\n",
            "iter 1530: loss 1.8346, time 444.72ms, mfu 0.11%\n",
            "iter 1540: loss 1.7535, time 436.29ms, mfu 0.11%\n",
            "iter 1550: loss 1.8768, time 438.12ms, mfu 0.11%\n",
            "iter 1560: loss 1.8451, time 439.41ms, mfu 0.11%\n",
            "iter 1570: loss 1.7778, time 433.18ms, mfu 0.11%\n",
            "iter 1580: loss 1.8208, time 441.66ms, mfu 0.11%\n",
            "iter 1590: loss 1.9542, time 437.26ms, mfu 0.11%\n",
            "step 1600: train loss 1.7013, val loss 1.8554\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1600: loss 1.7860, time 2136.72ms, mfu 0.10%\n",
            "iter 1610: loss 1.6323, time 438.04ms, mfu 0.11%\n",
            "iter 1620: loss 1.8121, time 439.44ms, mfu 0.11%\n",
            "iter 1630: loss 1.8395, time 438.26ms, mfu 0.11%\n",
            "iter 1640: loss 1.7528, time 437.94ms, mfu 0.11%\n",
            "iter 1650: loss 1.7005, time 447.55ms, mfu 0.11%\n",
            "iter 1660: loss 1.7081, time 452.26ms, mfu 0.11%\n",
            "iter 1670: loss 1.7067, time 443.32ms, mfu 0.11%\n",
            "iter 1680: loss 1.8586, time 442.88ms, mfu 0.11%\n",
            "iter 1690: loss 1.5701, time 447.52ms, mfu 0.11%\n",
            "iter 1700: loss 1.6517, time 440.19ms, mfu 0.11%\n",
            "iter 1710: loss 1.8212, time 442.09ms, mfu 0.11%\n",
            "iter 1720: loss 1.9210, time 441.03ms, mfu 0.11%\n",
            "iter 1730: loss 1.7755, time 442.67ms, mfu 0.11%\n",
            "iter 1740: loss 1.7927, time 440.60ms, mfu 0.11%\n",
            "iter 1750: loss 1.8214, time 442.42ms, mfu 0.11%\n",
            "iter 1760: loss 1.5769, time 440.42ms, mfu 0.11%\n",
            "iter 1770: loss 1.8173, time 443.78ms, mfu 0.11%\n",
            "iter 1780: loss 1.7567, time 434.79ms, mfu 0.11%\n",
            "iter 1790: loss 1.7806, time 440.58ms, mfu 0.11%\n",
            "step 1800: train loss 1.6288, val loss 1.7891\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "iter 1800: loss 1.8174, time 2150.10ms, mfu 0.10%\n",
            "iter 1810: loss 1.6123, time 434.86ms, mfu 0.11%\n",
            "iter 1820: loss 1.7457, time 448.81ms, mfu 0.11%\n",
            "iter 1830: loss 1.8651, time 442.57ms, mfu 0.11%\n",
            "iter 1840: loss 1.7108, time 441.78ms, mfu 0.11%\n",
            "iter 1850: loss 1.8256, time 437.95ms, mfu 0.11%\n",
            "iter 1860: loss 1.7348, time 443.74ms, mfu 0.11%\n",
            "iter 1870: loss 1.6984, time 443.76ms, mfu 0.11%\n",
            "iter 1880: loss 1.6560, time 438.95ms, mfu 0.11%\n",
            "iter 1890: loss 1.6895, time 459.70ms, mfu 0.11%\n",
            "iter 1900: loss 1.7292, time 442.92ms, mfu 0.11%\n",
            "iter 1910: loss 1.6124, time 449.97ms, mfu 0.11%\n",
            "iter 1920: loss 1.7674, time 449.40ms, mfu 0.11%\n",
            "iter 1930: loss 1.7207, time 445.97ms, mfu 0.11%\n",
            "iter 1940: loss 1.7509, time 443.05ms, mfu 0.11%\n",
            "iter 1950: loss 1.6472, time 442.68ms, mfu 0.11%\n",
            "iter 1960: loss 1.6262, time 456.76ms, mfu 0.11%\n",
            "iter 1970: loss 1.7412, time 440.39ms, mfu 0.11%\n",
            "iter 1980: loss 1.6717, time 442.36ms, mfu 0.11%\n",
            "iter 1990: loss 1.7736, time 446.93ms, mfu 0.11%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 20/32: b64_L6_H8_E128_BS8_MI2000_D20_s20 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS8_MI2000_D20_s20.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 20\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1872, val loss 4.1823\n",
            "iter 0: loss 4.2109, time 2430.66ms, mfu -100.00%\n",
            "iter 10: loss 4.1878, time 437.78ms, mfu 0.12%\n",
            "iter 20: loss 4.1413, time 439.41ms, mfu 0.12%\n",
            "iter 30: loss 4.0663, time 439.60ms, mfu 0.12%\n",
            "iter 40: loss 3.9897, time 430.60ms, mfu 0.12%\n",
            "iter 50: loss 3.8888, time 434.56ms, mfu 0.12%\n",
            "iter 60: loss 3.8766, time 432.70ms, mfu 0.12%\n",
            "iter 70: loss 3.7336, time 429.76ms, mfu 0.12%\n",
            "iter 80: loss 3.6844, time 443.42ms, mfu 0.12%\n",
            "iter 90: loss 3.6405, time 436.12ms, mfu 0.12%\n",
            "iter 100: loss 3.6261, time 434.80ms, mfu 0.12%\n",
            "iter 110: loss 3.5604, time 447.28ms, mfu 0.12%\n",
            "iter 120: loss 3.5425, time 434.73ms, mfu 0.12%\n",
            "iter 130: loss 3.4968, time 431.95ms, mfu 0.12%\n",
            "iter 140: loss 3.4829, time 444.13ms, mfu 0.12%\n",
            "iter 150: loss 3.4208, time 450.05ms, mfu 0.12%\n",
            "iter 160: loss 3.4242, time 436.35ms, mfu 0.12%\n",
            "iter 170: loss 3.4016, time 437.94ms, mfu 0.12%\n",
            "iter 180: loss 3.3275, time 436.02ms, mfu 0.12%\n",
            "iter 190: loss 3.2778, time 433.37ms, mfu 0.12%\n",
            "step 200: train loss 3.2216, val loss 3.2272\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 200: loss 3.2269, time 2120.55ms, mfu 0.11%\n",
            "iter 210: loss 3.2166, time 429.53ms, mfu 0.11%\n",
            "iter 220: loss 3.2630, time 440.88ms, mfu 0.11%\n",
            "iter 230: loss 3.1655, time 437.53ms, mfu 0.11%\n",
            "iter 240: loss 3.1708, time 437.96ms, mfu 0.11%\n",
            "iter 250: loss 3.1443, time 439.76ms, mfu 0.11%\n",
            "iter 260: loss 3.1080, time 432.82ms, mfu 0.11%\n",
            "iter 270: loss 3.0955, time 432.57ms, mfu 0.11%\n",
            "iter 280: loss 3.0815, time 434.91ms, mfu 0.11%\n",
            "iter 290: loss 3.0139, time 427.34ms, mfu 0.11%\n",
            "iter 300: loss 2.9879, time 442.13ms, mfu 0.11%\n",
            "iter 310: loss 3.0114, time 441.87ms, mfu 0.11%\n",
            "iter 320: loss 3.0012, time 441.72ms, mfu 0.11%\n",
            "iter 330: loss 2.9617, time 439.14ms, mfu 0.11%\n",
            "iter 340: loss 2.9522, time 432.32ms, mfu 0.11%\n",
            "iter 350: loss 2.8785, time 437.81ms, mfu 0.11%\n",
            "iter 360: loss 2.9029, time 431.10ms, mfu 0.11%\n",
            "iter 370: loss 2.9186, time 431.25ms, mfu 0.11%\n",
            "iter 380: loss 2.8345, time 431.08ms, mfu 0.12%\n",
            "iter 390: loss 2.8020, time 434.86ms, mfu 0.12%\n",
            "step 400: train loss 2.7476, val loss 2.7613\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 400: loss 2.7929, time 2091.40ms, mfu 0.11%\n",
            "iter 410: loss 2.7362, time 445.51ms, mfu 0.11%\n",
            "iter 420: loss 2.7805, time 434.39ms, mfu 0.11%\n",
            "iter 430: loss 2.7293, time 435.14ms, mfu 0.11%\n",
            "iter 440: loss 2.7388, time 435.48ms, mfu 0.11%\n",
            "iter 450: loss 2.6832, time 434.39ms, mfu 0.11%\n",
            "iter 460: loss 2.6284, time 431.46ms, mfu 0.11%\n",
            "iter 470: loss 2.6833, time 439.32ms, mfu 0.11%\n",
            "iter 480: loss 2.6394, time 431.78ms, mfu 0.11%\n",
            "iter 490: loss 2.5629, time 432.21ms, mfu 0.11%\n",
            "iter 500: loss 2.6381, time 433.53ms, mfu 0.11%\n",
            "iter 510: loss 2.5179, time 440.13ms, mfu 0.11%\n",
            "iter 520: loss 2.6424, time 432.03ms, mfu 0.11%\n",
            "iter 530: loss 2.5482, time 430.23ms, mfu 0.11%\n",
            "iter 540: loss 2.6333, time 431.02ms, mfu 0.11%\n",
            "iter 550: loss 2.5667, time 445.99ms, mfu 0.11%\n",
            "iter 560: loss 2.6433, time 433.96ms, mfu 0.11%\n",
            "iter 570: loss 2.5314, time 430.70ms, mfu 0.11%\n",
            "iter 580: loss 2.5007, time 428.42ms, mfu 0.12%\n",
            "iter 590: loss 2.5646, time 440.36ms, mfu 0.12%\n",
            "step 600: train loss 2.4497, val loss 2.4512\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 600: loss 2.5488, time 2114.72ms, mfu 0.11%\n",
            "iter 610: loss 2.4178, time 442.32ms, mfu 0.11%\n",
            "iter 620: loss 2.4673, time 430.00ms, mfu 0.11%\n",
            "iter 630: loss 2.4939, time 430.98ms, mfu 0.11%\n",
            "iter 640: loss 2.5065, time 428.64ms, mfu 0.11%\n",
            "iter 650: loss 2.5242, time 426.47ms, mfu 0.11%\n",
            "iter 660: loss 2.5186, time 436.58ms, mfu 0.11%\n",
            "iter 670: loss 2.4102, time 436.73ms, mfu 0.11%\n",
            "iter 680: loss 2.4669, time 440.84ms, mfu 0.11%\n",
            "iter 690: loss 2.4366, time 461.06ms, mfu 0.11%\n",
            "iter 700: loss 2.4190, time 445.71ms, mfu 0.11%\n",
            "iter 710: loss 2.4689, time 445.20ms, mfu 0.11%\n",
            "iter 720: loss 2.3916, time 440.87ms, mfu 0.11%\n",
            "iter 730: loss 2.4086, time 431.51ms, mfu 0.11%\n",
            "iter 740: loss 2.4142, time 446.02ms, mfu 0.11%\n",
            "iter 750: loss 2.3349, time 437.96ms, mfu 0.11%\n",
            "iter 760: loss 2.4779, time 435.47ms, mfu 0.11%\n",
            "iter 770: loss 2.4549, time 430.51ms, mfu 0.11%\n",
            "iter 780: loss 2.3054, time 434.41ms, mfu 0.11%\n",
            "iter 790: loss 2.3748, time 433.62ms, mfu 0.11%\n",
            "step 800: train loss 2.2909, val loss 2.3035\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 800: loss 2.3709, time 2089.97ms, mfu 0.11%\n",
            "iter 810: loss 2.3446, time 431.33ms, mfu 0.11%\n",
            "iter 820: loss 2.4146, time 451.41ms, mfu 0.11%\n",
            "iter 830: loss 2.2695, time 435.50ms, mfu 0.11%\n",
            "iter 840: loss 2.3897, time 430.94ms, mfu 0.11%\n",
            "iter 850: loss 2.3505, time 433.37ms, mfu 0.11%\n",
            "iter 860: loss 2.2436, time 442.28ms, mfu 0.11%\n",
            "iter 870: loss 2.3992, time 456.05ms, mfu 0.11%\n",
            "iter 880: loss 2.2604, time 445.80ms, mfu 0.11%\n",
            "iter 890: loss 2.3818, time 432.48ms, mfu 0.11%\n",
            "iter 900: loss 2.2766, time 439.16ms, mfu 0.11%\n",
            "iter 910: loss 2.2598, time 430.63ms, mfu 0.11%\n",
            "iter 920: loss 2.3075, time 431.05ms, mfu 0.11%\n",
            "iter 930: loss 2.3892, time 439.22ms, mfu 0.11%\n",
            "iter 940: loss 2.3138, time 435.41ms, mfu 0.11%\n",
            "iter 950: loss 2.3408, time 436.41ms, mfu 0.11%\n",
            "iter 960: loss 2.3213, time 435.83ms, mfu 0.11%\n",
            "iter 970: loss 2.2661, time 434.49ms, mfu 0.11%\n",
            "iter 980: loss 2.2468, time 442.01ms, mfu 0.11%\n",
            "iter 990: loss 2.2040, time 433.61ms, mfu 0.11%\n",
            "step 1000: train loss 2.1584, val loss 2.1735\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1000: loss 2.2347, time 2100.27ms, mfu 0.11%\n",
            "iter 1010: loss 2.2038, time 432.10ms, mfu 0.11%\n",
            "iter 1020: loss 2.2097, time 434.49ms, mfu 0.11%\n",
            "iter 1030: loss 2.2244, time 433.20ms, mfu 0.11%\n",
            "iter 1040: loss 2.2046, time 438.40ms, mfu 0.11%\n",
            "iter 1050: loss 2.3108, time 436.69ms, mfu 0.11%\n",
            "iter 1060: loss 2.0581, time 440.97ms, mfu 0.11%\n",
            "iter 1070: loss 2.3040, time 437.00ms, mfu 0.11%\n",
            "iter 1080: loss 2.2841, time 437.97ms, mfu 0.11%\n",
            "iter 1090: loss 2.1716, time 439.18ms, mfu 0.11%\n",
            "iter 1100: loss 2.1568, time 436.83ms, mfu 0.11%\n",
            "iter 1110: loss 2.1334, time 434.69ms, mfu 0.11%\n",
            "iter 1120: loss 2.1605, time 441.36ms, mfu 0.11%\n",
            "iter 1130: loss 2.1178, time 435.49ms, mfu 0.11%\n",
            "iter 1140: loss 2.1929, time 433.07ms, mfu 0.11%\n",
            "iter 1150: loss 2.1619, time 429.70ms, mfu 0.11%\n",
            "iter 1160: loss 2.2272, time 429.29ms, mfu 0.11%\n",
            "iter 1170: loss 2.1866, time 439.45ms, mfu 0.11%\n",
            "iter 1180: loss 2.1300, time 441.46ms, mfu 0.11%\n",
            "iter 1190: loss 2.1673, time 437.30ms, mfu 0.11%\n",
            "step 1200: train loss 2.0164, val loss 2.0753\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1200: loss 2.0937, time 2126.55ms, mfu 0.11%\n",
            "iter 1210: loss 2.0550, time 431.97ms, mfu 0.11%\n",
            "iter 1220: loss 2.1315, time 438.56ms, mfu 0.11%\n",
            "iter 1230: loss 2.1548, time 436.17ms, mfu 0.11%\n",
            "iter 1240: loss 1.9865, time 439.81ms, mfu 0.11%\n",
            "iter 1250: loss 2.0763, time 439.82ms, mfu 0.11%\n",
            "iter 1260: loss 2.0654, time 432.59ms, mfu 0.11%\n",
            "iter 1270: loss 2.0672, time 431.59ms, mfu 0.11%\n",
            "iter 1280: loss 2.1462, time 441.45ms, mfu 0.11%\n",
            "iter 1290: loss 2.1138, time 425.81ms, mfu 0.11%\n",
            "iter 1300: loss 2.0716, time 434.87ms, mfu 0.11%\n",
            "iter 1310: loss 2.1241, time 439.26ms, mfu 0.11%\n",
            "iter 1320: loss 2.0794, time 438.40ms, mfu 0.11%\n",
            "iter 1330: loss 1.9897, time 430.90ms, mfu 0.11%\n",
            "iter 1340: loss 2.0727, time 439.01ms, mfu 0.11%\n",
            "iter 1350: loss 2.0574, time 432.78ms, mfu 0.11%\n",
            "iter 1360: loss 1.9441, time 440.34ms, mfu 0.11%\n",
            "iter 1370: loss 2.0771, time 435.55ms, mfu 0.11%\n",
            "iter 1380: loss 2.1382, time 434.58ms, mfu 0.11%\n",
            "iter 1390: loss 1.9982, time 442.56ms, mfu 0.11%\n",
            "step 1400: train loss 1.9063, val loss 1.9929\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1400: loss 2.0884, time 2088.95ms, mfu 0.11%\n",
            "iter 1410: loss 2.1525, time 430.18ms, mfu 0.11%\n",
            "iter 1420: loss 1.9118, time 443.50ms, mfu 0.11%\n",
            "iter 1430: loss 2.0292, time 444.25ms, mfu 0.11%\n",
            "iter 1440: loss 2.2397, time 445.97ms, mfu 0.11%\n",
            "iter 1450: loss 1.9548, time 431.23ms, mfu 0.11%\n",
            "iter 1460: loss 2.0640, time 436.63ms, mfu 0.11%\n",
            "iter 1470: loss 1.9829, time 435.25ms, mfu 0.11%\n",
            "iter 1480: loss 1.8104, time 439.12ms, mfu 0.11%\n",
            "iter 1490: loss 1.9140, time 435.65ms, mfu 0.11%\n",
            "iter 1500: loss 1.8828, time 442.27ms, mfu 0.11%\n",
            "iter 1510: loss 2.0134, time 441.03ms, mfu 0.11%\n",
            "iter 1520: loss 1.9056, time 442.57ms, mfu 0.11%\n",
            "iter 1530: loss 1.9107, time 443.15ms, mfu 0.11%\n",
            "iter 1540: loss 1.8598, time 442.22ms, mfu 0.11%\n",
            "iter 1550: loss 1.9692, time 446.01ms, mfu 0.11%\n",
            "iter 1560: loss 1.9502, time 444.42ms, mfu 0.11%\n",
            "iter 1570: loss 1.9191, time 434.36ms, mfu 0.11%\n",
            "iter 1580: loss 1.9386, time 435.16ms, mfu 0.11%\n",
            "iter 1590: loss 2.0122, time 432.88ms, mfu 0.11%\n",
            "step 1600: train loss 1.8104, val loss 1.9374\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1600: loss 1.9473, time 2078.80ms, mfu 0.11%\n",
            "iter 1610: loss 1.8005, time 437.19ms, mfu 0.11%\n",
            "iter 1620: loss 1.9127, time 434.42ms, mfu 0.11%\n",
            "iter 1630: loss 1.9094, time 445.92ms, mfu 0.11%\n",
            "iter 1640: loss 1.8973, time 436.32ms, mfu 0.11%\n",
            "iter 1650: loss 1.8343, time 439.09ms, mfu 0.11%\n",
            "iter 1660: loss 1.8071, time 454.86ms, mfu 0.11%\n",
            "iter 1670: loss 1.8380, time 434.77ms, mfu 0.11%\n",
            "iter 1680: loss 1.9618, time 437.11ms, mfu 0.11%\n",
            "iter 1690: loss 1.7540, time 433.67ms, mfu 0.11%\n",
            "iter 1700: loss 1.8078, time 430.38ms, mfu 0.11%\n",
            "iter 1710: loss 1.9410, time 429.63ms, mfu 0.11%\n",
            "iter 1720: loss 2.0317, time 434.11ms, mfu 0.11%\n",
            "iter 1730: loss 1.8727, time 433.72ms, mfu 0.11%\n",
            "iter 1740: loss 1.8893, time 447.59ms, mfu 0.11%\n",
            "iter 1750: loss 1.9632, time 430.16ms, mfu 0.11%\n",
            "iter 1760: loss 1.7096, time 427.29ms, mfu 0.11%\n",
            "iter 1770: loss 1.9130, time 455.46ms, mfu 0.11%\n",
            "iter 1780: loss 1.8497, time 439.85ms, mfu 0.11%\n",
            "iter 1790: loss 1.9320, time 437.48ms, mfu 0.11%\n",
            "step 1800: train loss 1.7327, val loss 1.8687\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "iter 1800: loss 1.9279, time 2106.68ms, mfu 0.11%\n",
            "iter 1810: loss 1.7555, time 444.78ms, mfu 0.11%\n",
            "iter 1820: loss 1.8836, time 448.46ms, mfu 0.11%\n",
            "iter 1830: loss 1.9827, time 442.99ms, mfu 0.11%\n",
            "iter 1840: loss 1.8172, time 439.23ms, mfu 0.11%\n",
            "iter 1850: loss 1.9153, time 448.29ms, mfu 0.11%\n",
            "iter 1860: loss 1.8524, time 441.19ms, mfu 0.11%\n",
            "iter 1870: loss 1.8277, time 438.91ms, mfu 0.11%\n",
            "iter 1880: loss 1.8153, time 440.62ms, mfu 0.11%\n",
            "iter 1890: loss 1.8442, time 439.00ms, mfu 0.11%\n",
            "iter 1900: loss 1.8685, time 443.06ms, mfu 0.11%\n",
            "iter 1910: loss 1.7711, time 436.74ms, mfu 0.11%\n",
            "iter 1920: loss 1.8871, time 432.09ms, mfu 0.11%\n",
            "iter 1930: loss 1.8249, time 449.91ms, mfu 0.11%\n",
            "iter 1940: loss 1.8085, time 437.32ms, mfu 0.11%\n",
            "iter 1950: loss 1.7868, time 435.10ms, mfu 0.11%\n",
            "iter 1960: loss 1.7226, time 438.81ms, mfu 0.11%\n",
            "iter 1970: loss 1.8832, time 443.25ms, mfu 0.11%\n",
            "iter 1980: loss 1.8348, time 438.96ms, mfu 0.11%\n",
            "iter 1990: loss 1.8983, time 434.27ms, mfu 0.11%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 21/32: b64_L6_H8_E128_BS16_MI1000_D10_s21 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS16_MI1000_D10_s21.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 21\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1868, val loss 4.1814\n",
            "iter 0: loss 4.1882, time 2595.55ms, mfu -100.00%\n",
            "iter 10: loss 4.1683, time 468.05ms, mfu 0.22%\n",
            "iter 20: loss 4.1164, time 456.70ms, mfu 0.22%\n",
            "iter 30: loss 4.0598, time 460.21ms, mfu 0.22%\n",
            "iter 40: loss 3.9439, time 462.17ms, mfu 0.22%\n",
            "iter 50: loss 3.8541, time 461.51ms, mfu 0.22%\n",
            "iter 60: loss 3.7791, time 463.38ms, mfu 0.22%\n",
            "iter 70: loss 3.7098, time 463.10ms, mfu 0.22%\n",
            "iter 80: loss 3.6800, time 466.79ms, mfu 0.22%\n",
            "iter 90: loss 3.6576, time 467.39ms, mfu 0.22%\n",
            "iter 100: loss 3.5746, time 459.63ms, mfu 0.22%\n",
            "iter 110: loss 3.5173, time 463.48ms, mfu 0.22%\n",
            "iter 120: loss 3.4817, time 461.08ms, mfu 0.22%\n",
            "iter 130: loss 3.4698, time 464.57ms, mfu 0.22%\n",
            "iter 140: loss 3.4492, time 454.58ms, mfu 0.22%\n",
            "iter 150: loss 3.3332, time 456.38ms, mfu 0.22%\n",
            "iter 160: loss 3.3305, time 460.31ms, mfu 0.22%\n",
            "iter 170: loss 3.3144, time 458.25ms, mfu 0.22%\n",
            "iter 180: loss 3.2517, time 458.87ms, mfu 0.22%\n",
            "iter 190: loss 3.2857, time 467.25ms, mfu 0.22%\n",
            "step 200: train loss 3.1997, val loss 3.2081\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 200: loss 3.2753, time 2291.69ms, mfu 0.20%\n",
            "iter 210: loss 3.2131, time 466.41ms, mfu 0.20%\n",
            "iter 220: loss 3.2104, time 463.09ms, mfu 0.21%\n",
            "iter 230: loss 3.1614, time 456.26ms, mfu 0.21%\n",
            "iter 240: loss 3.1481, time 458.37ms, mfu 0.21%\n",
            "iter 250: loss 3.1111, time 448.67ms, mfu 0.21%\n",
            "iter 260: loss 3.0471, time 461.27ms, mfu 0.21%\n",
            "iter 270: loss 3.0083, time 454.80ms, mfu 0.21%\n",
            "iter 280: loss 3.0446, time 460.93ms, mfu 0.21%\n",
            "iter 290: loss 2.9794, time 467.46ms, mfu 0.21%\n",
            "iter 300: loss 2.9241, time 464.07ms, mfu 0.21%\n",
            "iter 310: loss 2.9135, time 466.84ms, mfu 0.21%\n",
            "iter 320: loss 2.8937, time 456.86ms, mfu 0.22%\n",
            "iter 330: loss 2.8688, time 455.24ms, mfu 0.22%\n",
            "iter 340: loss 2.8500, time 455.92ms, mfu 0.22%\n",
            "iter 350: loss 2.8389, time 460.24ms, mfu 0.22%\n",
            "iter 360: loss 2.8551, time 459.10ms, mfu 0.22%\n",
            "iter 370: loss 2.8265, time 450.61ms, mfu 0.22%\n",
            "iter 380: loss 2.8224, time 449.29ms, mfu 0.22%\n",
            "iter 390: loss 2.7376, time 462.30ms, mfu 0.22%\n",
            "step 400: train loss 2.7038, val loss 2.7144\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 400: loss 2.7047, time 2259.54ms, mfu 0.20%\n",
            "iter 410: loss 2.7109, time 470.26ms, mfu 0.20%\n",
            "iter 420: loss 2.6800, time 459.55ms, mfu 0.20%\n",
            "iter 430: loss 2.6518, time 457.22ms, mfu 0.21%\n",
            "iter 440: loss 2.6966, time 474.87ms, mfu 0.21%\n",
            "iter 450: loss 2.5779, time 457.73ms, mfu 0.21%\n",
            "iter 460: loss 2.6080, time 458.52ms, mfu 0.21%\n",
            "iter 470: loss 2.5231, time 454.29ms, mfu 0.21%\n",
            "iter 480: loss 2.6460, time 458.33ms, mfu 0.21%\n",
            "iter 490: loss 2.5380, time 471.40ms, mfu 0.21%\n",
            "iter 500: loss 2.4926, time 463.49ms, mfu 0.21%\n",
            "iter 510: loss 2.5336, time 457.99ms, mfu 0.21%\n",
            "iter 520: loss 2.4734, time 464.80ms, mfu 0.21%\n",
            "iter 530: loss 2.4893, time 462.50ms, mfu 0.21%\n",
            "iter 540: loss 2.4857, time 464.31ms, mfu 0.22%\n",
            "iter 550: loss 2.5304, time 460.41ms, mfu 0.22%\n",
            "iter 560: loss 2.4918, time 461.40ms, mfu 0.22%\n",
            "iter 570: loss 2.4561, time 471.67ms, mfu 0.22%\n",
            "iter 580: loss 2.4738, time 463.57ms, mfu 0.22%\n",
            "iter 590: loss 2.4043, time 463.84ms, mfu 0.22%\n",
            "step 600: train loss 2.3945, val loss 2.3989\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 600: loss 2.4718, time 2246.13ms, mfu 0.20%\n",
            "iter 610: loss 2.4355, time 455.76ms, mfu 0.20%\n",
            "iter 620: loss 2.3255, time 464.33ms, mfu 0.20%\n",
            "iter 630: loss 2.4012, time 465.57ms, mfu 0.20%\n",
            "iter 640: loss 2.4790, time 464.33ms, mfu 0.21%\n",
            "iter 650: loss 2.4157, time 460.40ms, mfu 0.21%\n",
            "iter 660: loss 2.4347, time 464.03ms, mfu 0.21%\n",
            "iter 670: loss 2.3951, time 461.91ms, mfu 0.21%\n",
            "iter 680: loss 2.3738, time 461.47ms, mfu 0.21%\n",
            "iter 690: loss 2.3544, time 457.69ms, mfu 0.21%\n",
            "iter 700: loss 2.3446, time 454.77ms, mfu 0.21%\n",
            "iter 710: loss 2.2954, time 456.25ms, mfu 0.21%\n",
            "iter 720: loss 2.2953, time 457.39ms, mfu 0.21%\n",
            "iter 730: loss 2.4306, time 455.58ms, mfu 0.22%\n",
            "iter 740: loss 2.3094, time 456.78ms, mfu 0.22%\n",
            "iter 750: loss 2.2887, time 456.14ms, mfu 0.22%\n",
            "iter 760: loss 2.1773, time 454.89ms, mfu 0.22%\n",
            "iter 770: loss 2.2323, time 457.06ms, mfu 0.22%\n",
            "iter 780: loss 2.2213, time 453.70ms, mfu 0.22%\n",
            "iter 790: loss 2.3717, time 460.34ms, mfu 0.22%\n",
            "step 800: train loss 2.1866, val loss 2.2102\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 800: loss 2.1964, time 2276.66ms, mfu 0.20%\n",
            "iter 810: loss 2.2615, time 455.60ms, mfu 0.20%\n",
            "iter 820: loss 2.2958, time 460.15ms, mfu 0.21%\n",
            "iter 830: loss 2.2956, time 462.93ms, mfu 0.21%\n",
            "iter 840: loss 2.1740, time 451.04ms, mfu 0.21%\n",
            "iter 850: loss 2.2135, time 463.82ms, mfu 0.21%\n",
            "iter 860: loss 2.1490, time 457.83ms, mfu 0.21%\n",
            "iter 870: loss 2.1156, time 457.01ms, mfu 0.21%\n",
            "iter 880: loss 2.2287, time 469.68ms, mfu 0.21%\n",
            "iter 890: loss 2.2069, time 466.01ms, mfu 0.21%\n",
            "iter 900: loss 2.1927, time 469.93ms, mfu 0.21%\n",
            "iter 910: loss 2.0657, time 458.08ms, mfu 0.21%\n",
            "iter 920: loss 2.0552, time 459.23ms, mfu 0.21%\n",
            "iter 930: loss 2.0866, time 465.30ms, mfu 0.21%\n",
            "iter 940: loss 2.1024, time 464.81ms, mfu 0.22%\n",
            "iter 950: loss 2.1554, time 455.61ms, mfu 0.22%\n",
            "iter 960: loss 2.1236, time 456.01ms, mfu 0.22%\n",
            "iter 970: loss 2.0778, time 455.51ms, mfu 0.22%\n",
            "iter 980: loss 2.0874, time 460.40ms, mfu 0.22%\n",
            "iter 990: loss 2.1417, time 457.59ms, mfu 0.22%\n",
            "step 1000: train loss 1.9954, val loss 2.0543\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "iter 1000: loss 2.0415, time 2245.82ms, mfu 0.20%\n",
            "\n",
            "=== Experiment 22/32: b64_L6_H8_E128_BS16_MI1000_D20_s22 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS16_MI1000_D20_s22.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 22\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1868, val loss 4.1814\n",
            "iter 0: loss 4.1917, time 2554.04ms, mfu -100.00%\n",
            "iter 10: loss 4.1718, time 461.15ms, mfu 0.22%\n",
            "iter 20: loss 4.1365, time 458.16ms, mfu 0.22%\n",
            "iter 30: loss 4.0756, time 454.23ms, mfu 0.22%\n",
            "iter 40: loss 3.9721, time 452.35ms, mfu 0.22%\n",
            "iter 50: loss 3.8855, time 452.49ms, mfu 0.22%\n",
            "iter 60: loss 3.8055, time 453.82ms, mfu 0.22%\n",
            "iter 70: loss 3.7273, time 456.76ms, mfu 0.22%\n",
            "iter 80: loss 3.6979, time 456.79ms, mfu 0.22%\n",
            "iter 90: loss 3.6855, time 448.42ms, mfu 0.22%\n",
            "iter 100: loss 3.6077, time 457.78ms, mfu 0.22%\n",
            "iter 110: loss 3.5616, time 449.96ms, mfu 0.22%\n",
            "iter 120: loss 3.5211, time 452.54ms, mfu 0.22%\n",
            "iter 130: loss 3.5114, time 451.11ms, mfu 0.22%\n",
            "iter 140: loss 3.4863, time 456.62ms, mfu 0.22%\n",
            "iter 150: loss 3.3760, time 455.08ms, mfu 0.22%\n",
            "iter 160: loss 3.3630, time 443.24ms, mfu 0.22%\n",
            "iter 170: loss 3.3573, time 448.03ms, mfu 0.22%\n",
            "iter 180: loss 3.2882, time 447.54ms, mfu 0.22%\n",
            "iter 190: loss 3.3188, time 441.47ms, mfu 0.22%\n",
            "step 200: train loss 3.2162, val loss 3.2239\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 200: loss 3.3108, time 2230.25ms, mfu 0.21%\n",
            "iter 210: loss 3.2473, time 450.94ms, mfu 0.21%\n",
            "iter 220: loss 3.2398, time 450.65ms, mfu 0.21%\n",
            "iter 230: loss 3.1881, time 452.64ms, mfu 0.21%\n",
            "iter 240: loss 3.1844, time 452.83ms, mfu 0.21%\n",
            "iter 250: loss 3.1409, time 449.12ms, mfu 0.21%\n",
            "iter 260: loss 3.0795, time 451.20ms, mfu 0.22%\n",
            "iter 270: loss 3.0406, time 448.53ms, mfu 0.22%\n",
            "iter 280: loss 3.0763, time 460.41ms, mfu 0.22%\n",
            "iter 290: loss 3.0150, time 444.96ms, mfu 0.22%\n",
            "iter 300: loss 2.9542, time 447.04ms, mfu 0.22%\n",
            "iter 310: loss 2.9571, time 449.17ms, mfu 0.22%\n",
            "iter 320: loss 2.9318, time 451.25ms, mfu 0.22%\n",
            "iter 330: loss 2.8944, time 453.55ms, mfu 0.22%\n",
            "iter 340: loss 2.8912, time 450.06ms, mfu 0.22%\n",
            "iter 350: loss 2.8724, time 449.29ms, mfu 0.22%\n",
            "iter 360: loss 2.8914, time 456.49ms, mfu 0.22%\n",
            "iter 370: loss 2.8517, time 455.01ms, mfu 0.22%\n",
            "iter 380: loss 2.8584, time 455.88ms, mfu 0.22%\n",
            "iter 390: loss 2.7702, time 452.59ms, mfu 0.22%\n",
            "step 400: train loss 2.7319, val loss 2.7396\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 400: loss 2.7355, time 2229.98ms, mfu 0.20%\n",
            "iter 410: loss 2.7404, time 453.43ms, mfu 0.21%\n",
            "iter 420: loss 2.7236, time 447.27ms, mfu 0.21%\n",
            "iter 430: loss 2.6829, time 448.96ms, mfu 0.21%\n",
            "iter 440: loss 2.7358, time 450.74ms, mfu 0.21%\n",
            "iter 450: loss 2.6126, time 454.57ms, mfu 0.21%\n",
            "iter 460: loss 2.6439, time 445.03ms, mfu 0.21%\n",
            "iter 470: loss 2.5616, time 452.53ms, mfu 0.22%\n",
            "iter 480: loss 2.6786, time 455.75ms, mfu 0.22%\n",
            "iter 490: loss 2.5746, time 456.90ms, mfu 0.22%\n",
            "iter 500: loss 2.5377, time 448.10ms, mfu 0.22%\n",
            "iter 510: loss 2.5764, time 449.46ms, mfu 0.22%\n",
            "iter 520: loss 2.5204, time 446.99ms, mfu 0.22%\n",
            "iter 530: loss 2.5511, time 444.26ms, mfu 0.22%\n",
            "iter 540: loss 2.5332, time 443.51ms, mfu 0.22%\n",
            "iter 550: loss 2.5697, time 451.25ms, mfu 0.22%\n",
            "iter 560: loss 2.5330, time 448.59ms, mfu 0.22%\n",
            "iter 570: loss 2.4990, time 455.52ms, mfu 0.22%\n",
            "iter 580: loss 2.5106, time 449.37ms, mfu 0.22%\n",
            "iter 590: loss 2.4559, time 447.11ms, mfu 0.22%\n",
            "step 600: train loss 2.4286, val loss 2.4254\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 600: loss 2.4944, time 2215.05ms, mfu 0.21%\n",
            "iter 610: loss 2.4846, time 452.41ms, mfu 0.21%\n",
            "iter 620: loss 2.3696, time 453.99ms, mfu 0.21%\n",
            "iter 630: loss 2.4391, time 453.46ms, mfu 0.21%\n",
            "iter 640: loss 2.5161, time 460.27ms, mfu 0.21%\n",
            "iter 650: loss 2.4517, time 455.93ms, mfu 0.21%\n",
            "iter 660: loss 2.4644, time 445.75ms, mfu 0.21%\n",
            "iter 670: loss 2.4287, time 452.53ms, mfu 0.21%\n",
            "iter 680: loss 2.4128, time 451.08ms, mfu 0.22%\n",
            "iter 690: loss 2.3899, time 444.31ms, mfu 0.22%\n",
            "iter 700: loss 2.4005, time 455.94ms, mfu 0.22%\n",
            "iter 710: loss 2.3506, time 450.98ms, mfu 0.22%\n",
            "iter 720: loss 2.3753, time 451.63ms, mfu 0.22%\n",
            "iter 730: loss 2.4684, time 450.59ms, mfu 0.22%\n",
            "iter 740: loss 2.3700, time 449.41ms, mfu 0.22%\n",
            "iter 750: loss 2.3569, time 453.01ms, mfu 0.22%\n",
            "iter 760: loss 2.2469, time 455.72ms, mfu 0.22%\n",
            "iter 770: loss 2.3350, time 449.29ms, mfu 0.22%\n",
            "iter 780: loss 2.2744, time 457.11ms, mfu 0.22%\n",
            "iter 790: loss 2.4422, time 449.45ms, mfu 0.22%\n",
            "step 800: train loss 2.2458, val loss 2.2587\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 800: loss 2.2772, time 2211.75ms, mfu 0.20%\n",
            "iter 810: loss 2.3218, time 450.79ms, mfu 0.21%\n",
            "iter 820: loss 2.3559, time 449.52ms, mfu 0.21%\n",
            "iter 830: loss 2.3568, time 463.58ms, mfu 0.21%\n",
            "iter 840: loss 2.2693, time 456.03ms, mfu 0.21%\n",
            "iter 850: loss 2.2780, time 444.48ms, mfu 0.21%\n",
            "iter 860: loss 2.2237, time 455.08ms, mfu 0.21%\n",
            "iter 870: loss 2.2166, time 452.45ms, mfu 0.21%\n",
            "iter 880: loss 2.3085, time 453.42ms, mfu 0.22%\n",
            "iter 890: loss 2.2791, time 453.29ms, mfu 0.22%\n",
            "iter 900: loss 2.2541, time 452.50ms, mfu 0.22%\n",
            "iter 910: loss 2.1491, time 455.92ms, mfu 0.22%\n",
            "iter 920: loss 2.1347, time 451.36ms, mfu 0.22%\n",
            "iter 930: loss 2.1545, time 447.02ms, mfu 0.22%\n",
            "iter 940: loss 2.1858, time 454.19ms, mfu 0.22%\n",
            "iter 950: loss 2.2518, time 448.73ms, mfu 0.22%\n",
            "iter 960: loss 2.2073, time 448.96ms, mfu 0.22%\n",
            "iter 970: loss 2.1757, time 453.02ms, mfu 0.22%\n",
            "iter 980: loss 2.1720, time 449.86ms, mfu 0.22%\n",
            "iter 990: loss 2.2087, time 464.52ms, mfu 0.22%\n",
            "step 1000: train loss 2.0783, val loss 2.1178\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "iter 1000: loss 2.1404, time 2221.37ms, mfu 0.20%\n",
            "\n",
            "=== Experiment 23/32: b64_L6_H8_E128_BS16_MI2000_D10_s23 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS16_MI2000_D10_s23.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 23\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1868, val loss 4.1814\n",
            "iter 0: loss 4.1882, time 2583.74ms, mfu -100.00%\n",
            "iter 10: loss 4.1683, time 454.80ms, mfu 0.22%\n",
            "iter 20: loss 4.1164, time 453.30ms, mfu 0.22%\n",
            "iter 30: loss 4.0598, time 455.80ms, mfu 0.22%\n",
            "iter 40: loss 3.9439, time 451.47ms, mfu 0.22%\n",
            "iter 50: loss 3.8541, time 455.21ms, mfu 0.22%\n",
            "iter 60: loss 3.7791, time 459.94ms, mfu 0.22%\n",
            "iter 70: loss 3.7098, time 455.78ms, mfu 0.22%\n",
            "iter 80: loss 3.6800, time 457.52ms, mfu 0.22%\n",
            "iter 90: loss 3.6576, time 454.23ms, mfu 0.22%\n",
            "iter 100: loss 3.5746, time 453.38ms, mfu 0.22%\n",
            "iter 110: loss 3.5173, time 455.64ms, mfu 0.22%\n",
            "iter 120: loss 3.4817, time 452.03ms, mfu 0.22%\n",
            "iter 130: loss 3.4698, time 453.62ms, mfu 0.22%\n",
            "iter 140: loss 3.4492, time 461.32ms, mfu 0.22%\n",
            "iter 150: loss 3.3332, time 456.34ms, mfu 0.22%\n",
            "iter 160: loss 3.3305, time 456.01ms, mfu 0.22%\n",
            "iter 170: loss 3.3144, time 454.34ms, mfu 0.22%\n",
            "iter 180: loss 3.2517, time 445.87ms, mfu 0.22%\n",
            "iter 190: loss 3.2857, time 459.73ms, mfu 0.22%\n",
            "step 200: train loss 3.1997, val loss 3.2081\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 200: loss 3.2753, time 2252.92ms, mfu 0.21%\n",
            "iter 210: loss 3.2131, time 471.80ms, mfu 0.21%\n",
            "iter 220: loss 3.2104, time 454.30ms, mfu 0.21%\n",
            "iter 230: loss 3.1614, time 458.31ms, mfu 0.21%\n",
            "iter 240: loss 3.1481, time 454.91ms, mfu 0.21%\n",
            "iter 250: loss 3.1111, time 454.18ms, mfu 0.21%\n",
            "iter 260: loss 3.0471, time 456.85ms, mfu 0.21%\n",
            "iter 270: loss 3.0083, time 449.66ms, mfu 0.21%\n",
            "iter 280: loss 3.0446, time 453.49ms, mfu 0.22%\n",
            "iter 290: loss 2.9794, time 460.88ms, mfu 0.22%\n",
            "iter 300: loss 2.9241, time 456.23ms, mfu 0.22%\n",
            "iter 310: loss 2.9135, time 456.73ms, mfu 0.22%\n",
            "iter 320: loss 2.8937, time 461.84ms, mfu 0.22%\n",
            "iter 330: loss 2.8688, time 453.44ms, mfu 0.22%\n",
            "iter 340: loss 2.8500, time 471.80ms, mfu 0.22%\n",
            "iter 350: loss 2.8389, time 455.36ms, mfu 0.22%\n",
            "iter 360: loss 2.8551, time 451.43ms, mfu 0.22%\n",
            "iter 370: loss 2.8265, time 459.26ms, mfu 0.22%\n",
            "iter 380: loss 2.8224, time 452.52ms, mfu 0.22%\n",
            "iter 390: loss 2.7376, time 450.33ms, mfu 0.22%\n",
            "step 400: train loss 2.7038, val loss 2.7144\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 400: loss 2.7047, time 2247.64ms, mfu 0.20%\n",
            "iter 410: loss 2.7109, time 461.57ms, mfu 0.20%\n",
            "iter 420: loss 2.6800, time 457.82ms, mfu 0.21%\n",
            "iter 430: loss 2.6518, time 450.36ms, mfu 0.21%\n",
            "iter 440: loss 2.6966, time 456.16ms, mfu 0.21%\n",
            "iter 450: loss 2.5779, time 456.29ms, mfu 0.21%\n",
            "iter 460: loss 2.6080, time 451.74ms, mfu 0.21%\n",
            "iter 470: loss 2.5231, time 455.12ms, mfu 0.21%\n",
            "iter 480: loss 2.6460, time 458.90ms, mfu 0.21%\n",
            "iter 490: loss 2.5380, time 460.81ms, mfu 0.21%\n",
            "iter 500: loss 2.4926, time 463.42ms, mfu 0.22%\n",
            "iter 510: loss 2.5336, time 468.31ms, mfu 0.22%\n",
            "iter 520: loss 2.4734, time 465.66ms, mfu 0.22%\n",
            "iter 530: loss 2.4893, time 450.52ms, mfu 0.22%\n",
            "iter 540: loss 2.4857, time 453.66ms, mfu 0.22%\n",
            "iter 550: loss 2.5304, time 485.34ms, mfu 0.22%\n",
            "iter 560: loss 2.4918, time 455.03ms, mfu 0.22%\n",
            "iter 570: loss 2.4561, time 452.86ms, mfu 0.22%\n",
            "iter 580: loss 2.4738, time 460.80ms, mfu 0.22%\n",
            "iter 590: loss 2.4043, time 447.37ms, mfu 0.22%\n",
            "step 600: train loss 2.3945, val loss 2.3989\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 600: loss 2.4718, time 2232.50ms, mfu 0.20%\n",
            "iter 610: loss 2.4355, time 453.45ms, mfu 0.20%\n",
            "iter 620: loss 2.3255, time 458.89ms, mfu 0.21%\n",
            "iter 630: loss 2.4012, time 454.88ms, mfu 0.21%\n",
            "iter 640: loss 2.4790, time 450.76ms, mfu 0.21%\n",
            "iter 650: loss 2.4157, time 463.35ms, mfu 0.21%\n",
            "iter 660: loss 2.4347, time 459.58ms, mfu 0.21%\n",
            "iter 670: loss 2.3951, time 450.62ms, mfu 0.21%\n",
            "iter 680: loss 2.3738, time 453.94ms, mfu 0.21%\n",
            "iter 690: loss 2.3544, time 452.48ms, mfu 0.21%\n",
            "iter 700: loss 2.3446, time 454.50ms, mfu 0.22%\n",
            "iter 710: loss 2.2954, time 463.19ms, mfu 0.22%\n",
            "iter 720: loss 2.2953, time 457.76ms, mfu 0.22%\n",
            "iter 730: loss 2.4306, time 467.31ms, mfu 0.22%\n",
            "iter 740: loss 2.3094, time 454.59ms, mfu 0.22%\n",
            "iter 750: loss 2.2887, time 457.45ms, mfu 0.22%\n",
            "iter 760: loss 2.1773, time 460.97ms, mfu 0.22%\n",
            "iter 770: loss 2.2323, time 460.53ms, mfu 0.22%\n",
            "iter 780: loss 2.2213, time 449.69ms, mfu 0.22%\n",
            "iter 790: loss 2.3717, time 453.30ms, mfu 0.22%\n",
            "step 800: train loss 2.1866, val loss 2.2102\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 800: loss 2.1964, time 2250.87ms, mfu 0.20%\n",
            "iter 810: loss 2.2615, time 459.05ms, mfu 0.20%\n",
            "iter 820: loss 2.2958, time 451.57ms, mfu 0.21%\n",
            "iter 830: loss 2.2956, time 467.49ms, mfu 0.21%\n",
            "iter 840: loss 2.1740, time 447.91ms, mfu 0.21%\n",
            "iter 850: loss 2.2135, time 454.54ms, mfu 0.21%\n",
            "iter 860: loss 2.1490, time 478.79ms, mfu 0.21%\n",
            "iter 870: loss 2.1156, time 455.21ms, mfu 0.21%\n",
            "iter 880: loss 2.2287, time 461.64ms, mfu 0.21%\n",
            "iter 890: loss 2.2069, time 458.51ms, mfu 0.21%\n",
            "iter 900: loss 2.1927, time 460.75ms, mfu 0.21%\n",
            "iter 910: loss 2.0657, time 456.78ms, mfu 0.21%\n",
            "iter 920: loss 2.0552, time 458.74ms, mfu 0.22%\n",
            "iter 930: loss 2.0866, time 456.35ms, mfu 0.22%\n",
            "iter 940: loss 2.1024, time 464.57ms, mfu 0.22%\n",
            "iter 950: loss 2.1554, time 457.60ms, mfu 0.22%\n",
            "iter 960: loss 2.1236, time 455.87ms, mfu 0.22%\n",
            "iter 970: loss 2.0778, time 457.47ms, mfu 0.22%\n",
            "iter 980: loss 2.0874, time 454.07ms, mfu 0.22%\n",
            "iter 990: loss 2.1417, time 466.57ms, mfu 0.22%\n",
            "step 1000: train loss 1.9954, val loss 2.0543\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1000: loss 2.0415, time 2246.10ms, mfu 0.20%\n",
            "iter 1010: loss 2.1178, time 453.17ms, mfu 0.20%\n",
            "iter 1020: loss 2.0266, time 459.63ms, mfu 0.21%\n",
            "iter 1030: loss 2.0361, time 453.73ms, mfu 0.21%\n",
            "iter 1040: loss 2.0313, time 457.08ms, mfu 0.21%\n",
            "iter 1050: loss 2.0472, time 451.63ms, mfu 0.21%\n",
            "iter 1060: loss 2.0197, time 450.01ms, mfu 0.21%\n",
            "iter 1070: loss 2.0562, time 450.54ms, mfu 0.21%\n",
            "iter 1080: loss 2.0258, time 457.60ms, mfu 0.21%\n",
            "iter 1090: loss 2.0101, time 463.18ms, mfu 0.21%\n",
            "iter 1100: loss 2.0069, time 453.90ms, mfu 0.22%\n",
            "iter 1110: loss 1.9898, time 456.73ms, mfu 0.22%\n",
            "iter 1120: loss 2.0477, time 461.24ms, mfu 0.22%\n",
            "iter 1130: loss 1.9699, time 453.01ms, mfu 0.22%\n",
            "iter 1140: loss 1.9040, time 454.28ms, mfu 0.22%\n",
            "iter 1150: loss 2.0319, time 459.06ms, mfu 0.22%\n",
            "iter 1160: loss 2.0721, time 462.11ms, mfu 0.22%\n",
            "iter 1170: loss 1.9824, time 462.15ms, mfu 0.22%\n",
            "iter 1180: loss 2.0094, time 456.13ms, mfu 0.22%\n",
            "iter 1190: loss 1.9659, time 451.93ms, mfu 0.22%\n",
            "step 1200: train loss 1.8269, val loss 1.9413\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1200: loss 1.9757, time 2253.26ms, mfu 0.20%\n",
            "iter 1210: loss 1.9090, time 459.19ms, mfu 0.20%\n",
            "iter 1220: loss 1.9331, time 460.56ms, mfu 0.21%\n",
            "iter 1230: loss 1.9590, time 460.60ms, mfu 0.21%\n",
            "iter 1240: loss 1.8924, time 456.56ms, mfu 0.21%\n",
            "iter 1250: loss 1.9375, time 492.70ms, mfu 0.21%\n",
            "iter 1260: loss 1.8839, time 461.12ms, mfu 0.21%\n",
            "iter 1270: loss 1.9380, time 454.68ms, mfu 0.21%\n",
            "iter 1280: loss 1.9260, time 453.67ms, mfu 0.21%\n",
            "iter 1290: loss 1.8357, time 460.27ms, mfu 0.21%\n",
            "iter 1300: loss 1.8639, time 466.50ms, mfu 0.21%\n",
            "iter 1310: loss 1.8169, time 460.78ms, mfu 0.21%\n",
            "iter 1320: loss 1.7649, time 460.96ms, mfu 0.21%\n",
            "iter 1330: loss 1.8757, time 453.80ms, mfu 0.22%\n",
            "iter 1340: loss 1.7961, time 455.84ms, mfu 0.22%\n",
            "iter 1350: loss 1.9471, time 458.66ms, mfu 0.22%\n",
            "iter 1360: loss 1.7982, time 454.28ms, mfu 0.22%\n",
            "iter 1370: loss 1.8267, time 452.45ms, mfu 0.22%\n",
            "iter 1380: loss 1.8868, time 460.30ms, mfu 0.22%\n",
            "iter 1390: loss 1.7967, time 457.91ms, mfu 0.22%\n",
            "step 1400: train loss 1.7044, val loss 1.8509\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1400: loss 1.8057, time 2267.33ms, mfu 0.20%\n",
            "iter 1410: loss 1.8856, time 459.12ms, mfu 0.20%\n",
            "iter 1420: loss 1.8265, time 458.28ms, mfu 0.21%\n",
            "iter 1430: loss 1.8222, time 466.29ms, mfu 0.21%\n",
            "iter 1440: loss 1.8058, time 454.73ms, mfu 0.21%\n",
            "iter 1450: loss 1.9077, time 459.89ms, mfu 0.21%\n",
            "iter 1460: loss 1.7428, time 461.69ms, mfu 0.21%\n",
            "iter 1470: loss 1.7775, time 459.21ms, mfu 0.21%\n",
            "iter 1480: loss 1.7022, time 464.16ms, mfu 0.21%\n",
            "iter 1490: loss 1.8261, time 458.65ms, mfu 0.21%\n",
            "iter 1500: loss 1.8224, time 454.53ms, mfu 0.21%\n",
            "iter 1510: loss 1.7686, time 489.93ms, mfu 0.21%\n",
            "iter 1520: loss 1.6465, time 452.77ms, mfu 0.21%\n",
            "iter 1530: loss 1.7802, time 452.35ms, mfu 0.22%\n",
            "iter 1540: loss 1.7196, time 457.55ms, mfu 0.22%\n",
            "iter 1550: loss 1.7360, time 460.60ms, mfu 0.22%\n",
            "iter 1560: loss 1.7663, time 455.91ms, mfu 0.22%\n",
            "iter 1570: loss 1.7319, time 464.74ms, mfu 0.22%\n",
            "iter 1580: loss 1.7893, time 459.90ms, mfu 0.22%\n",
            "iter 1590: loss 1.7343, time 466.16ms, mfu 0.22%\n",
            "step 1600: train loss 1.6253, val loss 1.8010\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1600: loss 1.7682, time 2262.16ms, mfu 0.20%\n",
            "iter 1610: loss 1.7452, time 467.66ms, mfu 0.20%\n",
            "iter 1620: loss 1.7400, time 462.08ms, mfu 0.20%\n",
            "iter 1630: loss 1.6987, time 459.88ms, mfu 0.21%\n",
            "iter 1640: loss 1.5835, time 466.96ms, mfu 0.21%\n",
            "iter 1650: loss 1.6378, time 471.21ms, mfu 0.21%\n",
            "iter 1660: loss 1.7698, time 468.14ms, mfu 0.21%\n",
            "iter 1670: loss 1.7183, time 462.92ms, mfu 0.21%\n",
            "iter 1680: loss 1.7167, time 464.26ms, mfu 0.21%\n",
            "iter 1690: loss 1.6576, time 467.34ms, mfu 0.21%\n",
            "iter 1700: loss 1.6256, time 461.44ms, mfu 0.21%\n",
            "iter 1710: loss 1.5974, time 463.27ms, mfu 0.21%\n",
            "iter 1720: loss 1.5853, time 455.30ms, mfu 0.21%\n",
            "iter 1730: loss 1.6886, time 452.07ms, mfu 0.21%\n",
            "iter 1740: loss 1.5936, time 458.94ms, mfu 0.22%\n",
            "iter 1750: loss 1.7346, time 460.44ms, mfu 0.22%\n",
            "iter 1760: loss 1.8031, time 452.69ms, mfu 0.22%\n",
            "iter 1770: loss 1.6192, time 461.65ms, mfu 0.22%\n",
            "iter 1780: loss 1.7280, time 454.86ms, mfu 0.22%\n",
            "iter 1790: loss 1.6259, time 468.67ms, mfu 0.22%\n",
            "step 1800: train loss 1.5604, val loss 1.7390\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "iter 1800: loss 1.6161, time 2259.37ms, mfu 0.20%\n",
            "iter 1810: loss 1.6472, time 452.54ms, mfu 0.20%\n",
            "iter 1820: loss 1.5252, time 460.92ms, mfu 0.20%\n",
            "iter 1830: loss 1.6484, time 457.80ms, mfu 0.21%\n",
            "iter 1840: loss 1.7390, time 459.93ms, mfu 0.21%\n",
            "iter 1850: loss 1.7549, time 464.05ms, mfu 0.21%\n",
            "iter 1860: loss 1.6396, time 456.78ms, mfu 0.21%\n",
            "iter 1870: loss 1.7308, time 466.22ms, mfu 0.21%\n",
            "iter 1880: loss 1.6480, time 464.52ms, mfu 0.21%\n",
            "iter 1890: loss 1.6585, time 455.76ms, mfu 0.21%\n",
            "iter 1900: loss 1.5881, time 464.64ms, mfu 0.21%\n",
            "iter 1910: loss 1.5779, time 463.17ms, mfu 0.21%\n",
            "iter 1920: loss 1.5715, time 462.36ms, mfu 0.21%\n",
            "iter 1930: loss 1.6094, time 453.70ms, mfu 0.22%\n",
            "iter 1940: loss 1.6226, time 452.88ms, mfu 0.22%\n",
            "iter 1950: loss 1.6041, time 455.82ms, mfu 0.22%\n",
            "iter 1960: loss 1.6134, time 459.37ms, mfu 0.22%\n",
            "iter 1970: loss 1.5739, time 456.63ms, mfu 0.22%\n",
            "iter 1980: loss 1.6208, time 460.66ms, mfu 0.22%\n",
            "iter 1990: loss 1.5227, time 458.72ms, mfu 0.22%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 24/32: b64_L6_H8_E128_BS16_MI2000_D20_s24 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E128_BS16_MI2000_D20_s24.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 128\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 24\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.19M\n",
            "num decayed parameter tensors: 26, with 1,196,160 parameters\n",
            "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.1868, val loss 4.1814\n",
            "iter 0: loss 4.1917, time 2581.36ms, mfu -100.00%\n",
            "iter 10: loss 4.1718, time 448.83ms, mfu 0.23%\n",
            "iter 20: loss 4.1365, time 457.55ms, mfu 0.23%\n",
            "iter 30: loss 4.0756, time 457.74ms, mfu 0.23%\n",
            "iter 40: loss 3.9721, time 459.10ms, mfu 0.22%\n",
            "iter 50: loss 3.8855, time 452.85ms, mfu 0.22%\n",
            "iter 60: loss 3.8055, time 453.90ms, mfu 0.22%\n",
            "iter 70: loss 3.7273, time 460.09ms, mfu 0.22%\n",
            "iter 80: loss 3.6979, time 458.36ms, mfu 0.22%\n",
            "iter 90: loss 3.6855, time 459.98ms, mfu 0.22%\n",
            "iter 100: loss 3.6077, time 452.95ms, mfu 0.22%\n",
            "iter 110: loss 3.5616, time 454.94ms, mfu 0.22%\n",
            "iter 120: loss 3.5211, time 465.37ms, mfu 0.22%\n",
            "iter 130: loss 3.5114, time 453.30ms, mfu 0.22%\n",
            "iter 140: loss 3.4863, time 454.86ms, mfu 0.22%\n",
            "iter 150: loss 3.3760, time 459.23ms, mfu 0.22%\n",
            "iter 160: loss 3.3630, time 461.06ms, mfu 0.22%\n",
            "iter 170: loss 3.3573, time 464.13ms, mfu 0.22%\n",
            "iter 180: loss 3.2882, time 452.65ms, mfu 0.22%\n",
            "iter 190: loss 3.3188, time 458.27ms, mfu 0.22%\n",
            "step 200: train loss 3.2162, val loss 3.2239\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 200: loss 3.3108, time 2273.75ms, mfu 0.20%\n",
            "iter 210: loss 3.2473, time 453.32ms, mfu 0.21%\n",
            "iter 220: loss 3.2398, time 457.03ms, mfu 0.21%\n",
            "iter 230: loss 3.1881, time 457.85ms, mfu 0.21%\n",
            "iter 240: loss 3.1844, time 451.24ms, mfu 0.21%\n",
            "iter 250: loss 3.1409, time 457.33ms, mfu 0.21%\n",
            "iter 260: loss 3.0795, time 451.12ms, mfu 0.21%\n",
            "iter 270: loss 3.0406, time 451.27ms, mfu 0.21%\n",
            "iter 280: loss 3.0763, time 455.44ms, mfu 0.22%\n",
            "iter 290: loss 3.0150, time 449.35ms, mfu 0.22%\n",
            "iter 300: loss 2.9542, time 462.05ms, mfu 0.22%\n",
            "iter 310: loss 2.9571, time 449.58ms, mfu 0.22%\n",
            "iter 320: loss 2.9318, time 453.97ms, mfu 0.22%\n",
            "iter 330: loss 2.8944, time 449.23ms, mfu 0.22%\n",
            "iter 340: loss 2.8912, time 455.05ms, mfu 0.22%\n",
            "iter 350: loss 2.8724, time 450.58ms, mfu 0.22%\n",
            "iter 360: loss 2.8914, time 454.71ms, mfu 0.22%\n",
            "iter 370: loss 2.8517, time 450.77ms, mfu 0.22%\n",
            "iter 380: loss 2.8584, time 468.70ms, mfu 0.22%\n",
            "iter 390: loss 2.7702, time 467.14ms, mfu 0.22%\n",
            "step 400: train loss 2.7319, val loss 2.7396\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 400: loss 2.7355, time 2230.12ms, mfu 0.20%\n",
            "iter 410: loss 2.7404, time 458.85ms, mfu 0.20%\n",
            "iter 420: loss 2.7236, time 460.48ms, mfu 0.21%\n",
            "iter 430: loss 2.6829, time 461.24ms, mfu 0.21%\n",
            "iter 440: loss 2.7358, time 459.09ms, mfu 0.21%\n",
            "iter 450: loss 2.6126, time 456.49ms, mfu 0.21%\n",
            "iter 460: loss 2.6439, time 458.96ms, mfu 0.21%\n",
            "iter 470: loss 2.5616, time 456.49ms, mfu 0.21%\n",
            "iter 480: loss 2.6786, time 462.66ms, mfu 0.21%\n",
            "iter 490: loss 2.5746, time 446.62ms, mfu 0.21%\n",
            "iter 500: loss 2.5377, time 447.55ms, mfu 0.22%\n",
            "iter 510: loss 2.5764, time 459.69ms, mfu 0.22%\n",
            "iter 520: loss 2.5204, time 453.37ms, mfu 0.22%\n",
            "iter 530: loss 2.5511, time 455.74ms, mfu 0.22%\n",
            "iter 540: loss 2.5332, time 452.29ms, mfu 0.22%\n",
            "iter 550: loss 2.5697, time 454.75ms, mfu 0.22%\n",
            "iter 560: loss 2.5330, time 460.37ms, mfu 0.22%\n",
            "iter 570: loss 2.4990, time 461.95ms, mfu 0.22%\n",
            "iter 580: loss 2.5106, time 461.47ms, mfu 0.22%\n",
            "iter 590: loss 2.4559, time 462.42ms, mfu 0.22%\n",
            "step 600: train loss 2.4286, val loss 2.4254\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 600: loss 2.4944, time 2254.67ms, mfu 0.20%\n",
            "iter 610: loss 2.4846, time 464.97ms, mfu 0.20%\n",
            "iter 620: loss 2.3696, time 462.68ms, mfu 0.20%\n",
            "iter 630: loss 2.4391, time 463.02ms, mfu 0.21%\n",
            "iter 640: loss 2.5161, time 473.19ms, mfu 0.21%\n",
            "iter 650: loss 2.4517, time 454.87ms, mfu 0.21%\n",
            "iter 660: loss 2.4644, time 454.66ms, mfu 0.21%\n",
            "iter 670: loss 2.4287, time 452.13ms, mfu 0.21%\n",
            "iter 680: loss 2.4128, time 458.80ms, mfu 0.21%\n",
            "iter 690: loss 2.3899, time 468.52ms, mfu 0.21%\n",
            "iter 700: loss 2.4005, time 455.10ms, mfu 0.21%\n",
            "iter 710: loss 2.3506, time 456.30ms, mfu 0.21%\n",
            "iter 720: loss 2.3753, time 456.45ms, mfu 0.22%\n",
            "iter 730: loss 2.4684, time 451.41ms, mfu 0.22%\n",
            "iter 740: loss 2.3700, time 455.03ms, mfu 0.22%\n",
            "iter 750: loss 2.3569, time 456.06ms, mfu 0.22%\n",
            "iter 760: loss 2.2469, time 451.53ms, mfu 0.22%\n",
            "iter 770: loss 2.3350, time 461.74ms, mfu 0.22%\n",
            "iter 780: loss 2.2744, time 449.38ms, mfu 0.22%\n",
            "iter 790: loss 2.4422, time 447.43ms, mfu 0.22%\n",
            "step 800: train loss 2.2458, val loss 2.2587\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 800: loss 2.2772, time 2256.15ms, mfu 0.20%\n",
            "iter 810: loss 2.3218, time 454.57ms, mfu 0.20%\n",
            "iter 820: loss 2.3559, time 466.20ms, mfu 0.21%\n",
            "iter 830: loss 2.3568, time 461.41ms, mfu 0.21%\n",
            "iter 840: loss 2.2693, time 463.14ms, mfu 0.21%\n",
            "iter 850: loss 2.2780, time 462.12ms, mfu 0.21%\n",
            "iter 860: loss 2.2237, time 463.84ms, mfu 0.21%\n",
            "iter 870: loss 2.2166, time 464.77ms, mfu 0.21%\n",
            "iter 880: loss 2.3085, time 452.89ms, mfu 0.21%\n",
            "iter 890: loss 2.2791, time 461.63ms, mfu 0.21%\n",
            "iter 900: loss 2.2541, time 462.81ms, mfu 0.21%\n",
            "iter 910: loss 2.1491, time 456.95ms, mfu 0.21%\n",
            "iter 920: loss 2.1347, time 458.66ms, mfu 0.22%\n",
            "iter 930: loss 2.1545, time 458.25ms, mfu 0.22%\n",
            "iter 940: loss 2.1858, time 458.85ms, mfu 0.22%\n",
            "iter 950: loss 2.2518, time 462.50ms, mfu 0.22%\n",
            "iter 960: loss 2.2073, time 461.18ms, mfu 0.22%\n",
            "iter 970: loss 2.1757, time 462.06ms, mfu 0.22%\n",
            "iter 980: loss 2.1720, time 451.96ms, mfu 0.22%\n",
            "iter 990: loss 2.2087, time 460.26ms, mfu 0.22%\n",
            "step 1000: train loss 2.0783, val loss 2.1178\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1000: loss 2.1404, time 2296.64ms, mfu 0.20%\n",
            "iter 1010: loss 2.2518, time 454.26ms, mfu 0.20%\n",
            "iter 1020: loss 2.1291, time 451.71ms, mfu 0.21%\n",
            "iter 1030: loss 2.1331, time 449.54ms, mfu 0.21%\n",
            "iter 1040: loss 2.1240, time 451.39ms, mfu 0.21%\n",
            "iter 1050: loss 2.1177, time 454.97ms, mfu 0.21%\n",
            "iter 1060: loss 2.1241, time 451.69ms, mfu 0.21%\n",
            "iter 1070: loss 2.1683, time 451.72ms, mfu 0.21%\n",
            "iter 1080: loss 2.1224, time 455.47ms, mfu 0.21%\n",
            "iter 1090: loss 2.1294, time 454.30ms, mfu 0.22%\n",
            "iter 1100: loss 2.0970, time 455.55ms, mfu 0.22%\n",
            "iter 1110: loss 2.1052, time 450.55ms, mfu 0.22%\n",
            "iter 1120: loss 2.1729, time 455.22ms, mfu 0.22%\n",
            "iter 1130: loss 2.0744, time 462.89ms, mfu 0.22%\n",
            "iter 1140: loss 2.0044, time 451.12ms, mfu 0.22%\n",
            "iter 1150: loss 2.1624, time 446.93ms, mfu 0.22%\n",
            "iter 1160: loss 2.1746, time 468.28ms, mfu 0.22%\n",
            "iter 1170: loss 2.0986, time 454.10ms, mfu 0.22%\n",
            "iter 1180: loss 2.0949, time 456.82ms, mfu 0.22%\n",
            "iter 1190: loss 2.0933, time 459.69ms, mfu 0.22%\n",
            "step 1200: train loss 1.9303, val loss 2.0114\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1200: loss 2.0949, time 2215.37ms, mfu 0.20%\n",
            "iter 1210: loss 2.0210, time 470.91ms, mfu 0.20%\n",
            "iter 1220: loss 2.0258, time 458.39ms, mfu 0.21%\n",
            "iter 1230: loss 2.0501, time 457.38ms, mfu 0.21%\n",
            "iter 1240: loss 2.0012, time 458.60ms, mfu 0.21%\n",
            "iter 1250: loss 2.0555, time 450.71ms, mfu 0.21%\n",
            "iter 1260: loss 1.9976, time 466.31ms, mfu 0.21%\n",
            "iter 1270: loss 2.0522, time 449.56ms, mfu 0.21%\n",
            "iter 1280: loss 2.0291, time 458.18ms, mfu 0.21%\n",
            "iter 1290: loss 1.9574, time 464.11ms, mfu 0.21%\n",
            "iter 1300: loss 1.9871, time 460.43ms, mfu 0.21%\n",
            "iter 1310: loss 1.9423, time 463.56ms, mfu 0.21%\n",
            "iter 1320: loss 1.9084, time 448.71ms, mfu 0.22%\n",
            "iter 1330: loss 1.9942, time 453.63ms, mfu 0.22%\n",
            "iter 1340: loss 1.8904, time 460.46ms, mfu 0.22%\n",
            "iter 1350: loss 2.0553, time 453.47ms, mfu 0.22%\n",
            "iter 1360: loss 1.9325, time 450.83ms, mfu 0.22%\n",
            "iter 1370: loss 1.9564, time 461.29ms, mfu 0.22%\n",
            "iter 1380: loss 2.0308, time 462.95ms, mfu 0.22%\n",
            "iter 1390: loss 1.9174, time 450.99ms, mfu 0.22%\n",
            "step 1400: train loss 1.8167, val loss 1.9259\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1400: loss 1.9528, time 2235.65ms, mfu 0.20%\n",
            "iter 1410: loss 1.9898, time 452.02ms, mfu 0.20%\n",
            "iter 1420: loss 1.9313, time 453.39ms, mfu 0.21%\n",
            "iter 1430: loss 1.9501, time 456.36ms, mfu 0.21%\n",
            "iter 1440: loss 1.9095, time 446.79ms, mfu 0.21%\n",
            "iter 1450: loss 2.0397, time 447.92ms, mfu 0.21%\n",
            "iter 1460: loss 1.8527, time 460.81ms, mfu 0.21%\n",
            "iter 1470: loss 1.8827, time 463.75ms, mfu 0.21%\n",
            "iter 1480: loss 1.8056, time 460.35ms, mfu 0.21%\n",
            "iter 1490: loss 1.9789, time 460.54ms, mfu 0.21%\n",
            "iter 1500: loss 1.9487, time 463.42ms, mfu 0.21%\n",
            "iter 1510: loss 1.8935, time 465.75ms, mfu 0.22%\n",
            "iter 1520: loss 1.8024, time 466.82ms, mfu 0.22%\n",
            "iter 1530: loss 1.8923, time 469.47ms, mfu 0.22%\n",
            "iter 1540: loss 1.7959, time 452.72ms, mfu 0.22%\n",
            "iter 1550: loss 1.8825, time 463.41ms, mfu 0.22%\n",
            "iter 1560: loss 1.8992, time 455.25ms, mfu 0.22%\n",
            "iter 1570: loss 1.8495, time 452.99ms, mfu 0.22%\n",
            "iter 1580: loss 1.9179, time 452.18ms, mfu 0.22%\n",
            "iter 1590: loss 1.8913, time 447.80ms, mfu 0.22%\n",
            "step 1600: train loss 1.7308, val loss 1.8830\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1600: loss 1.8827, time 2254.95ms, mfu 0.20%\n",
            "iter 1610: loss 1.8896, time 456.79ms, mfu 0.20%\n",
            "iter 1620: loss 1.8863, time 460.08ms, mfu 0.21%\n",
            "iter 1630: loss 1.8396, time 450.86ms, mfu 0.21%\n",
            "iter 1640: loss 1.7271, time 452.53ms, mfu 0.21%\n",
            "iter 1650: loss 1.7749, time 454.19ms, mfu 0.21%\n",
            "iter 1660: loss 1.8835, time 449.11ms, mfu 0.21%\n",
            "iter 1670: loss 1.8412, time 450.97ms, mfu 0.21%\n",
            "iter 1680: loss 1.8321, time 456.52ms, mfu 0.21%\n",
            "iter 1690: loss 1.7868, time 447.96ms, mfu 0.22%\n",
            "iter 1700: loss 1.7462, time 446.85ms, mfu 0.22%\n",
            "iter 1710: loss 1.7310, time 454.21ms, mfu 0.22%\n",
            "iter 1720: loss 1.7210, time 453.03ms, mfu 0.22%\n",
            "iter 1730: loss 1.8297, time 462.33ms, mfu 0.22%\n",
            "iter 1740: loss 1.7061, time 455.06ms, mfu 0.22%\n",
            "iter 1750: loss 1.8477, time 457.27ms, mfu 0.22%\n",
            "iter 1760: loss 1.9354, time 467.12ms, mfu 0.22%\n",
            "iter 1770: loss 1.7513, time 461.87ms, mfu 0.22%\n",
            "iter 1780: loss 1.8507, time 464.62ms, mfu 0.22%\n",
            "iter 1790: loss 1.7815, time 467.21ms, mfu 0.22%\n",
            "step 1800: train loss 1.6609, val loss 1.8272\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "iter 1800: loss 1.7612, time 2244.98ms, mfu 0.20%\n",
            "iter 1810: loss 1.7687, time 460.66ms, mfu 0.20%\n",
            "iter 1820: loss 1.6641, time 456.30ms, mfu 0.21%\n",
            "iter 1830: loss 1.7677, time 460.70ms, mfu 0.21%\n",
            "iter 1840: loss 1.8440, time 456.41ms, mfu 0.21%\n",
            "iter 1850: loss 1.8437, time 453.79ms, mfu 0.21%\n",
            "iter 1860: loss 1.7645, time 462.80ms, mfu 0.21%\n",
            "iter 1870: loss 1.8220, time 457.31ms, mfu 0.21%\n",
            "iter 1880: loss 1.7847, time 451.93ms, mfu 0.21%\n",
            "iter 1890: loss 1.7903, time 453.32ms, mfu 0.21%\n",
            "iter 1900: loss 1.6941, time 449.82ms, mfu 0.22%\n",
            "iter 1910: loss 1.6983, time 463.38ms, mfu 0.22%\n",
            "iter 1920: loss 1.6877, time 456.35ms, mfu 0.22%\n",
            "iter 1930: loss 1.7269, time 455.13ms, mfu 0.22%\n",
            "iter 1940: loss 1.7410, time 467.86ms, mfu 0.22%\n",
            "iter 1950: loss 1.7172, time 456.69ms, mfu 0.22%\n",
            "iter 1960: loss 1.7366, time 459.70ms, mfu 0.22%\n",
            "iter 1970: loss 1.6775, time 449.85ms, mfu 0.22%\n",
            "iter 1980: loss 1.7504, time 446.92ms, mfu 0.22%\n",
            "iter 1990: loss 1.6507, time 459.42ms, mfu 0.22%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 25/32: b64_L6_H8_E256_BS8_MI1000_D10_s25 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS8_MI1000_D10_s25.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 25\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2411, val loss 4.2378\n",
            "iter 0: loss 4.2435, time 2380.30ms, mfu -100.00%\n",
            "iter 10: loss 4.1605, time 441.13ms, mfu 0.44%\n",
            "iter 20: loss 3.9467, time 442.22ms, mfu 0.44%\n",
            "iter 30: loss 3.7615, time 441.62ms, mfu 0.44%\n",
            "iter 40: loss 3.5996, time 439.41ms, mfu 0.44%\n",
            "iter 50: loss 3.4351, time 443.24ms, mfu 0.44%\n",
            "iter 60: loss 3.4295, time 445.56ms, mfu 0.44%\n",
            "iter 70: loss 3.3327, time 434.81ms, mfu 0.44%\n",
            "iter 80: loss 3.2045, time 436.94ms, mfu 0.44%\n",
            "iter 90: loss 3.1101, time 438.50ms, mfu 0.44%\n",
            "iter 100: loss 2.9710, time 432.01ms, mfu 0.44%\n",
            "iter 110: loss 3.0406, time 432.19ms, mfu 0.44%\n",
            "iter 120: loss 2.8999, time 442.14ms, mfu 0.44%\n",
            "iter 130: loss 2.8016, time 434.48ms, mfu 0.44%\n",
            "iter 140: loss 2.8042, time 440.32ms, mfu 0.44%\n",
            "iter 150: loss 2.8062, time 430.00ms, mfu 0.44%\n",
            "iter 160: loss 2.7709, time 431.39ms, mfu 0.44%\n",
            "iter 170: loss 2.8100, time 444.57ms, mfu 0.44%\n",
            "iter 180: loss 2.7623, time 435.06ms, mfu 0.44%\n",
            "iter 190: loss 2.7335, time 440.73ms, mfu 0.44%\n",
            "step 200: train loss 2.6787, val loss 2.6898\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 200: loss 2.7087, time 2177.45ms, mfu 0.41%\n",
            "iter 210: loss 2.6803, time 436.48ms, mfu 0.41%\n",
            "iter 220: loss 2.6431, time 441.77ms, mfu 0.41%\n",
            "iter 230: loss 2.6946, time 443.42ms, mfu 0.42%\n",
            "iter 240: loss 2.6675, time 437.39ms, mfu 0.42%\n",
            "iter 250: loss 2.6827, time 435.34ms, mfu 0.42%\n",
            "iter 260: loss 2.5731, time 429.41ms, mfu 0.43%\n",
            "iter 270: loss 2.5592, time 430.27ms, mfu 0.43%\n",
            "iter 280: loss 2.5529, time 455.23ms, mfu 0.43%\n",
            "iter 290: loss 2.5792, time 425.43ms, mfu 0.43%\n",
            "iter 300: loss 2.5435, time 425.46ms, mfu 0.43%\n",
            "iter 310: loss 2.5109, time 434.61ms, mfu 0.43%\n",
            "iter 320: loss 2.4547, time 432.58ms, mfu 0.44%\n",
            "iter 330: loss 2.4661, time 432.43ms, mfu 0.44%\n",
            "iter 340: loss 2.4964, time 437.89ms, mfu 0.44%\n",
            "iter 350: loss 2.4234, time 438.14ms, mfu 0.44%\n",
            "iter 360: loss 2.3481, time 447.11ms, mfu 0.44%\n",
            "iter 370: loss 2.4318, time 439.02ms, mfu 0.44%\n",
            "iter 380: loss 2.3884, time 432.69ms, mfu 0.44%\n",
            "iter 390: loss 2.3312, time 436.25ms, mfu 0.44%\n",
            "step 400: train loss 2.3417, val loss 2.3558\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 400: loss 2.3553, time 2167.52ms, mfu 0.41%\n",
            "iter 410: loss 2.2756, time 437.48ms, mfu 0.41%\n",
            "iter 420: loss 2.3687, time 434.16ms, mfu 0.41%\n",
            "iter 430: loss 2.4840, time 439.67ms, mfu 0.42%\n",
            "iter 440: loss 2.2976, time 445.66ms, mfu 0.42%\n",
            "iter 450: loss 2.3297, time 437.20ms, mfu 0.42%\n",
            "iter 460: loss 2.3945, time 432.56ms, mfu 0.42%\n",
            "iter 470: loss 2.2572, time 444.08ms, mfu 0.42%\n",
            "iter 480: loss 2.3091, time 438.25ms, mfu 0.43%\n",
            "iter 490: loss 2.2517, time 435.83ms, mfu 0.43%\n",
            "iter 500: loss 2.2409, time 434.00ms, mfu 0.43%\n",
            "iter 510: loss 2.3065, time 442.45ms, mfu 0.43%\n",
            "iter 520: loss 2.2256, time 435.16ms, mfu 0.43%\n",
            "iter 530: loss 2.2086, time 436.99ms, mfu 0.43%\n",
            "iter 540: loss 2.2639, time 433.71ms, mfu 0.44%\n",
            "iter 550: loss 2.2295, time 443.48ms, mfu 0.44%\n",
            "iter 560: loss 2.1133, time 433.48ms, mfu 0.44%\n",
            "iter 570: loss 2.2360, time 435.60ms, mfu 0.44%\n",
            "iter 580: loss 2.2396, time 435.46ms, mfu 0.44%\n",
            "iter 590: loss 2.0716, time 429.00ms, mfu 0.44%\n",
            "step 600: train loss 2.1012, val loss 2.1388\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 600: loss 2.1698, time 2156.17ms, mfu 0.41%\n",
            "iter 610: loss 2.1932, time 431.05ms, mfu 0.41%\n",
            "iter 620: loss 2.1227, time 425.03ms, mfu 0.41%\n",
            "iter 630: loss 2.0869, time 449.33ms, mfu 0.42%\n",
            "iter 640: loss 2.1300, time 433.64ms, mfu 0.42%\n",
            "iter 650: loss 2.1207, time 436.89ms, mfu 0.42%\n",
            "iter 660: loss 1.9883, time 443.64ms, mfu 0.42%\n",
            "iter 670: loss 2.0794, time 428.71ms, mfu 0.43%\n",
            "iter 680: loss 2.0127, time 431.61ms, mfu 0.43%\n",
            "iter 690: loss 2.1325, time 435.83ms, mfu 0.43%\n",
            "iter 700: loss 2.1061, time 437.64ms, mfu 0.43%\n",
            "iter 710: loss 2.0580, time 434.54ms, mfu 0.43%\n",
            "iter 720: loss 2.1344, time 433.85ms, mfu 0.44%\n",
            "iter 730: loss 2.0175, time 427.81ms, mfu 0.44%\n",
            "iter 740: loss 1.9815, time 440.41ms, mfu 0.44%\n",
            "iter 750: loss 1.9350, time 436.16ms, mfu 0.44%\n",
            "iter 760: loss 1.9666, time 438.15ms, mfu 0.44%\n",
            "iter 770: loss 1.8916, time 437.92ms, mfu 0.44%\n",
            "iter 780: loss 2.0062, time 429.31ms, mfu 0.44%\n",
            "iter 790: loss 1.9432, time 432.33ms, mfu 0.44%\n",
            "step 800: train loss 1.8591, val loss 1.9611\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 800: loss 1.8586, time 2175.01ms, mfu 0.41%\n",
            "iter 810: loss 1.8210, time 441.60ms, mfu 0.41%\n",
            "iter 820: loss 2.1284, time 434.42ms, mfu 0.41%\n",
            "iter 830: loss 1.8964, time 444.56ms, mfu 0.42%\n",
            "iter 840: loss 1.8727, time 440.33ms, mfu 0.42%\n",
            "iter 850: loss 1.8519, time 439.26ms, mfu 0.42%\n",
            "iter 860: loss 1.8863, time 435.55ms, mfu 0.42%\n",
            "iter 870: loss 1.8919, time 433.21ms, mfu 0.43%\n",
            "iter 880: loss 1.8020, time 448.26ms, mfu 0.43%\n",
            "iter 890: loss 1.8341, time 442.69ms, mfu 0.43%\n",
            "iter 900: loss 1.7979, time 442.06ms, mfu 0.43%\n",
            "iter 910: loss 1.8776, time 436.01ms, mfu 0.43%\n",
            "iter 920: loss 1.7926, time 439.03ms, mfu 0.43%\n",
            "iter 930: loss 1.7997, time 443.57ms, mfu 0.43%\n",
            "iter 940: loss 1.6786, time 429.13ms, mfu 0.43%\n",
            "iter 950: loss 1.5625, time 431.97ms, mfu 0.44%\n",
            "iter 960: loss 1.6558, time 441.88ms, mfu 0.44%\n",
            "iter 970: loss 1.7129, time 442.83ms, mfu 0.44%\n",
            "iter 980: loss 1.8088, time 427.16ms, mfu 0.44%\n",
            "iter 990: loss 1.8918, time 441.32ms, mfu 0.44%\n",
            "step 1000: train loss 1.6729, val loss 1.8430\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "iter 1000: loss 1.7955, time 2159.32ms, mfu 0.40%\n",
            "\n",
            "=== Experiment 26/32: b64_L6_H8_E256_BS8_MI1000_D20_s26 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS8_MI1000_D20_s26.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 26\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2411, val loss 4.2378\n",
            "iter 0: loss 4.2349, time 2432.82ms, mfu -100.00%\n",
            "iter 10: loss 4.1801, time 436.29ms, mfu 0.45%\n",
            "iter 20: loss 3.9991, time 437.17ms, mfu 0.45%\n",
            "iter 30: loss 3.8161, time 443.66ms, mfu 0.44%\n",
            "iter 40: loss 3.6382, time 435.99ms, mfu 0.44%\n",
            "iter 50: loss 3.4678, time 435.08ms, mfu 0.44%\n",
            "iter 60: loss 3.4823, time 436.61ms, mfu 0.45%\n",
            "iter 70: loss 3.3892, time 429.72ms, mfu 0.45%\n",
            "iter 80: loss 3.2946, time 442.69ms, mfu 0.45%\n",
            "iter 90: loss 3.1820, time 444.84ms, mfu 0.44%\n",
            "iter 100: loss 3.0290, time 434.28ms, mfu 0.44%\n",
            "iter 110: loss 3.1077, time 461.79ms, mfu 0.44%\n",
            "iter 120: loss 2.9680, time 431.15ms, mfu 0.44%\n",
            "iter 130: loss 2.8484, time 436.93ms, mfu 0.44%\n",
            "iter 140: loss 2.8559, time 435.89ms, mfu 0.44%\n",
            "iter 150: loss 2.8602, time 436.85ms, mfu 0.44%\n",
            "iter 160: loss 2.8099, time 439.04ms, mfu 0.44%\n",
            "iter 170: loss 2.8487, time 434.28ms, mfu 0.44%\n",
            "iter 180: loss 2.8000, time 440.62ms, mfu 0.44%\n",
            "iter 190: loss 2.7764, time 438.54ms, mfu 0.44%\n",
            "step 200: train loss 2.6979, val loss 2.7061\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 200: loss 2.7324, time 2161.60ms, mfu 0.41%\n",
            "iter 210: loss 2.7232, time 435.24ms, mfu 0.41%\n",
            "iter 220: loss 2.6801, time 431.97ms, mfu 0.42%\n",
            "iter 230: loss 2.7593, time 430.02ms, mfu 0.42%\n",
            "iter 240: loss 2.7147, time 439.34ms, mfu 0.42%\n",
            "iter 250: loss 2.7234, time 437.69ms, mfu 0.42%\n",
            "iter 260: loss 2.6256, time 434.04ms, mfu 0.43%\n",
            "iter 270: loss 2.6039, time 446.55ms, mfu 0.43%\n",
            "iter 280: loss 2.5878, time 440.98ms, mfu 0.43%\n",
            "iter 290: loss 2.6244, time 436.84ms, mfu 0.43%\n",
            "iter 300: loss 2.5778, time 440.11ms, mfu 0.43%\n",
            "iter 310: loss 2.5423, time 439.88ms, mfu 0.43%\n",
            "iter 320: loss 2.4951, time 444.46ms, mfu 0.43%\n",
            "iter 330: loss 2.5052, time 432.34ms, mfu 0.43%\n",
            "iter 340: loss 2.5403, time 436.36ms, mfu 0.44%\n",
            "iter 350: loss 2.4753, time 441.89ms, mfu 0.44%\n",
            "iter 360: loss 2.4071, time 434.13ms, mfu 0.44%\n",
            "iter 370: loss 2.4756, time 436.92ms, mfu 0.44%\n",
            "iter 380: loss 2.4338, time 457.46ms, mfu 0.44%\n",
            "iter 390: loss 2.3823, time 433.61ms, mfu 0.44%\n",
            "step 400: train loss 2.3851, val loss 2.3969\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 400: loss 2.3987, time 2158.74ms, mfu 0.40%\n",
            "iter 410: loss 2.3573, time 436.92ms, mfu 0.41%\n",
            "iter 420: loss 2.3935, time 437.32ms, mfu 0.41%\n",
            "iter 430: loss 2.5333, time 449.07ms, mfu 0.41%\n",
            "iter 440: loss 2.3211, time 436.15ms, mfu 0.42%\n",
            "iter 450: loss 2.3982, time 435.62ms, mfu 0.42%\n",
            "iter 460: loss 2.4564, time 452.03ms, mfu 0.42%\n",
            "iter 470: loss 2.2974, time 432.88ms, mfu 0.42%\n",
            "iter 480: loss 2.3828, time 434.84ms, mfu 0.43%\n",
            "iter 490: loss 2.3164, time 434.96ms, mfu 0.43%\n",
            "iter 500: loss 2.3121, time 435.91ms, mfu 0.43%\n",
            "iter 510: loss 2.3481, time 433.66ms, mfu 0.43%\n",
            "iter 520: loss 2.2679, time 442.89ms, mfu 0.43%\n",
            "iter 530: loss 2.2887, time 442.08ms, mfu 0.43%\n",
            "iter 540: loss 2.3443, time 442.63ms, mfu 0.43%\n",
            "iter 550: loss 2.3163, time 441.98ms, mfu 0.43%\n",
            "iter 560: loss 2.2080, time 445.52ms, mfu 0.43%\n",
            "iter 570: loss 2.3136, time 448.10ms, mfu 0.43%\n",
            "iter 580: loss 2.3057, time 442.09ms, mfu 0.43%\n",
            "iter 590: loss 2.1433, time 436.68ms, mfu 0.44%\n",
            "step 600: train loss 2.1772, val loss 2.2017\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 600: loss 2.2685, time 2202.29ms, mfu 0.40%\n",
            "iter 610: loss 2.2845, time 432.51ms, mfu 0.41%\n",
            "iter 620: loss 2.2037, time 439.45ms, mfu 0.41%\n",
            "iter 630: loss 2.1986, time 442.63ms, mfu 0.41%\n",
            "iter 640: loss 2.2251, time 442.32ms, mfu 0.42%\n",
            "iter 650: loss 2.1931, time 442.40ms, mfu 0.42%\n",
            "iter 660: loss 2.0606, time 434.89ms, mfu 0.42%\n",
            "iter 670: loss 2.1769, time 433.65ms, mfu 0.42%\n",
            "iter 680: loss 2.0779, time 436.79ms, mfu 0.43%\n",
            "iter 690: loss 2.2005, time 437.71ms, mfu 0.43%\n",
            "iter 700: loss 2.1855, time 449.87ms, mfu 0.43%\n",
            "iter 710: loss 2.1773, time 437.03ms, mfu 0.43%\n",
            "iter 720: loss 2.2191, time 444.60ms, mfu 0.43%\n",
            "iter 730: loss 2.1475, time 448.51ms, mfu 0.43%\n",
            "iter 740: loss 2.0926, time 428.97ms, mfu 0.43%\n",
            "iter 750: loss 2.0222, time 437.30ms, mfu 0.43%\n",
            "iter 760: loss 2.1127, time 444.80ms, mfu 0.43%\n",
            "iter 770: loss 2.0133, time 433.17ms, mfu 0.44%\n",
            "iter 780: loss 2.1171, time 434.07ms, mfu 0.44%\n",
            "iter 790: loss 2.0113, time 429.87ms, mfu 0.44%\n",
            "step 800: train loss 1.9606, val loss 2.0274\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 800: loss 1.9824, time 2168.43ms, mfu 0.40%\n",
            "iter 810: loss 1.9225, time 441.53ms, mfu 0.41%\n",
            "iter 820: loss 2.2513, time 430.74ms, mfu 0.41%\n",
            "iter 830: loss 2.0477, time 444.92ms, mfu 0.41%\n",
            "iter 840: loss 2.0227, time 435.37ms, mfu 0.42%\n",
            "iter 850: loss 1.9681, time 436.90ms, mfu 0.42%\n",
            "iter 860: loss 2.0201, time 435.12ms, mfu 0.42%\n",
            "iter 870: loss 1.9794, time 438.64ms, mfu 0.42%\n",
            "iter 880: loss 1.9332, time 436.54ms, mfu 0.43%\n",
            "iter 890: loss 1.9391, time 443.22ms, mfu 0.43%\n",
            "iter 900: loss 1.8931, time 433.71ms, mfu 0.43%\n",
            "iter 910: loss 1.9666, time 428.17ms, mfu 0.43%\n",
            "iter 920: loss 1.9477, time 439.54ms, mfu 0.43%\n",
            "iter 930: loss 1.9323, time 441.13ms, mfu 0.43%\n",
            "iter 940: loss 1.8478, time 444.54ms, mfu 0.43%\n",
            "iter 950: loss 1.7413, time 440.44ms, mfu 0.44%\n",
            "iter 960: loss 1.7978, time 450.25ms, mfu 0.43%\n",
            "iter 970: loss 1.8195, time 440.73ms, mfu 0.44%\n",
            "iter 980: loss 1.9425, time 437.07ms, mfu 0.44%\n",
            "iter 990: loss 1.9967, time 438.51ms, mfu 0.44%\n",
            "step 1000: train loss 1.7789, val loss 1.9209\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "iter 1000: loss 1.9019, time 2185.53ms, mfu 0.40%\n",
            "\n",
            "=== Experiment 27/32: b64_L6_H8_E256_BS8_MI2000_D10_s27 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS8_MI2000_D10_s27.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 27\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2411, val loss 4.2378\n",
            "iter 0: loss 4.2435, time 2401.75ms, mfu -100.00%\n",
            "iter 10: loss 4.1605, time 439.48ms, mfu 0.44%\n",
            "iter 20: loss 3.9467, time 432.26ms, mfu 0.44%\n",
            "iter 30: loss 3.7615, time 436.28ms, mfu 0.44%\n",
            "iter 40: loss 3.5996, time 444.51ms, mfu 0.44%\n",
            "iter 50: loss 3.4351, time 430.35ms, mfu 0.44%\n",
            "iter 60: loss 3.4295, time 428.25ms, mfu 0.44%\n",
            "iter 70: loss 3.3327, time 453.99ms, mfu 0.44%\n",
            "iter 80: loss 3.2045, time 439.78ms, mfu 0.44%\n",
            "iter 90: loss 3.1101, time 431.40ms, mfu 0.44%\n",
            "iter 100: loss 2.9710, time 438.13ms, mfu 0.44%\n",
            "iter 110: loss 3.0406, time 440.24ms, mfu 0.44%\n",
            "iter 120: loss 2.8999, time 436.47ms, mfu 0.44%\n",
            "iter 130: loss 2.8016, time 434.31ms, mfu 0.44%\n",
            "iter 140: loss 2.8042, time 432.68ms, mfu 0.44%\n",
            "iter 150: loss 2.8062, time 444.50ms, mfu 0.44%\n",
            "iter 160: loss 2.7709, time 442.94ms, mfu 0.44%\n",
            "iter 170: loss 2.8100, time 433.72ms, mfu 0.44%\n",
            "iter 180: loss 2.7623, time 456.64ms, mfu 0.44%\n",
            "iter 190: loss 2.7335, time 435.94ms, mfu 0.44%\n",
            "step 200: train loss 2.6787, val loss 2.6898\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 200: loss 2.7087, time 2183.85ms, mfu 0.41%\n",
            "iter 210: loss 2.6803, time 438.55ms, mfu 0.41%\n",
            "iter 220: loss 2.6431, time 437.50ms, mfu 0.41%\n",
            "iter 230: loss 2.6946, time 442.40ms, mfu 0.42%\n",
            "iter 240: loss 2.6675, time 442.82ms, mfu 0.42%\n",
            "iter 250: loss 2.6827, time 437.20ms, mfu 0.42%\n",
            "iter 260: loss 2.5731, time 446.23ms, mfu 0.42%\n",
            "iter 270: loss 2.5592, time 432.52ms, mfu 0.43%\n",
            "iter 280: loss 2.5529, time 427.99ms, mfu 0.43%\n",
            "iter 290: loss 2.5792, time 433.94ms, mfu 0.43%\n",
            "iter 300: loss 2.5435, time 429.23ms, mfu 0.43%\n",
            "iter 310: loss 2.5109, time 433.62ms, mfu 0.43%\n",
            "iter 320: loss 2.4547, time 432.01ms, mfu 0.44%\n",
            "iter 330: loss 2.4661, time 429.14ms, mfu 0.44%\n",
            "iter 340: loss 2.4964, time 442.07ms, mfu 0.44%\n",
            "iter 350: loss 2.4234, time 436.61ms, mfu 0.44%\n",
            "iter 360: loss 2.3481, time 435.95ms, mfu 0.44%\n",
            "iter 370: loss 2.4318, time 440.96ms, mfu 0.44%\n",
            "iter 380: loss 2.3884, time 433.25ms, mfu 0.44%\n",
            "iter 390: loss 2.3312, time 431.80ms, mfu 0.44%\n",
            "step 400: train loss 2.3417, val loss 2.3558\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 400: loss 2.3553, time 2159.44ms, mfu 0.41%\n",
            "iter 410: loss 2.2756, time 430.43ms, mfu 0.41%\n",
            "iter 420: loss 2.3687, time 441.54ms, mfu 0.41%\n",
            "iter 430: loss 2.4840, time 440.55ms, mfu 0.42%\n",
            "iter 440: loss 2.2976, time 438.54ms, mfu 0.42%\n",
            "iter 450: loss 2.3297, time 454.01ms, mfu 0.42%\n",
            "iter 460: loss 2.3945, time 427.95ms, mfu 0.42%\n",
            "iter 470: loss 2.2572, time 431.81ms, mfu 0.43%\n",
            "iter 480: loss 2.3091, time 434.63ms, mfu 0.43%\n",
            "iter 490: loss 2.2517, time 430.21ms, mfu 0.43%\n",
            "iter 500: loss 2.2409, time 433.44ms, mfu 0.43%\n",
            "iter 510: loss 2.3065, time 434.96ms, mfu 0.43%\n",
            "iter 520: loss 2.2256, time 439.74ms, mfu 0.43%\n",
            "iter 530: loss 2.2086, time 438.63ms, mfu 0.44%\n",
            "iter 540: loss 2.2639, time 434.96ms, mfu 0.44%\n",
            "iter 550: loss 2.2295, time 437.00ms, mfu 0.44%\n",
            "iter 560: loss 2.1133, time 430.03ms, mfu 0.44%\n",
            "iter 570: loss 2.2360, time 436.10ms, mfu 0.44%\n",
            "iter 580: loss 2.2396, time 440.31ms, mfu 0.44%\n",
            "iter 590: loss 2.0716, time 435.95ms, mfu 0.44%\n",
            "step 600: train loss 2.1012, val loss 2.1388\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 600: loss 2.1698, time 2182.65ms, mfu 0.41%\n",
            "iter 610: loss 2.1932, time 444.35ms, mfu 0.41%\n",
            "iter 620: loss 2.1227, time 437.75ms, mfu 0.41%\n",
            "iter 630: loss 2.0869, time 474.13ms, mfu 0.41%\n",
            "iter 640: loss 2.1300, time 454.57ms, mfu 0.41%\n",
            "iter 650: loss 2.1207, time 431.07ms, mfu 0.42%\n",
            "iter 660: loss 1.9883, time 430.96ms, mfu 0.42%\n",
            "iter 670: loss 2.0794, time 434.98ms, mfu 0.42%\n",
            "iter 680: loss 2.0127, time 437.05ms, mfu 0.43%\n",
            "iter 690: loss 2.1325, time 432.54ms, mfu 0.43%\n",
            "iter 700: loss 2.1061, time 432.81ms, mfu 0.43%\n",
            "iter 710: loss 2.0580, time 438.27ms, mfu 0.43%\n",
            "iter 720: loss 2.1344, time 436.41ms, mfu 0.43%\n",
            "iter 730: loss 2.0175, time 437.12ms, mfu 0.43%\n",
            "iter 740: loss 1.9815, time 438.10ms, mfu 0.43%\n",
            "iter 750: loss 1.9350, time 441.96ms, mfu 0.44%\n",
            "iter 760: loss 1.9666, time 442.94ms, mfu 0.44%\n",
            "iter 770: loss 1.8916, time 433.98ms, mfu 0.44%\n",
            "iter 780: loss 2.0062, time 433.58ms, mfu 0.44%\n",
            "iter 790: loss 1.9432, time 431.05ms, mfu 0.44%\n",
            "step 800: train loss 1.8591, val loss 1.9611\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 800: loss 1.8586, time 2217.95ms, mfu 0.40%\n",
            "iter 810: loss 1.8210, time 434.40ms, mfu 0.41%\n",
            "iter 820: loss 2.1284, time 441.94ms, mfu 0.41%\n",
            "iter 830: loss 1.8964, time 457.53ms, mfu 0.41%\n",
            "iter 840: loss 1.8727, time 435.58ms, mfu 0.42%\n",
            "iter 850: loss 1.8519, time 436.71ms, mfu 0.42%\n",
            "iter 860: loss 1.8863, time 433.96ms, mfu 0.42%\n",
            "iter 870: loss 1.8919, time 442.99ms, mfu 0.42%\n",
            "iter 880: loss 1.8020, time 432.20ms, mfu 0.43%\n",
            "iter 890: loss 1.8341, time 435.73ms, mfu 0.43%\n",
            "iter 900: loss 1.7979, time 432.94ms, mfu 0.43%\n",
            "iter 910: loss 1.8776, time 435.94ms, mfu 0.43%\n",
            "iter 920: loss 1.7926, time 429.03ms, mfu 0.43%\n",
            "iter 930: loss 1.7997, time 433.57ms, mfu 0.44%\n",
            "iter 940: loss 1.6786, time 445.41ms, mfu 0.44%\n",
            "iter 950: loss 1.5625, time 434.14ms, mfu 0.44%\n",
            "iter 960: loss 1.6558, time 443.35ms, mfu 0.44%\n",
            "iter 970: loss 1.7129, time 432.67ms, mfu 0.44%\n",
            "iter 980: loss 1.8088, time 435.18ms, mfu 0.44%\n",
            "iter 990: loss 1.8918, time 438.09ms, mfu 0.44%\n",
            "step 1000: train loss 1.6729, val loss 1.8430\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1000: loss 1.7955, time 2164.99ms, mfu 0.40%\n",
            "iter 1010: loss 1.7818, time 437.24ms, mfu 0.41%\n",
            "iter 1020: loss 1.7848, time 449.04ms, mfu 0.41%\n",
            "iter 1030: loss 1.8854, time 442.43ms, mfu 0.41%\n",
            "iter 1040: loss 1.7162, time 435.30ms, mfu 0.42%\n",
            "iter 1050: loss 1.7708, time 437.55ms, mfu 0.42%\n",
            "iter 1060: loss 1.7557, time 434.24ms, mfu 0.42%\n",
            "iter 1070: loss 1.9268, time 435.22ms, mfu 0.42%\n",
            "iter 1080: loss 1.6392, time 441.08ms, mfu 0.43%\n",
            "iter 1090: loss 1.5841, time 439.00ms, mfu 0.43%\n",
            "iter 1100: loss 1.7195, time 443.92ms, mfu 0.43%\n",
            "iter 1110: loss 1.6703, time 439.76ms, mfu 0.43%\n",
            "iter 1120: loss 1.6402, time 431.76ms, mfu 0.43%\n",
            "iter 1130: loss 1.8003, time 449.10ms, mfu 0.43%\n",
            "iter 1140: loss 1.5443, time 430.38ms, mfu 0.43%\n",
            "iter 1150: loss 1.7550, time 432.58ms, mfu 0.44%\n",
            "iter 1160: loss 1.5690, time 432.40ms, mfu 0.44%\n",
            "iter 1170: loss 1.5928, time 430.14ms, mfu 0.44%\n",
            "iter 1180: loss 1.6212, time 434.74ms, mfu 0.44%\n",
            "iter 1190: loss 1.6219, time 433.99ms, mfu 0.44%\n",
            "step 1200: train loss 1.5749, val loss 1.7349\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1200: loss 1.8602, time 2190.89ms, mfu 0.41%\n",
            "iter 1210: loss 1.6389, time 445.69ms, mfu 0.41%\n",
            "iter 1220: loss 1.6578, time 437.59ms, mfu 0.41%\n",
            "iter 1230: loss 1.5684, time 439.45ms, mfu 0.41%\n",
            "iter 1240: loss 1.5368, time 444.37ms, mfu 0.42%\n",
            "iter 1250: loss 1.6122, time 442.93ms, mfu 0.42%\n",
            "iter 1260: loss 1.6351, time 437.43ms, mfu 0.42%\n",
            "iter 1270: loss 1.5381, time 438.61ms, mfu 0.42%\n",
            "iter 1280: loss 1.5110, time 433.53ms, mfu 0.43%\n",
            "iter 1290: loss 1.6684, time 450.04ms, mfu 0.43%\n",
            "iter 1300: loss 1.5804, time 443.98ms, mfu 0.43%\n",
            "iter 1310: loss 1.6113, time 432.67ms, mfu 0.43%\n",
            "iter 1320: loss 1.5741, time 452.05ms, mfu 0.43%\n",
            "iter 1330: loss 1.5305, time 436.19ms, mfu 0.43%\n",
            "iter 1340: loss 1.6604, time 433.85ms, mfu 0.43%\n",
            "iter 1350: loss 1.4699, time 435.82ms, mfu 0.43%\n",
            "iter 1360: loss 1.6012, time 434.70ms, mfu 0.44%\n",
            "iter 1370: loss 1.7151, time 448.90ms, mfu 0.44%\n",
            "iter 1380: loss 1.4890, time 432.35ms, mfu 0.44%\n",
            "iter 1390: loss 1.4487, time 435.75ms, mfu 0.44%\n",
            "step 1400: train loss 1.4896, val loss 1.6826\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1400: loss 1.5368, time 2223.23ms, mfu 0.40%\n",
            "iter 1410: loss 1.5317, time 435.58ms, mfu 0.41%\n",
            "iter 1420: loss 1.3758, time 431.90ms, mfu 0.41%\n",
            "iter 1430: loss 1.5728, time 442.87ms, mfu 0.41%\n",
            "iter 1440: loss 1.3838, time 442.37ms, mfu 0.42%\n",
            "iter 1450: loss 1.5613, time 450.99ms, mfu 0.42%\n",
            "iter 1460: loss 1.6424, time 440.86ms, mfu 0.42%\n",
            "iter 1470: loss 1.6362, time 438.53ms, mfu 0.42%\n",
            "iter 1480: loss 1.4872, time 441.15ms, mfu 0.42%\n",
            "iter 1490: loss 1.7071, time 430.72ms, mfu 0.43%\n",
            "iter 1500: loss 1.5447, time 441.05ms, mfu 0.43%\n",
            "iter 1510: loss 1.6236, time 437.93ms, mfu 0.43%\n",
            "iter 1520: loss 1.4756, time 435.02ms, mfu 0.43%\n",
            "iter 1530: loss 1.7511, time 439.48ms, mfu 0.43%\n",
            "iter 1540: loss 1.5313, time 436.17ms, mfu 0.43%\n",
            "iter 1550: loss 1.4736, time 431.33ms, mfu 0.44%\n",
            "iter 1560: loss 1.5865, time 441.69ms, mfu 0.44%\n",
            "iter 1570: loss 1.4346, time 425.80ms, mfu 0.44%\n",
            "iter 1580: loss 1.4935, time 433.42ms, mfu 0.44%\n",
            "iter 1590: loss 1.6126, time 445.24ms, mfu 0.44%\n",
            "step 1600: train loss 1.4274, val loss 1.6452\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1600: loss 1.6512, time 2183.80ms, mfu 0.40%\n",
            "iter 1610: loss 1.4626, time 433.48ms, mfu 0.41%\n",
            "iter 1620: loss 1.4632, time 433.27ms, mfu 0.41%\n",
            "iter 1630: loss 1.5818, time 447.44ms, mfu 0.41%\n",
            "iter 1640: loss 1.5057, time 443.12ms, mfu 0.42%\n",
            "iter 1650: loss 1.5375, time 434.81ms, mfu 0.42%\n",
            "iter 1660: loss 1.5974, time 435.87ms, mfu 0.42%\n",
            "iter 1670: loss 1.5154, time 434.72ms, mfu 0.43%\n",
            "iter 1680: loss 1.4181, time 430.13ms, mfu 0.43%\n",
            "iter 1690: loss 1.3632, time 434.26ms, mfu 0.43%\n",
            "iter 1700: loss 1.4997, time 441.29ms, mfu 0.43%\n",
            "iter 1710: loss 1.5332, time 432.75ms, mfu 0.43%\n",
            "iter 1720: loss 1.4871, time 433.69ms, mfu 0.43%\n",
            "iter 1730: loss 1.4978, time 434.28ms, mfu 0.44%\n",
            "iter 1740: loss 1.4685, time 429.04ms, mfu 0.44%\n",
            "iter 1750: loss 1.5544, time 432.94ms, mfu 0.44%\n",
            "iter 1760: loss 1.4987, time 435.28ms, mfu 0.44%\n",
            "iter 1770: loss 1.4881, time 435.92ms, mfu 0.44%\n",
            "iter 1780: loss 1.5357, time 442.60ms, mfu 0.44%\n",
            "iter 1790: loss 1.4319, time 436.01ms, mfu 0.44%\n",
            "step 1800: train loss 1.3889, val loss 1.5975\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "iter 1800: loss 1.6009, time 2177.27ms, mfu 0.41%\n",
            "iter 1810: loss 1.4927, time 444.86ms, mfu 0.41%\n",
            "iter 1820: loss 1.5405, time 441.01ms, mfu 0.41%\n",
            "iter 1830: loss 1.4098, time 449.96ms, mfu 0.41%\n",
            "iter 1840: loss 1.4172, time 429.90ms, mfu 0.42%\n",
            "iter 1850: loss 1.5537, time 429.02ms, mfu 0.42%\n",
            "iter 1860: loss 1.4484, time 439.07ms, mfu 0.42%\n",
            "iter 1870: loss 1.4875, time 434.74ms, mfu 0.43%\n",
            "iter 1880: loss 1.6326, time 434.91ms, mfu 0.43%\n",
            "iter 1890: loss 1.4191, time 437.76ms, mfu 0.43%\n",
            "iter 1900: loss 1.3075, time 436.84ms, mfu 0.43%\n",
            "iter 1910: loss 1.4057, time 433.83ms, mfu 0.43%\n",
            "iter 1920: loss 1.3341, time 433.45ms, mfu 0.43%\n",
            "iter 1930: loss 1.3723, time 432.43ms, mfu 0.44%\n",
            "iter 1940: loss 1.4637, time 432.28ms, mfu 0.44%\n",
            "iter 1950: loss 1.4794, time 434.63ms, mfu 0.44%\n",
            "iter 1960: loss 1.4199, time 431.42ms, mfu 0.44%\n",
            "iter 1970: loss 1.3746, time 434.17ms, mfu 0.44%\n",
            "iter 1980: loss 1.4348, time 430.79ms, mfu 0.44%\n",
            "iter 1990: loss 1.4510, time 437.29ms, mfu 0.44%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 28/32: b64_L6_H8_E256_BS8_MI2000_D20_s28 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS8_MI2000_D20_s28.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 8\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 28\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 20,480\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2411, val loss 4.2378\n",
            "iter 0: loss 4.2349, time 2406.30ms, mfu -100.00%\n",
            "iter 10: loss 4.1801, time 435.77ms, mfu 0.45%\n",
            "iter 20: loss 3.9991, time 435.27ms, mfu 0.45%\n",
            "iter 30: loss 3.8161, time 439.29ms, mfu 0.45%\n",
            "iter 40: loss 3.6382, time 439.72ms, mfu 0.45%\n",
            "iter 50: loss 3.4678, time 432.28ms, mfu 0.45%\n",
            "iter 60: loss 3.4823, time 431.11ms, mfu 0.45%\n",
            "iter 70: loss 3.3892, time 436.83ms, mfu 0.45%\n",
            "iter 80: loss 3.2946, time 435.44ms, mfu 0.45%\n",
            "iter 90: loss 3.1820, time 432.71ms, mfu 0.45%\n",
            "iter 100: loss 3.0290, time 432.66ms, mfu 0.45%\n",
            "iter 110: loss 3.1077, time 429.02ms, mfu 0.45%\n",
            "iter 120: loss 2.9680, time 438.76ms, mfu 0.45%\n",
            "iter 130: loss 2.8484, time 433.39ms, mfu 0.45%\n",
            "iter 140: loss 2.8559, time 434.71ms, mfu 0.45%\n",
            "iter 150: loss 2.8602, time 430.93ms, mfu 0.45%\n",
            "iter 160: loss 2.8099, time 439.59ms, mfu 0.45%\n",
            "iter 170: loss 2.8487, time 426.29ms, mfu 0.45%\n",
            "iter 180: loss 2.8000, time 434.01ms, mfu 0.45%\n",
            "iter 190: loss 2.7764, time 435.30ms, mfu 0.45%\n",
            "step 200: train loss 2.6979, val loss 2.7061\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 200: loss 2.7324, time 2151.96ms, mfu 0.41%\n",
            "iter 210: loss 2.7232, time 432.64ms, mfu 0.42%\n",
            "iter 220: loss 2.6801, time 435.19ms, mfu 0.42%\n",
            "iter 230: loss 2.7593, time 436.59ms, mfu 0.42%\n",
            "iter 240: loss 2.7147, time 432.40ms, mfu 0.42%\n",
            "iter 250: loss 2.7234, time 433.26ms, mfu 0.43%\n",
            "iter 260: loss 2.6256, time 448.87ms, mfu 0.43%\n",
            "iter 270: loss 2.6039, time 435.09ms, mfu 0.43%\n",
            "iter 280: loss 2.5878, time 430.24ms, mfu 0.43%\n",
            "iter 290: loss 2.6244, time 442.85ms, mfu 0.43%\n",
            "iter 300: loss 2.5778, time 434.41ms, mfu 0.43%\n",
            "iter 310: loss 2.5423, time 439.04ms, mfu 0.43%\n",
            "iter 320: loss 2.4951, time 440.21ms, mfu 0.44%\n",
            "iter 330: loss 2.5052, time 432.25ms, mfu 0.44%\n",
            "iter 340: loss 2.5403, time 430.02ms, mfu 0.44%\n",
            "iter 350: loss 2.4753, time 440.44ms, mfu 0.44%\n",
            "iter 360: loss 2.4071, time 429.66ms, mfu 0.44%\n",
            "iter 370: loss 2.4756, time 435.51ms, mfu 0.44%\n",
            "iter 380: loss 2.4338, time 427.75ms, mfu 0.44%\n",
            "iter 390: loss 2.3823, time 431.61ms, mfu 0.44%\n",
            "step 400: train loss 2.3851, val loss 2.3969\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 400: loss 2.3987, time 2192.54ms, mfu 0.41%\n",
            "iter 410: loss 2.3573, time 437.13ms, mfu 0.41%\n",
            "iter 420: loss 2.3935, time 429.54ms, mfu 0.42%\n",
            "iter 430: loss 2.5333, time 435.71ms, mfu 0.42%\n",
            "iter 440: loss 2.3211, time 431.39ms, mfu 0.42%\n",
            "iter 450: loss 2.3982, time 435.54ms, mfu 0.42%\n",
            "iter 460: loss 2.4564, time 432.94ms, mfu 0.43%\n",
            "iter 470: loss 2.2974, time 429.98ms, mfu 0.43%\n",
            "iter 480: loss 2.3828, time 439.50ms, mfu 0.43%\n",
            "iter 490: loss 2.3164, time 438.34ms, mfu 0.43%\n",
            "iter 500: loss 2.3121, time 430.93ms, mfu 0.43%\n",
            "iter 510: loss 2.3481, time 435.58ms, mfu 0.43%\n",
            "iter 520: loss 2.2679, time 431.71ms, mfu 0.44%\n",
            "iter 530: loss 2.2887, time 430.32ms, mfu 0.44%\n",
            "iter 540: loss 2.3443, time 437.41ms, mfu 0.44%\n",
            "iter 550: loss 2.3163, time 436.79ms, mfu 0.44%\n",
            "iter 560: loss 2.2080, time 444.46ms, mfu 0.44%\n",
            "iter 570: loss 2.3136, time 436.98ms, mfu 0.44%\n",
            "iter 580: loss 2.3057, time 439.61ms, mfu 0.44%\n",
            "iter 590: loss 2.1433, time 441.49ms, mfu 0.44%\n",
            "step 600: train loss 2.1772, val loss 2.2017\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 600: loss 2.2685, time 2168.44ms, mfu 0.40%\n",
            "iter 610: loss 2.2845, time 430.25ms, mfu 0.41%\n",
            "iter 620: loss 2.2037, time 427.28ms, mfu 0.41%\n",
            "iter 630: loss 2.1986, time 444.86ms, mfu 0.42%\n",
            "iter 640: loss 2.2251, time 432.17ms, mfu 0.42%\n",
            "iter 650: loss 2.1931, time 423.68ms, mfu 0.42%\n",
            "iter 660: loss 2.0606, time 430.30ms, mfu 0.43%\n",
            "iter 670: loss 2.1769, time 432.03ms, mfu 0.43%\n",
            "iter 680: loss 2.0779, time 430.25ms, mfu 0.43%\n",
            "iter 690: loss 2.2005, time 434.55ms, mfu 0.43%\n",
            "iter 700: loss 2.1855, time 448.13ms, mfu 0.43%\n",
            "iter 710: loss 2.1773, time 429.99ms, mfu 0.43%\n",
            "iter 720: loss 2.2191, time 432.50ms, mfu 0.44%\n",
            "iter 730: loss 2.1475, time 431.08ms, mfu 0.44%\n",
            "iter 740: loss 2.0926, time 431.15ms, mfu 0.44%\n",
            "iter 750: loss 2.0222, time 429.98ms, mfu 0.44%\n",
            "iter 760: loss 2.1127, time 427.99ms, mfu 0.44%\n",
            "iter 770: loss 2.0133, time 429.25ms, mfu 0.44%\n",
            "iter 780: loss 2.1171, time 444.98ms, mfu 0.44%\n",
            "iter 790: loss 2.0113, time 440.98ms, mfu 0.44%\n",
            "step 800: train loss 1.9606, val loss 2.0274\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 800: loss 1.9824, time 2147.60ms, mfu 0.41%\n",
            "iter 810: loss 1.9225, time 430.80ms, mfu 0.41%\n",
            "iter 820: loss 2.2513, time 432.15ms, mfu 0.42%\n",
            "iter 830: loss 2.0477, time 456.68ms, mfu 0.42%\n",
            "iter 840: loss 2.0227, time 436.02ms, mfu 0.42%\n",
            "iter 850: loss 1.9681, time 439.43ms, mfu 0.42%\n",
            "iter 860: loss 2.0201, time 444.49ms, mfu 0.42%\n",
            "iter 870: loss 1.9794, time 437.94ms, mfu 0.43%\n",
            "iter 880: loss 1.9332, time 438.85ms, mfu 0.43%\n",
            "iter 890: loss 1.9391, time 443.47ms, mfu 0.43%\n",
            "iter 900: loss 1.8931, time 430.53ms, mfu 0.43%\n",
            "iter 910: loss 1.9666, time 438.94ms, mfu 0.43%\n",
            "iter 920: loss 1.9477, time 432.91ms, mfu 0.43%\n",
            "iter 930: loss 1.9323, time 437.06ms, mfu 0.43%\n",
            "iter 940: loss 1.8478, time 429.91ms, mfu 0.44%\n",
            "iter 950: loss 1.7413, time 436.38ms, mfu 0.44%\n",
            "iter 960: loss 1.7978, time 430.94ms, mfu 0.44%\n",
            "iter 970: loss 1.8195, time 442.38ms, mfu 0.44%\n",
            "iter 980: loss 1.9425, time 432.39ms, mfu 0.44%\n",
            "iter 990: loss 1.9967, time 428.12ms, mfu 0.44%\n",
            "step 1000: train loss 1.7789, val loss 1.9209\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1000: loss 1.9019, time 2164.35ms, mfu 0.41%\n",
            "iter 1010: loss 1.8945, time 429.17ms, mfu 0.41%\n",
            "iter 1020: loss 1.9314, time 432.82ms, mfu 0.41%\n",
            "iter 1030: loss 2.0256, time 452.49ms, mfu 0.42%\n",
            "iter 1040: loss 1.8381, time 430.91ms, mfu 0.42%\n",
            "iter 1050: loss 1.8954, time 448.92ms, mfu 0.42%\n",
            "iter 1060: loss 1.8921, time 441.07ms, mfu 0.42%\n",
            "iter 1070: loss 2.0299, time 427.67ms, mfu 0.43%\n",
            "iter 1080: loss 1.7628, time 441.05ms, mfu 0.43%\n",
            "iter 1090: loss 1.7262, time 430.22ms, mfu 0.43%\n",
            "iter 1100: loss 1.8493, time 427.06ms, mfu 0.43%\n",
            "iter 1110: loss 1.7983, time 435.72ms, mfu 0.43%\n",
            "iter 1120: loss 1.7506, time 424.84ms, mfu 0.44%\n",
            "iter 1130: loss 1.9019, time 430.53ms, mfu 0.44%\n",
            "iter 1140: loss 1.6871, time 428.95ms, mfu 0.44%\n",
            "iter 1150: loss 1.8582, time 429.65ms, mfu 0.44%\n",
            "iter 1160: loss 1.6765, time 424.11ms, mfu 0.44%\n",
            "iter 1170: loss 1.7375, time 431.71ms, mfu 0.44%\n",
            "iter 1180: loss 1.7254, time 431.14ms, mfu 0.44%\n",
            "iter 1190: loss 1.7352, time 438.33ms, mfu 0.44%\n",
            "step 1200: train loss 1.6701, val loss 1.8157\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1200: loss 1.9754, time 2159.10ms, mfu 0.41%\n",
            "iter 1210: loss 1.7611, time 439.36ms, mfu 0.41%\n",
            "iter 1220: loss 1.7824, time 427.75ms, mfu 0.42%\n",
            "iter 1230: loss 1.6847, time 452.13ms, mfu 0.42%\n",
            "iter 1240: loss 1.7097, time 432.06ms, mfu 0.42%\n",
            "iter 1250: loss 1.6787, time 429.30ms, mfu 0.42%\n",
            "iter 1260: loss 1.7600, time 430.80ms, mfu 0.43%\n",
            "iter 1270: loss 1.6535, time 443.78ms, mfu 0.43%\n",
            "iter 1280: loss 1.6475, time 428.80ms, mfu 0.43%\n",
            "iter 1290: loss 1.7821, time 439.59ms, mfu 0.43%\n",
            "iter 1300: loss 1.6763, time 446.65ms, mfu 0.43%\n",
            "iter 1310: loss 1.7378, time 445.29ms, mfu 0.43%\n",
            "iter 1320: loss 1.6902, time 443.41ms, mfu 0.43%\n",
            "iter 1330: loss 1.6111, time 437.88ms, mfu 0.43%\n",
            "iter 1340: loss 1.7718, time 434.90ms, mfu 0.44%\n",
            "iter 1350: loss 1.6217, time 437.19ms, mfu 0.44%\n",
            "iter 1360: loss 1.7338, time 440.70ms, mfu 0.44%\n",
            "iter 1370: loss 1.8283, time 441.27ms, mfu 0.44%\n",
            "iter 1380: loss 1.5917, time 445.79ms, mfu 0.44%\n",
            "iter 1390: loss 1.5829, time 436.70ms, mfu 0.44%\n",
            "step 1400: train loss 1.5707, val loss 1.7473\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1400: loss 1.6387, time 2215.29ms, mfu 0.40%\n",
            "iter 1410: loss 1.5838, time 437.85ms, mfu 0.41%\n",
            "iter 1420: loss 1.5261, time 432.94ms, mfu 0.41%\n",
            "iter 1430: loss 1.6880, time 442.49ms, mfu 0.41%\n",
            "iter 1440: loss 1.5260, time 437.05ms, mfu 0.42%\n",
            "iter 1450: loss 1.6785, time 441.04ms, mfu 0.42%\n",
            "iter 1460: loss 1.7543, time 437.65ms, mfu 0.42%\n",
            "iter 1470: loss 1.7320, time 434.68ms, mfu 0.42%\n",
            "iter 1480: loss 1.5631, time 430.13ms, mfu 0.43%\n",
            "iter 1490: loss 1.8319, time 445.50ms, mfu 0.43%\n",
            "iter 1500: loss 1.6620, time 433.43ms, mfu 0.43%\n",
            "iter 1510: loss 1.6864, time 440.57ms, mfu 0.43%\n",
            "iter 1520: loss 1.5861, time 441.71ms, mfu 0.43%\n",
            "iter 1530: loss 1.8147, time 441.47ms, mfu 0.43%\n",
            "iter 1540: loss 1.6193, time 426.71ms, mfu 0.44%\n",
            "iter 1550: loss 1.5886, time 435.56ms, mfu 0.44%\n",
            "iter 1560: loss 1.6893, time 427.10ms, mfu 0.44%\n",
            "iter 1570: loss 1.5421, time 445.68ms, mfu 0.44%\n",
            "iter 1580: loss 1.5492, time 437.92ms, mfu 0.44%\n",
            "iter 1590: loss 1.6976, time 434.87ms, mfu 0.44%\n",
            "step 1600: train loss 1.5009, val loss 1.7078\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1600: loss 1.7017, time 2210.49ms, mfu 0.40%\n",
            "iter 1610: loss 1.6014, time 440.43ms, mfu 0.41%\n",
            "iter 1620: loss 1.5541, time 444.15ms, mfu 0.41%\n",
            "iter 1630: loss 1.6826, time 438.61ms, mfu 0.41%\n",
            "iter 1640: loss 1.5932, time 434.02ms, mfu 0.42%\n",
            "iter 1650: loss 1.6145, time 435.01ms, mfu 0.42%\n",
            "iter 1660: loss 1.6935, time 434.89ms, mfu 0.42%\n",
            "iter 1670: loss 1.5591, time 434.41ms, mfu 0.43%\n",
            "iter 1680: loss 1.5173, time 466.23ms, mfu 0.42%\n",
            "iter 1690: loss 1.4888, time 429.09ms, mfu 0.43%\n",
            "iter 1700: loss 1.6595, time 428.72ms, mfu 0.43%\n",
            "iter 1710: loss 1.6000, time 435.23ms, mfu 0.43%\n",
            "iter 1720: loss 1.5985, time 430.83ms, mfu 0.43%\n",
            "iter 1730: loss 1.6267, time 431.62ms, mfu 0.44%\n",
            "iter 1740: loss 1.5944, time 433.39ms, mfu 0.44%\n",
            "iter 1750: loss 1.6688, time 434.97ms, mfu 0.44%\n",
            "iter 1760: loss 1.5472, time 439.85ms, mfu 0.44%\n",
            "iter 1770: loss 1.5797, time 430.69ms, mfu 0.44%\n",
            "iter 1780: loss 1.5943, time 431.51ms, mfu 0.44%\n",
            "iter 1790: loss 1.5444, time 432.70ms, mfu 0.44%\n",
            "step 1800: train loss 1.4565, val loss 1.6488\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            "iter 1800: loss 1.6897, time 2162.78ms, mfu 0.41%\n",
            "iter 1810: loss 1.6077, time 431.59ms, mfu 0.41%\n",
            "iter 1820: loss 1.6257, time 434.05ms, mfu 0.41%\n",
            "iter 1830: loss 1.4821, time 445.61ms, mfu 0.42%\n",
            "iter 1840: loss 1.5441, time 442.51ms, mfu 0.42%\n",
            "iter 1850: loss 1.6748, time 433.23ms, mfu 0.42%\n",
            "iter 1860: loss 1.5030, time 431.90ms, mfu 0.42%\n",
            "iter 1870: loss 1.5144, time 436.13ms, mfu 0.43%\n",
            "iter 1880: loss 1.7402, time 432.03ms, mfu 0.43%\n",
            "iter 1890: loss 1.5572, time 432.09ms, mfu 0.43%\n",
            "iter 1900: loss 1.4073, time 432.82ms, mfu 0.43%\n",
            "iter 1910: loss 1.5205, time 426.19ms, mfu 0.44%\n",
            "iter 1920: loss 1.3852, time 426.72ms, mfu 0.44%\n",
            "iter 1930: loss 1.4777, time 429.08ms, mfu 0.44%\n",
            "iter 1940: loss 1.5692, time 428.30ms, mfu 0.44%\n",
            "iter 1950: loss 1.6074, time 433.40ms, mfu 0.44%\n",
            "iter 1960: loss 1.4925, time 436.13ms, mfu 0.44%\n",
            "iter 1970: loss 1.4547, time 436.96ms, mfu 0.44%\n",
            "iter 1980: loss 1.4924, time 446.24ms, mfu 0.44%\n",
            "iter 1990: loss 1.5547, time 434.83ms, mfu 0.44%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 29/32: b64_L6_H8_E256_BS16_MI1000_D10_s29 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS16_MI1000_D10_s29.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 29\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2408, val loss 4.2359\n",
            "iter 0: loss 4.2340, time 2591.04ms, mfu -100.00%\n",
            "iter 10: loss 4.1784, time 453.71ms, mfu 0.86%\n",
            "iter 20: loss 3.9832, time 453.78ms, mfu 0.86%\n",
            "iter 30: loss 3.7881, time 445.78ms, mfu 0.86%\n",
            "iter 40: loss 3.6349, time 449.16ms, mfu 0.86%\n",
            "iter 50: loss 3.5114, time 451.03ms, mfu 0.86%\n",
            "iter 60: loss 3.4159, time 465.69ms, mfu 0.86%\n",
            "iter 70: loss 3.2906, time 460.02ms, mfu 0.86%\n",
            "iter 80: loss 3.1289, time 451.30ms, mfu 0.86%\n",
            "iter 90: loss 3.1066, time 448.65ms, mfu 0.86%\n",
            "iter 100: loss 3.0378, time 450.90ms, mfu 0.86%\n",
            "iter 110: loss 2.9807, time 448.66ms, mfu 0.86%\n",
            "iter 120: loss 2.8831, time 449.06ms, mfu 0.86%\n",
            "iter 130: loss 2.8974, time 463.55ms, mfu 0.86%\n",
            "iter 140: loss 2.8373, time 452.05ms, mfu 0.86%\n",
            "iter 150: loss 2.8501, time 443.88ms, mfu 0.86%\n",
            "iter 160: loss 2.8062, time 451.80ms, mfu 0.86%\n",
            "iter 170: loss 2.7351, time 452.69ms, mfu 0.86%\n",
            "iter 180: loss 2.7435, time 452.91ms, mfu 0.86%\n",
            "iter 190: loss 2.6228, time 449.02ms, mfu 0.86%\n",
            "step 200: train loss 2.6593, val loss 2.6745\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 200: loss 2.6477, time 2299.40ms, mfu 0.79%\n",
            "iter 210: loss 2.6748, time 459.56ms, mfu 0.80%\n",
            "iter 220: loss 2.5849, time 448.20ms, mfu 0.80%\n",
            "iter 230: loss 2.6212, time 455.81ms, mfu 0.81%\n",
            "iter 240: loss 2.6118, time 444.63ms, mfu 0.81%\n",
            "iter 250: loss 2.6366, time 444.58ms, mfu 0.82%\n",
            "iter 260: loss 2.5320, time 460.47ms, mfu 0.82%\n",
            "iter 270: loss 2.5815, time 450.16ms, mfu 0.83%\n",
            "iter 280: loss 2.5656, time 447.37ms, mfu 0.83%\n",
            "iter 290: loss 2.5108, time 454.04ms, mfu 0.83%\n",
            "iter 300: loss 2.4549, time 445.70ms, mfu 0.84%\n",
            "iter 310: loss 2.4530, time 448.62ms, mfu 0.84%\n",
            "iter 320: loss 2.4447, time 451.71ms, mfu 0.84%\n",
            "iter 330: loss 2.4249, time 450.88ms, mfu 0.84%\n",
            "iter 340: loss 2.4034, time 452.31ms, mfu 0.85%\n",
            "iter 350: loss 2.4390, time 448.72ms, mfu 0.85%\n",
            "iter 360: loss 2.3592, time 444.28ms, mfu 0.85%\n",
            "iter 370: loss 2.3644, time 452.18ms, mfu 0.85%\n",
            "iter 380: loss 2.3486, time 447.36ms, mfu 0.85%\n",
            "iter 390: loss 2.3175, time 448.52ms, mfu 0.85%\n",
            "step 400: train loss 2.2961, val loss 2.3163\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 400: loss 2.3337, time 2319.83ms, mfu 0.79%\n",
            "iter 410: loss 2.3284, time 445.70ms, mfu 0.79%\n",
            "iter 420: loss 2.3399, time 456.57ms, mfu 0.80%\n",
            "iter 430: loss 2.3161, time 474.00ms, mfu 0.80%\n",
            "iter 440: loss 2.3390, time 450.53ms, mfu 0.81%\n",
            "iter 450: loss 2.3478, time 463.76ms, mfu 0.81%\n",
            "iter 460: loss 2.2799, time 444.51ms, mfu 0.82%\n",
            "iter 470: loss 2.2956, time 459.23ms, mfu 0.82%\n",
            "iter 480: loss 2.3480, time 445.37ms, mfu 0.83%\n",
            "iter 490: loss 2.2113, time 445.18ms, mfu 0.83%\n",
            "iter 500: loss 2.1847, time 454.46ms, mfu 0.83%\n",
            "iter 510: loss 2.1695, time 455.04ms, mfu 0.84%\n",
            "iter 520: loss 2.1056, time 460.56ms, mfu 0.84%\n",
            "iter 530: loss 2.1667, time 446.82ms, mfu 0.84%\n",
            "iter 540: loss 2.2062, time 444.72ms, mfu 0.84%\n",
            "iter 550: loss 2.1375, time 461.56ms, mfu 0.84%\n",
            "iter 560: loss 2.1040, time 445.04ms, mfu 0.85%\n",
            "iter 570: loss 2.2113, time 445.43ms, mfu 0.85%\n",
            "iter 580: loss 2.1182, time 451.94ms, mfu 0.85%\n",
            "iter 590: loss 2.1347, time 447.65ms, mfu 0.85%\n",
            "step 600: train loss 2.0249, val loss 2.0809\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 600: loss 2.0441, time 2335.62ms, mfu 0.78%\n",
            "iter 610: loss 2.0490, time 450.41ms, mfu 0.79%\n",
            "iter 620: loss 2.0667, time 446.88ms, mfu 0.80%\n",
            "iter 630: loss 2.0218, time 456.10ms, mfu 0.80%\n",
            "iter 640: loss 2.0297, time 451.02ms, mfu 0.81%\n",
            "iter 650: loss 1.9488, time 456.28ms, mfu 0.81%\n",
            "iter 660: loss 2.1061, time 450.64ms, mfu 0.82%\n",
            "iter 670: loss 1.9732, time 450.36ms, mfu 0.82%\n",
            "iter 680: loss 1.9935, time 462.30ms, mfu 0.83%\n",
            "iter 690: loss 1.9405, time 451.61ms, mfu 0.83%\n",
            "iter 700: loss 1.8285, time 453.17ms, mfu 0.83%\n",
            "iter 710: loss 2.0020, time 448.11ms, mfu 0.84%\n",
            "iter 720: loss 1.8811, time 449.38ms, mfu 0.84%\n",
            "iter 730: loss 1.9837, time 448.36ms, mfu 0.84%\n",
            "iter 740: loss 1.8933, time 449.83ms, mfu 0.84%\n",
            "iter 750: loss 1.8372, time 447.53ms, mfu 0.85%\n",
            "iter 760: loss 1.8977, time 457.13ms, mfu 0.85%\n",
            "iter 770: loss 1.9227, time 456.34ms, mfu 0.85%\n",
            "iter 780: loss 1.8385, time 458.52ms, mfu 0.85%\n",
            "iter 790: loss 1.9149, time 465.90ms, mfu 0.85%\n",
            "step 800: train loss 1.7612, val loss 1.8995\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 800: loss 1.7624, time 2321.34ms, mfu 0.78%\n",
            "iter 810: loss 1.8628, time 456.73ms, mfu 0.79%\n",
            "iter 820: loss 1.7639, time 450.37ms, mfu 0.79%\n",
            "iter 830: loss 1.7942, time 453.18ms, mfu 0.80%\n",
            "iter 840: loss 1.9116, time 448.08ms, mfu 0.81%\n",
            "iter 850: loss 1.7954, time 443.29ms, mfu 0.81%\n",
            "iter 860: loss 1.7342, time 449.51ms, mfu 0.82%\n",
            "iter 870: loss 1.7672, time 443.00ms, mfu 0.82%\n",
            "iter 880: loss 1.7129, time 452.94ms, mfu 0.83%\n",
            "iter 890: loss 1.7641, time 454.13ms, mfu 0.83%\n",
            "iter 900: loss 1.7188, time 458.01ms, mfu 0.83%\n",
            "iter 910: loss 1.7826, time 453.33ms, mfu 0.83%\n",
            "iter 920: loss 1.8165, time 450.89ms, mfu 0.84%\n",
            "iter 930: loss 1.6694, time 452.18ms, mfu 0.84%\n",
            "iter 940: loss 1.7652, time 447.50ms, mfu 0.84%\n",
            "iter 950: loss 1.6506, time 449.46ms, mfu 0.84%\n",
            "iter 960: loss 1.6678, time 448.17ms, mfu 0.85%\n",
            "iter 970: loss 1.7651, time 460.83ms, mfu 0.85%\n",
            "iter 980: loss 1.6410, time 448.28ms, mfu 0.85%\n",
            "iter 990: loss 1.7153, time 450.00ms, mfu 0.85%\n",
            "step 1000: train loss 1.5980, val loss 1.7742\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "iter 1000: loss 1.7331, time 2315.91ms, mfu 0.78%\n",
            "\n",
            "=== Experiment 30/32: b64_L6_H8_E256_BS16_MI1000_D20_s30 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS16_MI1000_D20_s30.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 1000\n",
            "lr_decay_iters = 1000\n",
            "\n",
            "seed = 30\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2408, val loss 4.2359\n",
            "iter 0: loss 4.2316, time 2605.43ms, mfu -100.00%\n",
            "iter 10: loss 4.1785, time 470.58ms, mfu 0.83%\n",
            "iter 20: loss 4.0184, time 459.31ms, mfu 0.83%\n",
            "iter 30: loss 3.8291, time 454.19ms, mfu 0.83%\n",
            "iter 40: loss 3.6738, time 450.07ms, mfu 0.83%\n",
            "iter 50: loss 3.5488, time 461.00ms, mfu 0.84%\n",
            "iter 60: loss 3.4573, time 458.60ms, mfu 0.84%\n",
            "iter 70: loss 3.3539, time 468.32ms, mfu 0.84%\n",
            "iter 80: loss 3.2114, time 461.77ms, mfu 0.84%\n",
            "iter 90: loss 3.1713, time 461.90ms, mfu 0.84%\n",
            "iter 100: loss 3.1119, time 457.35ms, mfu 0.84%\n",
            "iter 110: loss 3.0505, time 461.93ms, mfu 0.84%\n",
            "iter 120: loss 2.9446, time 460.81ms, mfu 0.84%\n",
            "iter 130: loss 2.9511, time 454.43ms, mfu 0.84%\n",
            "iter 140: loss 2.8864, time 462.82ms, mfu 0.84%\n",
            "iter 150: loss 2.9090, time 461.77ms, mfu 0.84%\n",
            "iter 160: loss 2.8526, time 454.29ms, mfu 0.84%\n",
            "iter 170: loss 2.7690, time 470.04ms, mfu 0.84%\n",
            "iter 180: loss 2.7823, time 455.94ms, mfu 0.84%\n",
            "iter 190: loss 2.6701, time 464.03ms, mfu 0.84%\n",
            "step 200: train loss 2.6847, val loss 2.6952\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 200: loss 2.6935, time 2353.46ms, mfu 0.77%\n",
            "iter 210: loss 2.7187, time 466.21ms, mfu 0.78%\n",
            "iter 220: loss 2.6259, time 451.04ms, mfu 0.79%\n",
            "iter 230: loss 2.6732, time 446.76ms, mfu 0.80%\n",
            "iter 240: loss 2.6569, time 468.99ms, mfu 0.80%\n",
            "iter 250: loss 2.6795, time 451.57ms, mfu 0.81%\n",
            "iter 260: loss 2.5692, time 452.29ms, mfu 0.81%\n",
            "iter 270: loss 2.6251, time 443.49ms, mfu 0.82%\n",
            "iter 280: loss 2.6181, time 447.19ms, mfu 0.82%\n",
            "iter 290: loss 2.5510, time 444.29ms, mfu 0.83%\n",
            "iter 300: loss 2.5052, time 458.38ms, mfu 0.83%\n",
            "iter 310: loss 2.4990, time 447.06ms, mfu 0.83%\n",
            "iter 320: loss 2.4883, time 451.41ms, mfu 0.84%\n",
            "iter 330: loss 2.4910, time 449.90ms, mfu 0.84%\n",
            "iter 340: loss 2.4608, time 450.35ms, mfu 0.84%\n",
            "iter 350: loss 2.4940, time 453.49ms, mfu 0.84%\n",
            "iter 360: loss 2.4081, time 452.07ms, mfu 0.84%\n",
            "iter 370: loss 2.4027, time 460.57ms, mfu 0.84%\n",
            "iter 380: loss 2.4209, time 447.46ms, mfu 0.85%\n",
            "iter 390: loss 2.3937, time 450.69ms, mfu 0.85%\n",
            "step 400: train loss 2.3501, val loss 2.3629\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 400: loss 2.3774, time 2341.04ms, mfu 0.78%\n",
            "iter 410: loss 2.3910, time 458.24ms, mfu 0.79%\n",
            "iter 420: loss 2.4028, time 459.89ms, mfu 0.79%\n",
            "iter 430: loss 2.3812, time 450.83ms, mfu 0.80%\n",
            "iter 440: loss 2.4086, time 450.06ms, mfu 0.81%\n",
            "iter 450: loss 2.4013, time 462.51ms, mfu 0.81%\n",
            "iter 460: loss 2.3194, time 458.48ms, mfu 0.81%\n",
            "iter 470: loss 2.3777, time 458.05ms, mfu 0.82%\n",
            "iter 480: loss 2.3803, time 452.55ms, mfu 0.82%\n",
            "iter 490: loss 2.2917, time 452.99ms, mfu 0.82%\n",
            "iter 500: loss 2.2650, time 463.42ms, mfu 0.83%\n",
            "iter 510: loss 2.2298, time 448.95ms, mfu 0.83%\n",
            "iter 520: loss 2.1733, time 449.98ms, mfu 0.83%\n",
            "iter 530: loss 2.2286, time 463.71ms, mfu 0.83%\n",
            "iter 540: loss 2.2807, time 453.20ms, mfu 0.84%\n",
            "iter 550: loss 2.2127, time 451.14ms, mfu 0.84%\n",
            "iter 560: loss 2.1870, time 451.69ms, mfu 0.84%\n",
            "iter 570: loss 2.2642, time 450.85ms, mfu 0.84%\n",
            "iter 580: loss 2.2031, time 467.05ms, mfu 0.84%\n",
            "iter 590: loss 2.2200, time 459.60ms, mfu 0.84%\n",
            "step 600: train loss 2.0954, val loss 2.1383\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 600: loss 2.1401, time 2323.84ms, mfu 0.78%\n",
            "iter 610: loss 2.1657, time 462.72ms, mfu 0.78%\n",
            "iter 620: loss 2.1370, time 459.47ms, mfu 0.79%\n",
            "iter 630: loss 2.1152, time 499.45ms, mfu 0.79%\n",
            "iter 640: loss 2.1352, time 456.78ms, mfu 0.79%\n",
            "iter 650: loss 2.0644, time 453.73ms, mfu 0.80%\n",
            "iter 660: loss 2.2028, time 458.06ms, mfu 0.80%\n",
            "iter 670: loss 2.0524, time 456.35ms, mfu 0.81%\n",
            "iter 680: loss 2.0937, time 454.40ms, mfu 0.81%\n",
            "iter 690: loss 2.0651, time 455.64ms, mfu 0.82%\n",
            "iter 700: loss 1.9872, time 452.82ms, mfu 0.82%\n",
            "iter 710: loss 2.0962, time 461.76ms, mfu 0.82%\n",
            "iter 720: loss 1.9829, time 460.45ms, mfu 0.83%\n",
            "iter 730: loss 2.0770, time 462.36ms, mfu 0.83%\n",
            "iter 740: loss 1.9826, time 461.14ms, mfu 0.83%\n",
            "iter 750: loss 1.9383, time 460.64ms, mfu 0.83%\n",
            "iter 760: loss 2.0069, time 459.19ms, mfu 0.83%\n",
            "iter 770: loss 2.0701, time 452.48ms, mfu 0.83%\n",
            "iter 780: loss 1.9821, time 448.26ms, mfu 0.84%\n",
            "iter 790: loss 2.0313, time 461.92ms, mfu 0.84%\n",
            "step 800: train loss 1.8647, val loss 1.9700\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 800: loss 1.8763, time 2367.04ms, mfu 0.77%\n",
            "iter 810: loss 1.9935, time 458.11ms, mfu 0.78%\n",
            "iter 820: loss 1.9133, time 451.74ms, mfu 0.79%\n",
            "iter 830: loss 1.8924, time 453.72ms, mfu 0.79%\n",
            "iter 840: loss 2.0191, time 456.35ms, mfu 0.80%\n",
            "iter 850: loss 1.9099, time 454.63ms, mfu 0.81%\n",
            "iter 860: loss 1.8687, time 453.12ms, mfu 0.81%\n",
            "iter 870: loss 1.8732, time 450.15ms, mfu 0.82%\n",
            "iter 880: loss 1.8232, time 446.60ms, mfu 0.82%\n",
            "iter 890: loss 1.8811, time 448.13ms, mfu 0.83%\n",
            "iter 900: loss 1.8738, time 450.22ms, mfu 0.83%\n",
            "iter 910: loss 1.8690, time 447.00ms, mfu 0.83%\n",
            "iter 920: loss 1.9176, time 450.96ms, mfu 0.84%\n",
            "iter 930: loss 1.7810, time 454.70ms, mfu 0.84%\n",
            "iter 940: loss 1.8750, time 454.85ms, mfu 0.84%\n",
            "iter 950: loss 1.7710, time 454.13ms, mfu 0.84%\n",
            "iter 960: loss 1.7803, time 451.12ms, mfu 0.84%\n",
            "iter 970: loss 1.8884, time 460.60ms, mfu 0.84%\n",
            "iter 980: loss 1.7615, time 453.24ms, mfu 0.85%\n",
            "iter 990: loss 1.8221, time 451.40ms, mfu 0.85%\n",
            "step 1000: train loss 1.6935, val loss 1.8591\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "iter 1000: loss 1.8250, time 2393.64ms, mfu 0.78%\n",
            "\n",
            "=== Experiment 31/32: b64_L6_H8_E256_BS16_MI2000_D10_s31 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS16_MI2000_D10_s31.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.1\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 31\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2408, val loss 4.2359\n",
            "iter 0: loss 4.2340, time 2602.94ms, mfu -100.00%\n",
            "iter 10: loss 4.1784, time 467.25ms, mfu 0.83%\n",
            "iter 20: loss 3.9832, time 462.28ms, mfu 0.83%\n",
            "iter 30: loss 3.7881, time 457.17ms, mfu 0.83%\n",
            "iter 40: loss 3.6349, time 453.54ms, mfu 0.84%\n",
            "iter 50: loss 3.5114, time 450.96ms, mfu 0.84%\n",
            "iter 60: loss 3.4159, time 462.43ms, mfu 0.84%\n",
            "iter 70: loss 3.2906, time 458.91ms, mfu 0.84%\n",
            "iter 80: loss 3.1289, time 452.42ms, mfu 0.84%\n",
            "iter 90: loss 3.1066, time 460.34ms, mfu 0.84%\n",
            "iter 100: loss 3.0378, time 455.65ms, mfu 0.84%\n",
            "iter 110: loss 2.9807, time 455.68ms, mfu 0.84%\n",
            "iter 120: loss 2.8831, time 451.66ms, mfu 0.85%\n",
            "iter 130: loss 2.8974, time 458.48ms, mfu 0.85%\n",
            "iter 140: loss 2.8373, time 462.85ms, mfu 0.85%\n",
            "iter 150: loss 2.8501, time 448.52ms, mfu 0.85%\n",
            "iter 160: loss 2.8062, time 452.98ms, mfu 0.85%\n",
            "iter 170: loss 2.7351, time 456.74ms, mfu 0.85%\n",
            "iter 180: loss 2.7435, time 457.76ms, mfu 0.85%\n",
            "iter 190: loss 2.6228, time 454.53ms, mfu 0.85%\n",
            "step 200: train loss 2.6593, val loss 2.6745\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 200: loss 2.6477, time 2327.55ms, mfu 0.78%\n",
            "iter 210: loss 2.6748, time 451.29ms, mfu 0.79%\n",
            "iter 220: loss 2.5849, time 455.80ms, mfu 0.80%\n",
            "iter 230: loss 2.6212, time 449.86ms, mfu 0.80%\n",
            "iter 240: loss 2.6118, time 453.52ms, mfu 0.81%\n",
            "iter 250: loss 2.6366, time 451.58ms, mfu 0.81%\n",
            "iter 260: loss 2.5320, time 456.20ms, mfu 0.82%\n",
            "iter 270: loss 2.5815, time 466.38ms, mfu 0.82%\n",
            "iter 280: loss 2.5656, time 453.74ms, mfu 0.82%\n",
            "iter 290: loss 2.5108, time 454.89ms, mfu 0.83%\n",
            "iter 300: loss 2.4549, time 456.98ms, mfu 0.83%\n",
            "iter 310: loss 2.4530, time 455.07ms, mfu 0.83%\n",
            "iter 320: loss 2.4447, time 465.46ms, mfu 0.83%\n",
            "iter 330: loss 2.4249, time 452.22ms, mfu 0.83%\n",
            "iter 340: loss 2.4034, time 451.97ms, mfu 0.84%\n",
            "iter 350: loss 2.4390, time 465.21ms, mfu 0.84%\n",
            "iter 360: loss 2.3592, time 463.67ms, mfu 0.84%\n",
            "iter 370: loss 2.3644, time 449.66ms, mfu 0.84%\n",
            "iter 380: loss 2.3486, time 450.48ms, mfu 0.84%\n",
            "iter 390: loss 2.3175, time 450.14ms, mfu 0.84%\n",
            "step 400: train loss 2.2961, val loss 2.3163\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 400: loss 2.3337, time 2366.12ms, mfu 0.78%\n",
            "iter 410: loss 2.3284, time 457.51ms, mfu 0.78%\n",
            "iter 420: loss 2.3399, time 452.89ms, mfu 0.79%\n",
            "iter 430: loss 2.3161, time 466.92ms, mfu 0.80%\n",
            "iter 440: loss 2.3390, time 452.66ms, mfu 0.80%\n",
            "iter 450: loss 2.3478, time 459.39ms, mfu 0.81%\n",
            "iter 460: loss 2.2799, time 454.92ms, mfu 0.81%\n",
            "iter 470: loss 2.2956, time 456.92ms, mfu 0.81%\n",
            "iter 480: loss 2.3480, time 462.01ms, mfu 0.82%\n",
            "iter 490: loss 2.2113, time 457.09ms, mfu 0.82%\n",
            "iter 500: loss 2.1847, time 455.71ms, mfu 0.82%\n",
            "iter 510: loss 2.1695, time 454.98ms, mfu 0.83%\n",
            "iter 520: loss 2.1056, time 460.44ms, mfu 0.83%\n",
            "iter 530: loss 2.1667, time 461.55ms, mfu 0.83%\n",
            "iter 540: loss 2.2062, time 456.37ms, mfu 0.83%\n",
            "iter 550: loss 2.1375, time 458.02ms, mfu 0.83%\n",
            "iter 560: loss 2.1040, time 457.44ms, mfu 0.84%\n",
            "iter 570: loss 2.2113, time 455.63ms, mfu 0.84%\n",
            "iter 580: loss 2.1182, time 452.91ms, mfu 0.84%\n",
            "iter 590: loss 2.1347, time 455.85ms, mfu 0.84%\n",
            "step 600: train loss 2.0249, val loss 2.0809\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 600: loss 2.0441, time 2339.29ms, mfu 0.77%\n",
            "iter 610: loss 2.0490, time 462.34ms, mfu 0.78%\n",
            "iter 620: loss 2.0667, time 458.42ms, mfu 0.79%\n",
            "iter 630: loss 2.0218, time 464.14ms, mfu 0.79%\n",
            "iter 640: loss 2.0297, time 456.68ms, mfu 0.80%\n",
            "iter 650: loss 1.9488, time 447.95ms, mfu 0.80%\n",
            "iter 660: loss 2.1061, time 474.21ms, mfu 0.81%\n",
            "iter 670: loss 1.9732, time 453.24ms, mfu 0.81%\n",
            "iter 680: loss 1.9935, time 453.04ms, mfu 0.82%\n",
            "iter 690: loss 1.9405, time 453.87ms, mfu 0.82%\n",
            "iter 700: loss 1.8285, time 451.51ms, mfu 0.82%\n",
            "iter 710: loss 2.0020, time 462.60ms, mfu 0.83%\n",
            "iter 720: loss 1.8811, time 448.21ms, mfu 0.83%\n",
            "iter 730: loss 1.9837, time 446.67ms, mfu 0.83%\n",
            "iter 740: loss 1.8933, time 469.04ms, mfu 0.83%\n",
            "iter 750: loss 1.8372, time 453.96ms, mfu 0.84%\n",
            "iter 760: loss 1.8977, time 450.23ms, mfu 0.84%\n",
            "iter 770: loss 1.9227, time 462.34ms, mfu 0.84%\n",
            "iter 780: loss 1.8385, time 459.30ms, mfu 0.84%\n",
            "iter 790: loss 1.9149, time 457.86ms, mfu 0.84%\n",
            "step 800: train loss 1.7612, val loss 1.8995\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 800: loss 1.7624, time 2332.05ms, mfu 0.77%\n",
            "iter 810: loss 1.8628, time 456.27ms, mfu 0.78%\n",
            "iter 820: loss 1.7639, time 451.39ms, mfu 0.79%\n",
            "iter 830: loss 1.7942, time 461.07ms, mfu 0.79%\n",
            "iter 840: loss 1.9116, time 462.84ms, mfu 0.80%\n",
            "iter 850: loss 1.7954, time 455.68ms, mfu 0.80%\n",
            "iter 860: loss 1.7342, time 454.11ms, mfu 0.81%\n",
            "iter 870: loss 1.7672, time 455.26ms, mfu 0.81%\n",
            "iter 880: loss 1.7129, time 456.60ms, mfu 0.82%\n",
            "iter 890: loss 1.7641, time 451.90ms, mfu 0.82%\n",
            "iter 900: loss 1.7188, time 454.95ms, mfu 0.83%\n",
            "iter 910: loss 1.7826, time 460.15ms, mfu 0.83%\n",
            "iter 920: loss 1.8165, time 458.65ms, mfu 0.83%\n",
            "iter 930: loss 1.6694, time 459.24ms, mfu 0.83%\n",
            "iter 940: loss 1.7652, time 461.37ms, mfu 0.83%\n",
            "iter 950: loss 1.6506, time 464.01ms, mfu 0.83%\n",
            "iter 960: loss 1.6678, time 456.47ms, mfu 0.83%\n",
            "iter 970: loss 1.7651, time 471.00ms, mfu 0.83%\n",
            "iter 980: loss 1.6410, time 459.11ms, mfu 0.83%\n",
            "iter 990: loss 1.7153, time 457.44ms, mfu 0.84%\n",
            "step 1000: train loss 1.5980, val loss 1.7742\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1000: loss 1.7331, time 2383.13ms, mfu 0.77%\n",
            "iter 1010: loss 1.6777, time 456.14ms, mfu 0.78%\n",
            "iter 1020: loss 1.6256, time 462.32ms, mfu 0.78%\n",
            "iter 1030: loss 1.5970, time 492.30ms, mfu 0.78%\n",
            "iter 1040: loss 1.7779, time 453.98ms, mfu 0.79%\n",
            "iter 1050: loss 1.6892, time 468.39ms, mfu 0.80%\n",
            "iter 1060: loss 1.7301, time 461.93ms, mfu 0.80%\n",
            "iter 1070: loss 1.6506, time 463.94ms, mfu 0.80%\n",
            "iter 1080: loss 1.6694, time 464.32ms, mfu 0.81%\n",
            "iter 1090: loss 1.6583, time 461.21ms, mfu 0.81%\n",
            "iter 1100: loss 1.5212, time 457.15ms, mfu 0.81%\n",
            "iter 1110: loss 1.6340, time 453.27ms, mfu 0.82%\n",
            "iter 1120: loss 1.5211, time 451.72ms, mfu 0.82%\n",
            "iter 1130: loss 1.6693, time 457.57ms, mfu 0.83%\n",
            "iter 1140: loss 1.6189, time 460.90ms, mfu 0.83%\n",
            "iter 1150: loss 1.6977, time 459.06ms, mfu 0.83%\n",
            "iter 1160: loss 1.6027, time 459.76ms, mfu 0.83%\n",
            "iter 1170: loss 1.6130, time 461.13ms, mfu 0.83%\n",
            "iter 1180: loss 1.6095, time 473.18ms, mfu 0.83%\n",
            "iter 1190: loss 1.5777, time 461.40ms, mfu 0.83%\n",
            "step 1200: train loss 1.4827, val loss 1.6628\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1200: loss 1.6113, time 2386.33ms, mfu 0.77%\n",
            "iter 1210: loss 1.6280, time 457.74ms, mfu 0.77%\n",
            "iter 1220: loss 1.4289, time 457.07ms, mfu 0.78%\n",
            "iter 1230: loss 1.5523, time 477.48ms, mfu 0.78%\n",
            "iter 1240: loss 1.4224, time 451.00ms, mfu 0.79%\n",
            "iter 1250: loss 1.4975, time 451.25ms, mfu 0.80%\n",
            "iter 1260: loss 1.6211, time 453.13ms, mfu 0.81%\n",
            "iter 1270: loss 1.4388, time 458.72ms, mfu 0.81%\n",
            "iter 1280: loss 1.5773, time 463.80ms, mfu 0.81%\n",
            "iter 1290: loss 1.5281, time 455.60ms, mfu 0.82%\n",
            "iter 1300: loss 1.5267, time 453.18ms, mfu 0.82%\n",
            "iter 1310: loss 1.5715, time 459.57ms, mfu 0.82%\n",
            "iter 1320: loss 1.4994, time 456.24ms, mfu 0.83%\n",
            "iter 1330: loss 1.6062, time 458.30ms, mfu 0.83%\n",
            "iter 1340: loss 1.4549, time 455.53ms, mfu 0.83%\n",
            "iter 1350: loss 1.5116, time 448.44ms, mfu 0.83%\n",
            "iter 1360: loss 1.4686, time 462.47ms, mfu 0.83%\n",
            "iter 1370: loss 1.4552, time 457.54ms, mfu 0.84%\n",
            "iter 1380: loss 1.4431, time 453.83ms, mfu 0.84%\n",
            "iter 1390: loss 1.4028, time 454.90ms, mfu 0.84%\n",
            "step 1400: train loss 1.4061, val loss 1.6145\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1400: loss 1.4605, time 2331.49ms, mfu 0.77%\n",
            "iter 1410: loss 1.4399, time 466.95ms, mfu 0.78%\n",
            "iter 1420: loss 1.4608, time 451.14ms, mfu 0.79%\n",
            "iter 1430: loss 1.4622, time 467.88ms, mfu 0.79%\n",
            "iter 1440: loss 1.5044, time 453.03ms, mfu 0.80%\n",
            "iter 1450: loss 1.4756, time 453.65ms, mfu 0.80%\n",
            "iter 1460: loss 1.4408, time 457.89ms, mfu 0.81%\n",
            "iter 1470: loss 1.4605, time 451.84ms, mfu 0.81%\n",
            "iter 1480: loss 1.4972, time 450.13ms, mfu 0.82%\n",
            "iter 1490: loss 1.4512, time 459.63ms, mfu 0.82%\n",
            "iter 1500: loss 1.4152, time 453.10ms, mfu 0.83%\n",
            "iter 1510: loss 1.3909, time 449.49ms, mfu 0.83%\n",
            "iter 1520: loss 1.3828, time 449.12ms, mfu 0.83%\n",
            "iter 1530: loss 1.5216, time 449.48ms, mfu 0.84%\n",
            "iter 1540: loss 1.4697, time 464.31ms, mfu 0.84%\n",
            "iter 1550: loss 1.4396, time 461.57ms, mfu 0.84%\n",
            "iter 1560: loss 1.3358, time 454.98ms, mfu 0.84%\n",
            "iter 1570: loss 1.4020, time 465.43ms, mfu 0.84%\n",
            "iter 1580: loss 1.4118, time 458.63ms, mfu 0.84%\n",
            "iter 1590: loss 1.3759, time 458.45ms, mfu 0.84%\n",
            "step 1600: train loss 1.3450, val loss 1.5716\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1600: loss 1.4051, time 2349.75ms, mfu 0.77%\n",
            "iter 1610: loss 1.5061, time 462.54ms, mfu 0.78%\n",
            "iter 1620: loss 1.4165, time 462.80ms, mfu 0.79%\n",
            "iter 1630: loss 1.4211, time 461.00ms, mfu 0.79%\n",
            "iter 1640: loss 1.3602, time 464.82ms, mfu 0.80%\n",
            "iter 1650: loss 1.4271, time 468.14ms, mfu 0.80%\n",
            "iter 1660: loss 1.3649, time 465.17ms, mfu 0.80%\n",
            "iter 1670: loss 1.3917, time 461.62ms, mfu 0.81%\n",
            "iter 1680: loss 1.2846, time 454.98ms, mfu 0.81%\n",
            "iter 1690: loss 1.4901, time 456.18ms, mfu 0.82%\n",
            "iter 1700: loss 1.4808, time 453.46ms, mfu 0.82%\n",
            "iter 1710: loss 1.3913, time 454.98ms, mfu 0.82%\n",
            "iter 1720: loss 1.3995, time 453.08ms, mfu 0.83%\n",
            "iter 1730: loss 1.4063, time 458.63ms, mfu 0.83%\n",
            "iter 1740: loss 1.3816, time 454.15ms, mfu 0.83%\n",
            "iter 1750: loss 1.4987, time 457.65ms, mfu 0.83%\n",
            "iter 1760: loss 1.4270, time 455.74ms, mfu 0.84%\n",
            "iter 1770: loss 1.3176, time 457.86ms, mfu 0.84%\n",
            "iter 1780: loss 1.3684, time 470.00ms, mfu 0.84%\n",
            "iter 1790: loss 1.4481, time 453.78ms, mfu 0.84%\n",
            "step 1800: train loss 1.3124, val loss 1.5507\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "iter 1800: loss 1.4363, time 2382.61ms, mfu 0.77%\n",
            "iter 1810: loss 1.3794, time 458.81ms, mfu 0.78%\n",
            "iter 1820: loss 1.4283, time 451.57ms, mfu 0.79%\n",
            "iter 1830: loss 1.4221, time 458.25ms, mfu 0.79%\n",
            "iter 1840: loss 1.3839, time 449.52ms, mfu 0.80%\n",
            "iter 1850: loss 1.4623, time 458.54ms, mfu 0.80%\n",
            "iter 1860: loss 1.3458, time 451.76ms, mfu 0.81%\n",
            "iter 1870: loss 1.3394, time 453.21ms, mfu 0.81%\n",
            "iter 1880: loss 1.3518, time 459.81ms, mfu 0.82%\n",
            "iter 1890: loss 1.3271, time 456.51ms, mfu 0.82%\n",
            "iter 1900: loss 1.4261, time 455.37ms, mfu 0.82%\n",
            "iter 1910: loss 1.3737, time 450.07ms, mfu 0.83%\n",
            "iter 1920: loss 1.3252, time 448.61ms, mfu 0.83%\n",
            "iter 1930: loss 1.3175, time 463.67ms, mfu 0.83%\n",
            "iter 1940: loss 1.3253, time 452.85ms, mfu 0.84%\n",
            "iter 1950: loss 1.3179, time 454.68ms, mfu 0.84%\n",
            "iter 1960: loss 1.2515, time 458.82ms, mfu 0.84%\n",
            "iter 1970: loss 1.3072, time 460.70ms, mfu 0.84%\n",
            "iter 1980: loss 1.3998, time 455.94ms, mfu 0.84%\n",
            "iter 1990: loss 1.3730, time 454.14ms, mfu 0.84%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n",
            "\n",
            "=== Experiment 32/32: b64_L6_H8_E256_BS16_MI2000_D20_s32 ===\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "Overriding config with b64_L6_H8_E256_BS16_MI2000_D20_s32.py:\n",
            "\n",
            "out_dir = \"/content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\"\n",
            "dataset = \"shakespeare_char\"\n",
            "eval_interval = 200\n",
            "log_interval = 10\n",
            "always_save_checkpoint = True\n",
            "\n",
            "batch_size = 16\n",
            "block_size = 64\n",
            "n_layer = 6\n",
            "n_head = 8\n",
            "n_embd = 256\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 3e-4\n",
            "max_iters = 2000\n",
            "lr_decay_iters = 2000\n",
            "\n",
            "seed = 32\n",
            "device = \"cuda\"\n",
            "\n",
            "num_workers = 0\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 40,960\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 4.74M\n",
            "num decayed parameter tensors: 26, with 4,751,616 parameters\n",
            "num non-decayed parameter tensors: 13, with 3,328 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2408, val loss 4.2359\n",
            "iter 0: loss 4.2316, time 2582.78ms, mfu -100.00%\n",
            "iter 10: loss 4.1785, time 450.70ms, mfu 0.86%\n",
            "iter 20: loss 4.0184, time 451.96ms, mfu 0.86%\n",
            "iter 30: loss 3.8291, time 457.44ms, mfu 0.86%\n",
            "iter 40: loss 3.6738, time 456.85ms, mfu 0.86%\n",
            "iter 50: loss 3.5488, time 462.03ms, mfu 0.86%\n",
            "iter 60: loss 3.4573, time 447.90ms, mfu 0.86%\n",
            "iter 70: loss 3.3539, time 452.46ms, mfu 0.86%\n",
            "iter 80: loss 3.2114, time 472.25ms, mfu 0.86%\n",
            "iter 90: loss 3.1713, time 450.43ms, mfu 0.86%\n",
            "iter 100: loss 3.1119, time 450.36ms, mfu 0.86%\n",
            "iter 110: loss 3.0505, time 450.27ms, mfu 0.86%\n",
            "iter 120: loss 2.9446, time 446.78ms, mfu 0.86%\n",
            "iter 130: loss 2.9511, time 462.48ms, mfu 0.86%\n",
            "iter 140: loss 2.8864, time 445.35ms, mfu 0.86%\n",
            "iter 150: loss 2.9090, time 447.83ms, mfu 0.86%\n",
            "iter 160: loss 2.8526, time 456.00ms, mfu 0.86%\n",
            "iter 170: loss 2.7690, time 456.75ms, mfu 0.86%\n",
            "iter 180: loss 2.7823, time 448.46ms, mfu 0.86%\n",
            "iter 190: loss 2.6701, time 450.69ms, mfu 0.86%\n",
            "step 200: train loss 2.6847, val loss 2.6952\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 200: loss 2.6935, time 2318.14ms, mfu 0.79%\n",
            "iter 210: loss 2.7187, time 462.86ms, mfu 0.80%\n",
            "iter 220: loss 2.6259, time 452.40ms, mfu 0.80%\n",
            "iter 230: loss 2.6732, time 446.94ms, mfu 0.81%\n",
            "iter 240: loss 2.6569, time 457.29ms, mfu 0.81%\n",
            "iter 250: loss 2.6795, time 456.53ms, mfu 0.82%\n",
            "iter 260: loss 2.5692, time 461.88ms, mfu 0.82%\n",
            "iter 270: loss 2.6251, time 452.89ms, mfu 0.82%\n",
            "iter 280: loss 2.6181, time 452.25ms, mfu 0.83%\n",
            "iter 290: loss 2.5510, time 462.78ms, mfu 0.83%\n",
            "iter 300: loss 2.5052, time 452.31ms, mfu 0.83%\n",
            "iter 310: loss 2.4990, time 453.62ms, mfu 0.83%\n",
            "iter 320: loss 2.4883, time 451.41ms, mfu 0.84%\n",
            "iter 330: loss 2.4910, time 450.26ms, mfu 0.84%\n",
            "iter 340: loss 2.4608, time 453.68ms, mfu 0.84%\n",
            "iter 350: loss 2.4940, time 445.94ms, mfu 0.84%\n",
            "iter 360: loss 2.4081, time 448.66ms, mfu 0.85%\n",
            "iter 370: loss 2.4027, time 476.39ms, mfu 0.84%\n",
            "iter 380: loss 2.4209, time 452.49ms, mfu 0.84%\n",
            "iter 390: loss 2.3937, time 456.36ms, mfu 0.85%\n",
            "step 400: train loss 2.3501, val loss 2.3629\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 400: loss 2.3774, time 2333.52ms, mfu 0.78%\n",
            "iter 410: loss 2.3910, time 450.02ms, mfu 0.79%\n",
            "iter 420: loss 2.4028, time 453.82ms, mfu 0.79%\n",
            "iter 430: loss 2.3812, time 453.88ms, mfu 0.80%\n",
            "iter 440: loss 2.4086, time 448.15ms, mfu 0.81%\n",
            "iter 450: loss 2.4013, time 459.69ms, mfu 0.81%\n",
            "iter 460: loss 2.3194, time 449.66ms, mfu 0.82%\n",
            "iter 470: loss 2.3777, time 461.40ms, mfu 0.82%\n",
            "iter 480: loss 2.3803, time 451.75ms, mfu 0.82%\n",
            "iter 490: loss 2.2917, time 446.60ms, mfu 0.83%\n",
            "iter 500: loss 2.2650, time 466.28ms, mfu 0.83%\n",
            "iter 510: loss 2.2298, time 449.79ms, mfu 0.83%\n",
            "iter 520: loss 2.1733, time 445.73ms, mfu 0.84%\n",
            "iter 530: loss 2.2286, time 445.73ms, mfu 0.84%\n",
            "iter 540: loss 2.2807, time 449.80ms, mfu 0.84%\n",
            "iter 550: loss 2.2127, time 450.22ms, mfu 0.84%\n",
            "iter 560: loss 2.1870, time 449.39ms, mfu 0.85%\n",
            "iter 570: loss 2.2642, time 444.36ms, mfu 0.85%\n",
            "iter 580: loss 2.2031, time 470.40ms, mfu 0.85%\n",
            "iter 590: loss 2.2200, time 453.80ms, mfu 0.85%\n",
            "step 600: train loss 2.0954, val loss 2.1383\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 600: loss 2.1401, time 2365.13ms, mfu 0.78%\n",
            "iter 610: loss 2.1657, time 454.42ms, mfu 0.79%\n",
            "iter 620: loss 2.1370, time 446.35ms, mfu 0.80%\n",
            "iter 630: loss 2.1152, time 496.00ms, mfu 0.79%\n",
            "iter 640: loss 2.1352, time 449.64ms, mfu 0.80%\n",
            "iter 650: loss 2.0644, time 450.01ms, mfu 0.81%\n",
            "iter 660: loss 2.2028, time 454.52ms, mfu 0.81%\n",
            "iter 670: loss 2.0524, time 447.27ms, mfu 0.82%\n",
            "iter 680: loss 2.0937, time 459.12ms, mfu 0.82%\n",
            "iter 690: loss 2.0651, time 449.86ms, mfu 0.83%\n",
            "iter 700: loss 1.9872, time 451.86ms, mfu 0.83%\n",
            "iter 710: loss 2.0962, time 467.05ms, mfu 0.83%\n",
            "iter 720: loss 1.9829, time 448.68ms, mfu 0.83%\n",
            "iter 730: loss 2.0770, time 449.00ms, mfu 0.84%\n",
            "iter 740: loss 1.9826, time 450.62ms, mfu 0.84%\n",
            "iter 750: loss 1.9383, time 450.94ms, mfu 0.84%\n",
            "iter 760: loss 2.0069, time 459.60ms, mfu 0.84%\n",
            "iter 770: loss 2.0701, time 447.98ms, mfu 0.84%\n",
            "iter 780: loss 1.9821, time 450.92ms, mfu 0.85%\n",
            "iter 790: loss 2.0313, time 462.94ms, mfu 0.85%\n",
            "step 800: train loss 1.8647, val loss 1.9700\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 800: loss 1.8763, time 2311.84ms, mfu 0.78%\n",
            "iter 810: loss 1.9935, time 459.02ms, mfu 0.78%\n",
            "iter 820: loss 1.9133, time 448.21ms, mfu 0.79%\n",
            "iter 830: loss 1.8924, time 482.77ms, mfu 0.79%\n",
            "iter 840: loss 2.0191, time 459.63ms, mfu 0.80%\n",
            "iter 850: loss 1.9099, time 458.03ms, mfu 0.80%\n",
            "iter 860: loss 1.8687, time 457.33ms, mfu 0.81%\n",
            "iter 870: loss 1.8732, time 460.83ms, mfu 0.81%\n",
            "iter 880: loss 1.8232, time 458.81ms, mfu 0.82%\n",
            "iter 890: loss 1.8811, time 458.41ms, mfu 0.82%\n",
            "iter 900: loss 1.8738, time 457.54ms, mfu 0.82%\n",
            "iter 910: loss 1.8690, time 459.26ms, mfu 0.82%\n",
            "iter 920: loss 1.9176, time 457.76ms, mfu 0.83%\n",
            "iter 930: loss 1.7810, time 445.87ms, mfu 0.83%\n",
            "iter 940: loss 1.8750, time 446.21ms, mfu 0.84%\n",
            "iter 950: loss 1.7710, time 450.09ms, mfu 0.84%\n",
            "iter 960: loss 1.7803, time 454.96ms, mfu 0.84%\n",
            "iter 970: loss 1.8884, time 467.51ms, mfu 0.84%\n",
            "iter 980: loss 1.7615, time 456.97ms, mfu 0.84%\n",
            "iter 990: loss 1.8221, time 449.74ms, mfu 0.84%\n",
            "step 1000: train loss 1.6935, val loss 1.8591\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1000: loss 1.8250, time 2362.59ms, mfu 0.77%\n",
            "iter 1010: loss 1.7723, time 452.12ms, mfu 0.78%\n",
            "iter 1020: loss 1.7398, time 455.70ms, mfu 0.79%\n",
            "iter 1030: loss 1.7029, time 472.50ms, mfu 0.79%\n",
            "iter 1040: loss 1.9103, time 450.06ms, mfu 0.80%\n",
            "iter 1050: loss 1.8359, time 455.69ms, mfu 0.81%\n",
            "iter 1060: loss 1.8592, time 459.04ms, mfu 0.81%\n",
            "iter 1070: loss 1.7871, time 450.59ms, mfu 0.82%\n",
            "iter 1080: loss 1.7727, time 451.31ms, mfu 0.82%\n",
            "iter 1090: loss 1.7645, time 453.94ms, mfu 0.82%\n",
            "iter 1100: loss 1.6254, time 448.35ms, mfu 0.83%\n",
            "iter 1110: loss 1.7499, time 456.25ms, mfu 0.83%\n",
            "iter 1120: loss 1.6282, time 454.98ms, mfu 0.83%\n",
            "iter 1130: loss 1.8078, time 454.97ms, mfu 0.83%\n",
            "iter 1140: loss 1.7239, time 453.99ms, mfu 0.84%\n",
            "iter 1150: loss 1.8150, time 449.15ms, mfu 0.84%\n",
            "iter 1160: loss 1.7082, time 447.79ms, mfu 0.84%\n",
            "iter 1170: loss 1.7129, time 452.63ms, mfu 0.84%\n",
            "iter 1180: loss 1.7061, time 463.97ms, mfu 0.84%\n",
            "iter 1190: loss 1.6840, time 451.01ms, mfu 0.85%\n",
            "step 1200: train loss 1.5683, val loss 1.7383\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1200: loss 1.7144, time 2366.76ms, mfu 0.78%\n",
            "iter 1210: loss 1.7295, time 459.00ms, mfu 0.78%\n",
            "iter 1220: loss 1.5255, time 459.41ms, mfu 0.79%\n",
            "iter 1230: loss 1.6688, time 480.28ms, mfu 0.79%\n",
            "iter 1240: loss 1.5495, time 450.10ms, mfu 0.80%\n",
            "iter 1250: loss 1.5937, time 448.77ms, mfu 0.81%\n",
            "iter 1260: loss 1.7292, time 452.20ms, mfu 0.81%\n",
            "iter 1270: loss 1.5326, time 453.91ms, mfu 0.82%\n",
            "iter 1280: loss 1.6744, time 459.46ms, mfu 0.82%\n",
            "iter 1290: loss 1.6195, time 452.87ms, mfu 0.82%\n",
            "iter 1300: loss 1.6346, time 451.99ms, mfu 0.83%\n",
            "iter 1310: loss 1.6672, time 458.43ms, mfu 0.83%\n",
            "iter 1320: loss 1.6117, time 458.69ms, mfu 0.83%\n",
            "iter 1330: loss 1.7096, time 459.83ms, mfu 0.83%\n",
            "iter 1340: loss 1.5772, time 449.58ms, mfu 0.84%\n",
            "iter 1350: loss 1.6201, time 451.41ms, mfu 0.84%\n",
            "iter 1360: loss 1.5792, time 456.95ms, mfu 0.84%\n",
            "iter 1370: loss 1.5448, time 450.93ms, mfu 0.84%\n",
            "iter 1380: loss 1.5475, time 452.74ms, mfu 0.84%\n",
            "iter 1390: loss 1.5092, time 465.28ms, mfu 0.84%\n",
            "step 1400: train loss 1.4872, val loss 1.6810\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1400: loss 1.5457, time 2330.76ms, mfu 0.77%\n",
            "iter 1410: loss 1.5440, time 457.74ms, mfu 0.78%\n",
            "iter 1420: loss 1.5804, time 453.72ms, mfu 0.79%\n",
            "iter 1430: loss 1.5552, time 460.25ms, mfu 0.80%\n",
            "iter 1440: loss 1.6103, time 461.58ms, mfu 0.80%\n",
            "iter 1450: loss 1.6066, time 451.42ms, mfu 0.81%\n",
            "iter 1460: loss 1.5406, time 451.75ms, mfu 0.81%\n",
            "iter 1470: loss 1.5545, time 453.76ms, mfu 0.82%\n",
            "iter 1480: loss 1.6055, time 455.59ms, mfu 0.82%\n",
            "iter 1490: loss 1.5854, time 456.64ms, mfu 0.82%\n",
            "iter 1500: loss 1.5157, time 457.73ms, mfu 0.83%\n",
            "iter 1510: loss 1.4981, time 449.60ms, mfu 0.83%\n",
            "iter 1520: loss 1.4706, time 458.24ms, mfu 0.83%\n",
            "iter 1530: loss 1.6273, time 454.26ms, mfu 0.83%\n",
            "iter 1540: loss 1.5393, time 448.64ms, mfu 0.84%\n",
            "iter 1550: loss 1.5392, time 449.53ms, mfu 0.84%\n",
            "iter 1560: loss 1.4264, time 452.69ms, mfu 0.84%\n",
            "iter 1570: loss 1.4832, time 456.24ms, mfu 0.84%\n",
            "iter 1580: loss 1.4945, time 453.17ms, mfu 0.84%\n",
            "iter 1590: loss 1.4414, time 452.58ms, mfu 0.85%\n",
            "step 1600: train loss 1.4178, val loss 1.6276\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1600: loss 1.5247, time 2340.83ms, mfu 0.78%\n",
            "iter 1610: loss 1.5738, time 450.73ms, mfu 0.79%\n",
            "iter 1620: loss 1.4963, time 458.30ms, mfu 0.79%\n",
            "iter 1630: loss 1.5103, time 462.45ms, mfu 0.80%\n",
            "iter 1640: loss 1.4731, time 453.46ms, mfu 0.80%\n",
            "iter 1650: loss 1.5389, time 454.33ms, mfu 0.81%\n",
            "iter 1660: loss 1.4856, time 458.94ms, mfu 0.81%\n",
            "iter 1670: loss 1.4806, time 450.80ms, mfu 0.82%\n",
            "iter 1680: loss 1.3222, time 457.69ms, mfu 0.82%\n",
            "iter 1690: loss 1.6329, time 458.97ms, mfu 0.82%\n",
            "iter 1700: loss 1.5747, time 456.31ms, mfu 0.83%\n",
            "iter 1710: loss 1.4744, time 451.14ms, mfu 0.83%\n",
            "iter 1720: loss 1.4982, time 450.82ms, mfu 0.83%\n",
            "iter 1730: loss 1.4879, time 446.79ms, mfu 0.84%\n",
            "iter 1740: loss 1.4755, time 452.53ms, mfu 0.84%\n",
            "iter 1750: loss 1.6033, time 447.14ms, mfu 0.84%\n",
            "iter 1760: loss 1.4768, time 452.96ms, mfu 0.84%\n",
            "iter 1770: loss 1.4094, time 449.19ms, mfu 0.85%\n",
            "iter 1780: loss 1.4850, time 460.98ms, mfu 0.85%\n",
            "iter 1790: loss 1.5325, time 447.57ms, mfu 0.85%\n",
            "step 1800: train loss 1.3807, val loss 1.6008\n",
            "saving checkpoint to /content/drive/MyDrive/nanoGPT_results/b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "iter 1800: loss 1.5159, time 2319.73ms, mfu 0.78%\n",
            "iter 1810: loss 1.4678, time 452.76ms, mfu 0.79%\n",
            "iter 1820: loss 1.4955, time 454.01ms, mfu 0.79%\n",
            "iter 1830: loss 1.5038, time 462.31ms, mfu 0.80%\n",
            "iter 1840: loss 1.4824, time 451.39ms, mfu 0.81%\n",
            "iter 1850: loss 1.5368, time 451.54ms, mfu 0.81%\n",
            "iter 1860: loss 1.4356, time 469.08ms, mfu 0.81%\n",
            "iter 1870: loss 1.4106, time 447.93ms, mfu 0.82%\n",
            "iter 1880: loss 1.4300, time 451.64ms, mfu 0.82%\n",
            "iter 1890: loss 1.4358, time 449.37ms, mfu 0.83%\n",
            "iter 1900: loss 1.5655, time 452.33ms, mfu 0.83%\n",
            "iter 1910: loss 1.4625, time 467.55ms, mfu 0.83%\n",
            "iter 1920: loss 1.4235, time 457.67ms, mfu 0.83%\n",
            "iter 1930: loss 1.3969, time 452.32ms, mfu 0.83%\n",
            "iter 1940: loss 1.4139, time 457.40ms, mfu 0.84%\n",
            "iter 1950: loss 1.4033, time 449.62ms, mfu 0.84%\n",
            "iter 1960: loss 1.3543, time 444.25ms, mfu 0.84%\n",
            "iter 1970: loss 1.3802, time 458.78ms, mfu 0.84%\n",
            "iter 1980: loss 1.4870, time 446.91ms, mfu 0.85%\n",
            "iter 1990: loss 1.4652, time 459.11ms, mfu 0.85%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 258, in <module>\n",
            "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoGPT/train.py\", line 239, in get_lr\n",
            "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
            "                  ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "ZeroDivisionError: division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/nanoGPT_results\"\n",
        "samples_dir = os.path.join(base_dir, \"samples\")\n",
        "\n",
        "os.makedirs(samples_dir, exist_ok=True)\n",
        "\n",
        "# iterate over each experiment folder\n",
        "exp_folders = sorted([\n",
        "    d for d in os.listdir(base_dir)\n",
        "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"b\")\n",
        "])\n",
        "\n",
        "print(f\"Found {len(exp_folders)} experiment folders.\")\n",
        "\n",
        "for i, exp in enumerate(exp_folders, 1):\n",
        "    exp_path = os.path.join(base_dir, exp)\n",
        "    ckpt_path = os.path.join(exp_path, \"ckpt.pt\")\n",
        "\n",
        "    if not os.path.isfile(ckpt_path):\n",
        "        print(f\"Skipping {exp} (no ckpt.pt found)\")\n",
        "        continue\n",
        "\n",
        "    out_sample = os.path.join(samples_dir, f\"{exp}_sample.txt\")\n",
        "\n",
        "    print(f\"[{i}/{len(exp_folders)}] Generating sample for {exp}\")\n",
        "\n",
        "    cmd = (\n",
        "        f\"python /content/nanoGPT/sample.py \"\n",
        "        f\"--out_dir={exp_path} \"\n",
        "        f\"--start=' ' \"\n",
        "        f\"--num_samples=3 \"\n",
        "        f\"--max_new_tokens=200 \"\n",
        "        f\"> '{out_sample}'\"\n",
        "    )\n",
        "    subprocess.run(cmd, shell=True)\n",
        "\n",
        "print(\" All samples generated and stored in:\", samples_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB31mY_Jm3BN",
        "outputId": "8db8495a-6ecb-4848-aaf6-6b238f2a684f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 32 experiment folders.\n",
            "[1/32] Generating sample for b64_L6_H4_E128_BS16_MI1000_D10_s5\n",
            "[2/32] Generating sample for b64_L6_H4_E128_BS16_MI1000_D20_s6\n",
            "[3/32] Generating sample for b64_L6_H4_E128_BS16_MI2000_D10_s7\n",
            "[4/32] Generating sample for b64_L6_H4_E128_BS16_MI2000_D20_s8\n",
            "[5/32] Generating sample for b64_L6_H4_E128_BS8_MI1000_D10_s1\n",
            "[6/32] Generating sample for b64_L6_H4_E128_BS8_MI1000_D20_s2\n",
            "[7/32] Generating sample for b64_L6_H4_E128_BS8_MI2000_D10_s3\n",
            "[8/32] Generating sample for b64_L6_H4_E128_BS8_MI2000_D20_s4\n",
            "[9/32] Generating sample for b64_L6_H4_E256_BS16_MI1000_D10_s13\n",
            "[10/32] Generating sample for b64_L6_H4_E256_BS16_MI1000_D20_s14\n",
            "[11/32] Generating sample for b64_L6_H4_E256_BS16_MI2000_D10_s15\n",
            "[12/32] Generating sample for b64_L6_H4_E256_BS16_MI2000_D20_s16\n",
            "[13/32] Generating sample for b64_L6_H4_E256_BS8_MI1000_D10_s9\n",
            "[14/32] Generating sample for b64_L6_H4_E256_BS8_MI1000_D20_s10\n",
            "[15/32] Generating sample for b64_L6_H4_E256_BS8_MI2000_D10_s11\n",
            "[16/32] Generating sample for b64_L6_H4_E256_BS8_MI2000_D20_s12\n",
            "[17/32] Generating sample for b64_L6_H8_E128_BS16_MI1000_D10_s21\n",
            "[18/32] Generating sample for b64_L6_H8_E128_BS16_MI1000_D20_s22\n",
            "[19/32] Generating sample for b64_L6_H8_E128_BS16_MI2000_D10_s23\n",
            "[20/32] Generating sample for b64_L6_H8_E128_BS16_MI2000_D20_s24\n",
            "[21/32] Generating sample for b64_L6_H8_E128_BS8_MI1000_D10_s17\n",
            "[22/32] Generating sample for b64_L6_H8_E128_BS8_MI1000_D20_s18\n",
            "[23/32] Generating sample for b64_L6_H8_E128_BS8_MI2000_D10_s19\n",
            "[24/32] Generating sample for b64_L6_H8_E128_BS8_MI2000_D20_s20\n",
            "[25/32] Generating sample for b64_L6_H8_E256_BS16_MI1000_D10_s29\n",
            "[26/32] Generating sample for b64_L6_H8_E256_BS16_MI1000_D20_s30\n",
            "[27/32] Generating sample for b64_L6_H8_E256_BS16_MI2000_D10_s31\n",
            "[28/32] Generating sample for b64_L6_H8_E256_BS16_MI2000_D20_s32\n",
            "[29/32] Generating sample for b64_L6_H8_E256_BS8_MI1000_D10_s25\n",
            "[30/32] Generating sample for b64_L6_H8_E256_BS8_MI1000_D20_s26\n",
            "[31/32] Generating sample for b64_L6_H8_E256_BS8_MI2000_D10_s27\n",
            "[32/32] Generating sample for b64_L6_H8_E256_BS8_MI2000_D20_s28\n",
            " All samples generated and stored in: /content/drive/MyDrive/nanoGPT_results/samples\n"
          ]
        }
      ]
    }
  ]
}